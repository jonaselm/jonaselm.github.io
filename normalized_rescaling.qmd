---
title: "Normalized Projective Estimator"
format: html
---

### Procedure

The procedure for the normalized projective estimator is as follows. Given a matrix of (stable) observations $\mathbf X = [X_{*,1}, \dots, X_{*,d}] \in \mathbb R^{n \times d}$ with each row representing a distinct realization and each of the $d$ columns a dimension of the dataset: 

1) Estimate marginal parameters of each column $X_{*,j}$ $j=1, \dots, d$ via univariate stable MLE.
2) Center each margin by subtracting its location paramter estimate: $X_{*,j}^{(c)} = X_{*,j} - \mu_j^{MLE}$
3) Normalize each observation by its Euclidean norm: $\tilde X_{i,*} = X_{i,*}^{(c)}/\|X_{i,*}^{(c)}\|$, $i = 1, \dots, n$
4) Rescale the data by the square root of the sum of the latent marginal variances (aka the the sum of the squared scale paramters in the stable parameterization scaled by $\sqrt 2$ to match conventional Gaussian parameterization). $\tilde{\mathbf G} = \sqrt d \bar \sigma\tilde{ \mathbf  X}$, where $\bar \sigma = \bigg(\frac{2}{d}\sum_{j=1}^d  {\gamma_j^{MLE}}^2\bigg)^{1/2}$.
5) The dispersion matrix estimate is the sample covariance matrix of $\tilde{\mathbf G}$:  $\hat \Sigma = \frac{1}{n}\tilde{\mathbf G}^{\top}\tilde{\mathbf G}$

Note that this doesn't rely on the fact that $X$ is stable, and would hold for any so-called "Gaussian scale mixture/scale mixture of normals/[normal variance-mean mixture](https://en.wikipedia.org/wiki/Normal_variance-mean_mixture)", including a multivariate t distribution and generalized hyperbolic distribution, as long as we can estimate marginal scale and location paramters. These might provide some interesting opportunities to compare this estimator against more common and nicely-behaved estimators with closed forms, etc.

In special cases (like if we can already assume $\mu$=0, and want to estimate the correlation matrix so that $\bar \sigma = 1$), we don't even need to estimate the marginal distributions.

### Benchmarking vs. Rotated MLE

I tested this estimator against prior aproaches as well as the "Rotated MLE" approach of estimating eigenvectors using the normalized sample covariance matrix and then rotating to the eigenbasis to estimate the eigenvalues along the coordinate axes via stable MLE. The parameters used were:

```python
n_samples = [100, 200, 500, 1000, 2500, 5000] 
dimensions = [2, 5, 10, 50, 75, 150, 250, 300] 
alphas = [0.5, 1.0, 1.5, 1.9] 
```
Interestingly, these two approaches perform *extremely* similarly:

![](images/projective_error.png)

Both approaches provide a huge improvement over state-of-the-art MLE. And both show convergence to the latent Gaussian sample covariance matrix:

![](images/projective_error_to_gaussian.png)

The Normalized Projective approach does seem to edge out Rotated MLE in most cases, however. Comparing the two approaches, the eigenvectors of the two estimates will always be identical (since they're both based on normalization), so the differences boil down to eigenvalue estimation.  Stable MLE is a bit of a black box, so it's possible that if error in scale estimates is increasing in the magnitude of the scale but very nonlinear, doing estimation along the principal axes makes these estimates more error-prone because the error increase in the largest eigenvalue estimate is more than the error reduction in the smallest. Since the projective method estimates scale along the original coordinate axes, these are arbitrary but perhaps lower error. I'm not convinced this is actually what's going on since I'd expect this edge to disappear as $\alpha$ increases.

Nevertheless, this led to the idea -- "Rotated Projective" -- of rotating to a new basis with approximately equal scale in all coordinate directions (this should be the minimum error estimate). Since the projective approach only needs to estimate $\text{Tr}(\Sigma)$, which is invariant under rotations, this approach should work reasonably well, even if the rotation isn't optimal.

In simulation, the Rotated Projective Estimator does seem to edge out the others (at least in total error), but not by much (this advantage is so small that on the plots above, it covers the projective approach). The benefits of this approach seem to disappear in the spiked covariance model. 

When the covariance matrix is spiked, the projective method is disadvantaged when the largest eigenvalues are dominant relative to the total variance of the data. The workaround is that we can work in a higher ambient dimension and only retain the relevant submatrix of the larger full-dimensional covariance matrix.

To test this, I used the covariance matrix from our internal U.S. equity factor model, and combined it with estimated factor exposures and specific risk with 6,422 dimensions (factors plus individual securities). Because specific risk is uncorrelated by assumption and amounts to approximately 60% of any security's total variance, working in a higher dimensional ambient space guarantees that we diminish the effects of larger eigenfactors, and get concentration of measure to our projected Gaussian. We want to retain only some subset of this ambient dimension (e.g. the factors themselves). The results of some simple/early testing are below, and are consistent with the uniform case:

![](images/projective_error_spiked.png)

As expected, we also get convergence to the latent Gaussian sample covariance matrix even when n is small relative to d:

![](images/projective_error_to_gaussian_spiked.png)

As the sample size increases, the performance gap gets closed relative to the projective method. Likewise, rotated MLE outperforms the projective method when the covariance matrix is spiked but the ambient dimension is low. Practically speaking, these aren't regimes we're operating in, but these are the limitations of this approach. We may want to talk about both projective and rotated MLE, since both give convergence guarantees to the latent Gaussian SCM but rotated MLE may be preferable if we're working in relatively low dimensions with many observations. 

We may also be able to create artificial observations that help acheive the concentration of measure effects that make the projection method work. The key is to have linearly independent components with known moments. If we can do some nonlinear transformation to make new columns with linearly independent noise, these random variables can be arbitrary as long as they're subgaussian.

### Explanation

Let {$X^{(d)}$} be a sequence of centered random vectors where $X^{(d)} \in \mathbb R^d$, having corresponding dispersion matrices {$\Sigma^{(d)}$}, $d \geq 1$ such that:

- The leading principal $k \times k$ submatrix of $\Sigma^{(d)}$ equals $\Sigma^{(k)}$ for all $k \leq d$. 
- For any d, $\Sigma^{(d)} \in \mathbb R^{d \times d}$ is well-behaved in the sense that it is positive definite and its eigenvalues $\lambda_i$ $i=1, \dots d$ are uniformly bounded so that there exist constants $m, M >0$ independent of $d$ such that $m \leq \lambda_i \leq M$ for all $i$. Additionally, assume that the average trace converges to a fixed constant as $d \rightarrow \infty$.

Using the subordinator representation, we can write $X=\sqrt W G$ in the usual way with $W \in \mathbb R^+$ a maximally skewed positive $\alpha/2$-stable subordinator and $G \sim N(0, \Sigma)$ independent of W. 

From the eigendecomposition of $\Sigma$, we can write $\Sigma = O\Lambda O^\top = O\Lambda^{1/2}\Lambda^{1/2} O^\top$. Call $\Sigma^{1/2} :=O\Lambda^{1/2}$. Let $Z \sim N(0, I_d)$ so that $G = \Sigma^{1/2}Z$.

Then we can write:

$$
\begin{aligned}
\frac{X}{\|X\|} = \frac{\sqrt WG}{\|\sqrt WG\|} = \frac{\sqrt WG}{\|\sqrt W\Sigma^{1/2}Z\|} &= \frac{\sqrt WG}{\sqrt W\|O\Lambda^{1/2}Z\|}\\ 
&= \frac{G}{\|\Lambda^{1/2}Z\|}
\end{aligned}
$$

Define the normalized trace from (Potters & Bouchaud, 2020) as $\tau(\mathbf A) := \frac{1}{d}\mathbb E[\text{Tr}(\mathbf A)]$. Then for a covariance matrix as described above, $\tau(\Sigma)$ is finite and $m \leq \tau(\Sigma) \leq M$. Note that we can also interpret the normalized trace as the average variance of the components of $G$, $\bar{\sigma}^2$ from the procedure described above.

Looking more closely at the norm from the expression above:

$$
\begin{aligned}
\frac{1}{d}\|\Lambda^{1/2}Z\|^2 & = \frac{1}{d} \sum_{i=1}^d \lambda_i Z_i^2 \\
&= \frac{1}{d}\sum_{i=1}^d (\lambda_i Z_i^2 - \lambda_i + \lambda_i)\\
&= \frac{1}{d}\sum_{i=1}^d \lambda_i(Z_i^2 - 1)+ \frac{1}{d}\sum_{i=1}^d \lambda_i \\
&=\frac{1}{d}\sum_{i=1}^d \lambda_i(Z_i^2 - 1)+ \frac{1}{d}\text{Tr}(\Sigma) \\
&= \frac{1}{d}\sum_{i=1}^d \lambda_i(Z_i^2 - 1)+ \bar{\sigma}^2\\
&\underset{d \rightarrow \infty}{\overset{\text{a.s.}}{\longrightarrow}} 0+ \bar{\sigma}^2\\
\end{aligned}
$$

The argument that $\frac{1}{d}\sum_{i=1}^d \lambda_i(Z_i^2 - 1)\underset{d \rightarrow \infty}{\overset{\text{a.s.}}{\longrightarrow}} 0$ follows from Kolmogorov's Criterion for the SLLN with independent but non-identically distributed random variables, since the sufficient conditions $Y_i:= \lambda_i(Z_i^2 - 1)$, $\mathbb E Y_i=0$ and $\sum_i^\infty \text{Var}(Y_i)/i^2 = \sum_i^\infty 2\lambda_i^2/i^2 \leq \sum_i^\infty 2M^2/i^2 =2M^2\frac{\pi^2}{6}<\infty$ are met.

Taking square roots and applying the continuous mapping theorem, we get $\frac{1}{d}\|\Lambda^{1/2}Z\|^2 \underset{d \rightarrow \infty}{\overset{\text{a.s.}}{\longrightarrow}} \bar{\sigma}^2 \implies \frac{\|\Lambda^{1/2}Z\|}{\sqrt d \bar{\sigma}}\underset{d \rightarrow \infty}{\overset{\text{a.s.}}{\longrightarrow}} 1$. 

Note for any fixed indices $i,j$, the bivariate vector elliptical vector $(X_i, X_j)$ doesn't depend on $d$, nor does the corresponding latent Gaussian bivariate vector $(G_i, G_j)$ -- (the same is true for arbitrary fixed dimension $k$). In other words, the components are the marginal distributions, and don't change by adding more dimensions to the vector.

Putting this all together, we get:

$$
\begin{aligned}
\sqrt d \bar \sigma \frac{(X_i, X_j)}{\|X\|} =\sqrt d  \bar \sigma \frac{(G_i, G_j)}{\|\Lambda^{1/2}Z\|}  \underset{d \rightarrow \infty}{\overset{\text{a.s.}}{\longrightarrow}} (G_i, G_j)
\end{aligned}
$$

Now, consider that for the full vector $G$, we can write: 
$$
\mathbb E [GG^\top] = 
\begin{pmatrix}
\mathbb{E}[G_1^2] & \mathbb{E}[G_1G_2] & \cdots & \mathbb{E}[G_1G_d] \\
\mathbb{E}[G_2G_1] & \mathbb{E}[G_2^2] & \cdots & \mathbb{E}[G_2G_d] \\
\vdots & \vdots & \ddots & \vdots \\
\mathbb{E}[G_dG_1] & \mathbb{E}[G_dG_2] & \cdots & \mathbb{E}[G_d^2]
\end{pmatrix}
$$

So that we can recover $\mathbb E [GG^\top] =\Sigma$ without ever looking at more than two components of $G$ at a time. While the matrix $\Sigma$ clearly grows with $d$, each entry is independent of the ambient dimension. 

Note that $(\frac{Z_i}{\|Z\|})^2$ $i=1, \dots, d$ is uniformly integrable, since in the uncorrelated case, a uniform random variable on the sphere $\mathbb S^{d-1}$ must satisfy $\sum_{i=1}^d (\frac{Z_i}{\|Z\|})^2=1$ and by symmetry $\mathbb E(\frac{Z_i}{\|Z\|})^2=\frac{1}{d}$. So $\mathbb E(\sqrt d \frac{Z_i}{\|Z\|})^2=1$ and doesn't depend on d. Since the eigenvalues of the covariance matrix are uniformly bounded and independent of $d$, $(\frac{\sqrt d X_i}{\|X\|})^2\overset{\text{a.s.}}{=}(\frac{\sqrt d G_i}{\|G\|})^2$ are also uniformly integrable. 

Then for any fixed $i,j \leq d$, $(\frac{\sqrt d X_i}{\|X\|}\frac{\sqrt d X_j}{\|X\|})$ is also uniformly integrable (since it's bounded above by $\max\{(\frac{\sqrt d X_i}{\|X\|})^2,(\frac{\sqrt d X_j}{\|X\|})^2\}$, and the following interchange of limits and expectation is warranted.  :
$$
\lim_{d\to\infty} \mathbb{E}\!\left[d\,\bar{\sigma}^2\,\frac{X_i}{\|X\|}\frac{X_j}{\|X\|}\right] = \mathbb{E}\!\left[ \bar{\sigma}^2\,\lim_{d\to\infty}d\,\frac{X_i}{\|X\|}\frac{X_j}{\|X\|}\right] = \mathbb{E}[G_iG_j] = \Sigma_{i,j}
$$

Since this quantity doesn't depend on $d$, this shows that
$$
d\,\bar{\sigma}^2\,\mathbb{E}\!\left[\frac{X}{\|X\|}\frac{X}{\|X\|}^\top\right] \to \mathbb{E}[GG^\top] = \Sigma
$$

in the sense that every finite block converges entrywise.

### Error Bounds

The fact that the denominator in the normalized vector simplifies to a Gaussian vector norm (almost surely) means that we should be able to apply standard quadratic form conentration bounds (e.g. Hanson-Wright) to determine the impact of this Gaussian approximation in finite dimensions (e.g. how close can we get to recovering the sample covariance matrix of the latent Gaussian vector).

If we're interested in how much error this approximation adds to the process, a reasonable thing to look at might be the probability of a large distance between the estimator versus the latent Gaussian sample covariance matrix, $\Sigma_G$ (if it were observable). This simplifies to:

$$
\begin{aligned}
\mathbb P\bigg(\bigg \| d\bar\sigma^2 \frac{XX^\top}{\|X\|^2} - GG^\top\bigg \|_F \geq t \bigg) & = \mathbb P\bigg(\bigg \| d\bar\sigma^2 \frac{GG^\top}{\|G\|^2} - GG^\top\bigg \|_F \geq t \bigg) \\ 
&= \mathbb P\bigg(\bigg \| GG^\top \bigg( \frac{d\bar\sigma^2}{\|G\|^2} - 1 \bigg) \bigg \|_F \geq t \bigg) \\
&= \mathbb P\bigg( \| GG^\top \|_F \bigg| \frac{d\bar\sigma^2}{\|G\|^2} - 1 \bigg|  \geq t \bigg) \\
&= \mathbb P\bigg( \| G \|^2 \bigg| \frac{d\bar\sigma^2}{\|G\|^2} - 1 \bigg|  \geq t \bigg) \\
&= \mathbb P( | \|G\|^2 - d \bar\sigma^2 |  \geq t ) \\
&= \mathbb P( | \|\Sigma^{1/2}Z\|^2 - d \bar\sigma^2 |  \geq t ) \\
&= \mathbb P( | (\Sigma^{1/2}Z)^\top(\Sigma^{1/2}Z) - d \bar\sigma^2 |  \geq t ) \\
&= \mathbb P( | Z^\top \Sigma Z - d \bar\sigma^2 |  \geq t ) \\
&\leq 2\exp\bigg[-c \min\bigg(\frac{t^2}{\|\Sigma\|_F^2}, \frac{t}{\|\Sigma\|_{\text{op}}}\bigg)\bigg]
\end{aligned}
$$

for some constant $c>0$. The last line is a direct plug of the Hanson-Wright inequality. 

### Efficiency

The projective estimator becomes a superefficient estimator of the dispersion matrix of a stable random vector when $d \rightarrow \infty$. This is because the Gaussian has the lowest CRLB for its scale parameter in the class of stable distributions:

![](images/CRLB.png)

This isn't actually an obvious result, since the Gaussian was thought by many to be the largest of all Cramér-Rao bounds (CRB) for many classes of similarly parameterized distributions (as claimed in B.3.26 in Spectral Analysis of Signals by Stoica and Moses), a fact disproven in [Abeida and Delmas (2019)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8825518). For instance, this is also a quality of the multivariate-t when variance is finite, and potentially extends to other distributions. It's not obvious how to extend this analytically to stable distributions, since the second moments needed for the subordinator (aka modular variate) aren't met. 

In [Dulek (2017)](https://ieeexplore.ieee.org/document/7811249), the author derives characteristic function-based lower bounds for Fisher information in the stable case. This could provide an analtyical argument for the superefficiency of this estimator.

The normalized projective estimator attains the Cramér-Rao Lower Bound (CRLB) for the underlying Gaussian covariance matrix estimate in the limit. And since the Gaussian has the lower CRLB in the class of elliptical distributions parameterized by a shape matrix, this estimator is superefficient for our stable dispersion matrix estimate.

While the distribution of $d\bar\sigma^2 X_iX_j/\|X\|^2$ doesn't have closed form moments, the variance of this estimator can be analyzed via its Taylor expansion.

#### Gaussian CRB

Assume like in our assumptions above that $G \sim N(0, \Sigma)$. Then the Slepian-Bangs formula gives the (i,j)th element of the Fisher Information Matrix for the Gaussian as  $[F]_{i,j} = N\cdot \text{Tr}\{{\Sigma}^{-1}\frac{\partial{\Sigma}}{\partial \theta_i}{\Sigma}^{-1}\frac{\partial{\Sigma}}{\partial \theta_j}\}$. 

Looking elementwise, the CRB for the (i,j)-th entry of the covariance matrix is $\text{CRB}(\Sigma_{ij}) = \frac 1 n [\Sigma_{ii}\Sigma_{jj}+\Sigma_{ij}^2]$

#### Projective Estimator 

**[Note: Need to change X to G to be consistent with the earlier notation]**

Call the estimate of  Using a Talor series expansion for the expectation and variance of the ratio $\hat\Sigma_{ij} = d\hat\sigma^2 X_iX_j/\|X\|^2$ around $(\mathbb E(d\hat\sigma^2 G_iG_j), \mathbb E(\|G\|^2))=(d\bar\sigma^2\text{Cov}(G_i,G_j), d\bar\sigma^2)$:

$$
\begin{aligned}
\mathbb E (d\bar\sigma^2 X_iX_j/\|X\|^2) &=\mathbb E (d\bar\sigma^2 G_iG_j/\|G\|^2) = \mathbb E \bigg [  \bigg] \\
&= \Sigma_{ij}+B_{ij}
\end{aligned}
$$

Where $B_{ij} =  \mathcal O(\frac{1}{d^\beta \sqrt{n}})$ with $0 < \beta \leq \frac{1}{2}$.

For the variance:

$$
\begin{aligned}
\text{Var}(d\bar\sigma^2 G_iG_j/\|G\|^2) &= \\
& = \Sigma_{ii}\Sigma_{jj}+\Sigma{ij}^2
\end{aligned}
$$

Using this, and defining $\Omega$ as the CRLB matrix. Choose $m=m(d)$ so that $\frac{m(d)}{d} < \mathcal O(1/d^2)$. 

Then assuming:

- Finite variance of univariate stable scale MLEs (DuMouchel's criteria)
- We are estimating the covariance matrix of a sub-vector, $X^{(m)}$ where $i,j \leq m$
- $\mathcal O(n) =\mathcal O(m)$

$$
\begin{aligned}
\mathbb E \|\hat \Sigma^{(m)} - \Sigma^{(m)}  \|_F^2 &= \mathbb E \bigg \| d\hat\sigma^2 \frac{X^{(m)} X^{(m)\top}}{\|X\|^2} - \Sigma^{(k)}  \bigg\|_F^2 \\
&= \mathbb E \bigg \| d\hat\sigma^2 \frac{G^{(m)}G^{(m)\top}}{\|G\|^2} - \Sigma \bigg\|_F^2 \\
&= \mathbb E \sum_{i,j} (d\hat\sigma^2 G_iG_j/\|G\|^2 - \Sigma_{ij})^2 \\
&= \sum_{i,j} \text{Var}(d\hat\sigma^2 G_iG_j/\|G\|^2)+ \sum_{i,j} \mathbb E (\Sigma_{ij} -\hat \Sigma_{ij})^2\\ 
&= \sum_{i,j} \text{Var}(d\hat\sigma^2 G_iG_j/\|G\|^2)+\sum_{i,j}\mathcal O (\frac{1}{d^{\beta/2}}) \\
&= \sum_{i,j}\text{Var}(G_iG_j) +\sum_{i,j}\mathcal O(\frac{1}{d^\beta \sqrt{n}}) \\
&= \sum_{i,j}\frac 1 n [\Sigma_{ii}\Sigma_{jj}+\Sigma_{ij}^2]+\mathcal O(\frac{1}{d^\beta \sqrt{n}}) \\
&= \text{Tr}(\Omega) + \mathcal O(\frac{1}{d^\beta \sqrt{n}}) \\
\implies  \frac{\mathbb E \|\hat \Sigma - \Sigma \|_F^2}{\text{Tr}(\Omega)} &= 1 + \mathcal O(\frac{1}{d^{1+\beta } \sqrt{n}}) \\ 
\end{aligned}
$$

When the ambient dimension grows sufficiently faster than the estimation dimension, no unbiased estimator can have a strictly smaller total MSE in the limit.  

* add assumptions 
* fix (a,b) expansion details for sigma hat
