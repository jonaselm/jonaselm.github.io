---
title: "Optimal RIE Derivation"
---

This comes from "A First Course in Random Matrix Theory", with some more simple details worked out and specifically for the multiplicative case (e.g. a covariance matrix)... This explanation is hopefully self-contained and doesn't rely on too much RMT knowledge.

### Preliminaries

Suppose we have a sample data matrix $X \in \mathbb R^{T \times N}$, with $X = Y\Sigma^{1/2}$, where $Y$ is a TxN matrix of standard Gaussian entries, and $\Sigma$ is the true covariance matrix of $X$.

Then $W_q = \frac{1}{T}Y ^ \text T Y$ is a wishart matrix with aspect ratio $q=N/T$, and the sample covariance matrix follows the model $E = \frac{1}{T}X^\text T X = \Sigma ^{1/2} {W_q \Sigma}^{1/2}$.

We want to estiamte $\Sigma$ optimally. 

### Basic Setup

Leveraging its eigendecomposition, we can also writ the sample covariance matrix as the average of rank-1 matrices: $E = \sum_{i=1}^N \lambda_i v_iv_i^{\text T}$. We want a Rotationally Invariant Estimator (RIE) of $\hat\Sigma = \Xi(E)$, such that $\Xi(OEO^{\text T})=O\Xi(E)O^{\text T}$ -- e.g. if the sample covariance matrix is rotated by some matrix O, then our estimation of $\Sigma$ will also be rotated the same way. The intuition (p.302) is that we don't have priors on the eigenvectors of $\Sigma$, so the sample eigenvectors provide at least as good a prior as any other choice we have. This means that our estimator has the same eigenvectors as $E$: 

$\Xi(E) = \sum_{i=1}^N \xi_i v_iv_i^{\text T}$, and the goal now becomes to find the adjustment to the empirical eigenvalues $\lambda_i$ of $\Sigma$ to get the corresponding "optimally adjusted" eigenvalues $\xi_i$.

### Oracle Estimator

Here, "optimal" means we want to minimize squared Frobenius error (there are many practical reasons to choose this error for financial applications). So, the optimal choice of eigenvalues $\xi_i$ minimizes:

$$
\begin{aligned}
||\Sigma - \hat \Sigma||_F^2 &= \text{Tr}(\Xi(E) - \Sigma)^2 \\
& = \sum_{i=1}^N v_i^{\text T}(\Xi(E) - \Sigma)v_i &(*) \\
&= \sum_{i=1}^N v_i^{\text T}(\Xi(E)^2 - 2\Xi(E)\Sigma + \Sigma^2)v_i \\
&= \sum_{i=1}^N \bigg( v_i^{\text T} \Xi(E)^2 v_i - 2v_i^{\text T}\Xi(E)\Sigma v_i +v_i^{\text T} \Sigma^2v_i \bigg) \\
&= \sum_{i=1}^N \bigg( v_i^{\text T} V\Omega^2V^{\text T} v_i - 2v_i^{\text T} V\Omega^2V^{\text T}\Sigma v_i +v_i^{\text T} \Sigma^2v_i \bigg) \\
&= \sum_{i=1}^N \bigg( \xi_i^2 - 2\xi_i v_i^{\text T}\Sigma v_i + v_i^{\text T} \Sigma^2v_i \bigg) 
\end{aligned}
$$

Where (*) holds as a property of a trace projected onto an orthonormal basis, and $\Omega = \text{diag}(\xi_1, \dots, \xi_n)$, the eigenvalues of the estimator.

Maximizing this quantity:

$$
\begin{aligned}
\frac{\partial}{\partial \xi_k} \bigg( \xi_k^2 - 2\xi_k v_k^{\text T}\Sigma v_k + v_k^{\text T} \Sigma^2v_k \bigg) &=  
2\xi_k - 2 v_k\Sigma v_k^{\text T}\\
\implies & \xi_k^* = v_k\Sigma v_k^{\text T}
\end{aligned}
$$

This $\xi_k^*$ is the Oracle Estimator for the true eigenvalues of $\Sigma$ given $E$. Of course, this isn't helpful since we don't know the true $\Sigma$ in the first place. This is where Random Matrix Theory comes in -- specifically, it's how we recast our dependence on $\Sigma$ as a representation that depends only on our realization $E$ (the "large-dimension miracle").

### Adding a Dash of RMT: Eigenvector Overlaps

Rewriting the estimator by expanding it over the eigenvectors of $\Sigma$:

$$
\begin{aligned}
\xi_k &= v_k\Sigma v_k^{\text T} \\
&=  v_k (\sum_{i=1}^N \mu_i u_i u_i^{\text T})v_k^{\text T} \\
&= \sum_{i=1}^N \mu_i v_k u_i u_i^{\text T}v_k^{\text T} \\
&= \sum_{i=1}^N \mu_i (u_i^{\text T}v_k)^2 \\
&\underset{N \rightarrow \infty}{\longrightarrow} \int \mu \Phi(\lambda_k, \mu) \rho_\Sigma(\mu) d\mu
\end{aligned}
$$

Where above, $\Phi(\lambda_k, \mu_j) = N\mathbb E[(v_i^{\text T}u_k)^2]$, with expectation taken over different realizations of the randomness. (See p.298 for an alternative interpretation). This expectation is necessary because unlike the eigenvalues of the sample covariance matrix, the eigenvectors will continue to flucuate when N goes to infinity, never reaching any kind of deterministic limit.

Note from the above that if it just so happens that our sample covariance matrix $E$ has the same eigenvectors as $\Sigma$, then $(u_i^{\text T}v_k)^2=1$, and the optimal estimate for $\xi_k$ is just the average eigenvalue of $\Sigma$. In general, it depends on the squared "overlaps" between the eigenvectors of the sample covariance matrix and the true covariance matrix. 

These overlaps are related to the Resolvent of the sample covariance matrix as follows:

$$
\begin{aligned}
G_E(z) &= (zI_d - E)^{-1} \\
&= (zI_d - V\Lambda V^{\text T})^{-1}\\
&= (zI_d - V\Lambda V^{\text T})^{-1}\\
&= (V(z-\Lambda)V^{\text T})^{-1}\\
&= V(z-\Lambda)^{-1}V^{\text T} \\
&= \sum_{i=1}^N\frac{v_iv_i^{\text T}}{z-\lambda_i}\\
\text{ So we get: }\\
u^{\text T}G_E(z)u &= \sum_{i=1}^N\frac{u^{\text T}v_iv_i^{\text T}u}{z-\lambda_i}\\
&= \sum_{i=1}^N\frac{(u^{\text T}v_i)^2}{z-\lambda_i}
\end{aligned}
$$

For any $u \in \mathbb R^N$ and (typically, but not necessarily small) $z \in \mathbb C$. 

The result above means that the projection of the Resolvent matrix onto a true eigenvector $u$ provides the weighted sum of squared overlaps (aka directional consistency) between the true and sample covariance matrices' eigenvectors, where weighting is inversely proportional to the spectral distances $z-\lambda_i$ using only the sample eigenvalues (e.g. those from the noisy matrix). This acts as a sort of bridge between the eigendecomposition of our noisy matrix $E$ and $\Sigma$. 

When our u is a unit vector (e.g. a vector extracted from our orthonormal basis of $\Sigma$), observe that for all $i$ we have $(u^{\text T}v_i)^2\leq 1$ and $\sum_{i=1}^N(u^{\text T}v_i)^2=1$.

Looking carefully at our new sum, we notice that the projection of the Resolvent matrix results in something resembling the Stieltjes transform, defined as $\mathfrak g_E(z)=\frac{1}{N}\text{Tr}(G_E(z))=\frac{1}{N}\sum_{i=1}\frac{1}{z-\lambda_i}$. While the Stieltjes transform $\mathfrak g_E(z)$ averages over all eigenvalues with equal weight 1/N, the projection onto $u$ gives a weighted average where weights are the squared overlaps. So, we give more weight to directions where the eigenvectors match up well. 

Now, we can use the Stieltjes inversion formula on this weighted term:

$$
\begin{aligned}
\Im \{\ u^{\text T}G_E(\lambda_i - i\eta)u \}\ &\approx \pi \rho_E(\lambda_i)\Phi(\lambda_k, \mu_j) \\
&\implies \Phi(\lambda_k, \mu_j) \approx \frac{\Im \{\ u^{\text T}G_E(\lambda_i - i\eta)u \}\ }{\pi\rho_E(\lambda_i)}
\end{aligned}
$$

In other words, we get the spectral density of E, with multiplicative scaling factors that encode the alignment of the eigenvectors. For example, if our eigenvectors are perfectly aligned, we give full weight to the sample eigenvalues. If they're somehow orthogonal, the sample eigenvalue contributes zero weight to the density.

Unfortunately, we still need to know the true eigenvectors in order to prject the Resolvent on to them...

### Another Dash of RMT: Removing Dependence on $\Sigma$

Using the stuff we derived above, we can now rewrite our adjusted eigenvalues by plugging the Stieltjes representation of the projection into the limiting case: 

$$
\begin{aligned}
\xi_k = \sum_{i=1}^N \mu_i (u_i^{\text T}v_k)^2 
&\longrightarrow \lim_{\eta \rightarrow 0^+} \sum_{i=1}^N \mu_i \frac{\Im \{\ u_i^{\text T}G_E(\lambda_i - i\eta)u_i \}\ }{\pi\rho_E(\lambda_i)} \\
&= \frac{1}{\pi\rho_E(\lambda_k)} \lim_{\eta \rightarrow 0^+} \Im \bigg\{\ \sum_{i=1}^N  \bigg(u_i^{\text T}\mu_i G_E(\lambda_i - i\eta)u_i \bigg)\bigg\}\ \\
&= \frac{1}{\pi\rho_E(\lambda_k)}\lim_{\eta \rightarrow 0^+} \Im\bigg[\text{Tr}(\Sigma G_E(\lambda_i - i\eta)) \bigg]\\
&=\frac{1}{\pi\rho_E(\lambda_k)}\lim_{\eta \rightarrow 0^+} \Im \bigg[\tau(\Sigma G_E(\lambda_i - i\eta))\bigg]
\end{aligned}
$$

Here, the last line is an informal convention where the normalized trace is used by convention in place of the trace in the high-dimensional asymptotic setting to ensure convergence to a number, the second-to-last line comes from the fact that if we expand the trace by the eigenbasis of $\Sigma$, we get Sigma back...

**Okay, here's where the magic happens:** Continuing using the subordination equation (19.30), we write $G_E(z) = Y(z)G_\Sigma(Z(z))$, where in the multiplicative case, $Y(z) = Z(z)/z$.

::: {.callout-note}

**Note, if we don't want to take it at face value that this subordination relationship is true, let's derive it...**

Since our original setup involves multiplying $E = \Sigma ^{1/2} {W_q \Sigma}^{1/2}$, we can use the S-transform for freely convoluted matrices (remember, $\Sigma$ is a deterministic matrix, so it is automatically mutually free w/r/t any random matrix): $S_E(t) = S_\Sigma(t) S_{W_q}(t)$.

For a Wishart matrix $W_q$, the S-transform is explicitly known: $S_{W_q}(t)=\frac{1}{1+qt}$. So now we have $S_E(t) = S_\Sigma(t) \frac{1}{1+qt}$.

Next, we leverage the t-transform, which is related to the Stieltjes transform $\mathfrak{g}_E(z)$, of $E$ by: $t(z)=z\mathfrak g_E(z)-1$. The S-transform is (by definition): $S_\Sigma(t)= \frac{1+t_\Sigma(z)}{t_\Sigma(z)}\frac{z}{t_\Sigma(z)}$. Plugging this in:

$S_E(t) = \frac{1+t_E(z)}{t_E(z)}\frac{z}{t_E(z)} = \frac{1+t_\Sigma(z)}{t_\Sigma(z)}\frac{z}{t_\Sigma(z)} \frac{1}{1+qt_E(z)}$.

Simplifying the equality involving the right two terms, this becomes: $t_E(z) = t_\Sigma\bigg( \frac{t_E(z)}{1+qt_E(z)}\bigg)$. Plugging back in the definition of the t-transform:

$z\mathfrak g_E(z)-1 = Z\mathfrak g_\Sigma(Z)-1$, where $Z=\frac{t_E(z)}{1+qt_E(z)}$ (from the argument in $t_\Sigma$ above). 

So $\mathfrak g_E(z) = \frac{1}{z}\frac{t_E(z)}{1+qt_E(z)}\mathfrak g_\Sigma(Z)$. Letting $Y(z)=Z(z)/z=\frac{1}{z}\frac{t_E(z)}{1+qt_E(z)}$, we finally get $\mathfrak g_E(z) = Y(z)\mathfrak g_\Sigma(Z(z))$.This is weaker than the original claim from the book that the resolvent matrices are related by this subordination relationship, but it's sufficient to prove the next part.

:::

Plugging this into the expression above:

$$
\begin{aligned}
\tau(\Sigma G_E(z))=Y\tau(\Sigma G_\Sigma(Z))&=Y\tau\bigg((\Sigma-Z\mathbf I_d + Z \mathbf I_d)(Z \mathbf I_d - C)^{-1}\bigg) \\
&=YZ\mathfrak g_\Sigma(Z)-Y \\
&= Z\mathfrak g_E(z) -Y(z)
\end{aligned}
$$

This is an expression that does not expliticly depend on $\Sigma$ anymore. 

This means we have a general form for the multiplicative RIE eigenvalues that doesn't depend on anything more than the multiplicative form of the matrix $E$ and freeness:

$$
\begin{aligned}
\xi_k &= \frac{1}{\pi\rho_E(\lambda_k)}\lim_{\eta \rightarrow 0^+} Z(z_k)\mathfrak g_E(z_k) -Y(z_k), & z_k := \lambda_k-i\eta
\end{aligned}
$$

Now, all of the quantities on the right hand side can be estimated from data. For our specific case of Wishart noise, it's appealing to use the subordination relation of (13.46) using the T-matrix, which is an alternative form of the resolvent, and not to be confused with the T transform: $T_E(z) = zG_E(z)-I$. The T-transform has its own subordination relation for multiplication of this form, making it a little easier to deal with due to the leading $z$ term, and a wonder why we didn't start with that in the first place... $T_E(z)=T_\Sigma[zS_W(t_E(z))]$. Rewriting (19.29) using this:

$$
\begin{aligned}
\xi_k &=\frac{1}{\pi\rho_E(\lambda_k)}\lim_{\eta \rightarrow 0^+} \Im \bigg[\tau(\Sigma G_E(\lambda_i - i\eta))\bigg] \\
&= \frac{1}{\pi\rho_E(\lambda_k)}\lim_{\eta \rightarrow 0^+} \Im \bigg[\tau(\Sigma \frac{1}{z}T_\Sigma[zS_W(t_E(z))])\bigg]\\
&=\frac{\lim_{\eta \rightarrow 0^+} \Im \bigg[\tau(\Sigma T_\Sigma[zS_W(t_E(z))])\bigg]}{\lim_{\eta \rightarrow 0^+} \Im t_E(z)} & z:=\lambda -i\eta
\end{aligned}
$$

Hopefully you've been reading the book carefully at this point, since this derivation is left out but uses the not-so-obvious special T-version of the Sokhotski-Plemelj formula: $\lim_{\eta \rightarrow 0^+} \Im t(x-i\eta) = \pi x \rho(x)$ found in (11.90)

Simplifying the numerator, using the fact that $T_\Sigma(z)=\Sigma G_\Sigma(z) =\Sigma(zI-\Sigma)^{-1}$ (11.91):

$$
\begin{aligned}
\tau(\Sigma T_\Sigma[zS_W(t_E(z))]) &= \tau(\Sigma^2(zS_W(t)I-\Sigma)^{-1})\\
&= \tau(\Sigma(\Sigma - zS_W(t)I+zS_W(t)I)(zS_W(t)I-\Sigma)^{-1})\\
&= \tau(\Sigma)+zS_W(t)t_\Sigma(zS_W(t)) \\
&= \tau(\Sigma)+zS_W(t)t_E(z)\\
&\implies \Im[\tau(\Sigma)+zS_W(t)t_E(z)] = \Im[zS_W(t)t_E(z)]
\end{aligned}
$$

FINALLY, putting all of this together:

$$
\begin{aligned}
\xi_k 
&=\frac{\lim_{\eta \rightarrow 0^+} \Im \bigg[\tau(\Sigma T_\Sigma[zS_W(t_E(z))])\bigg]}{\lim_{\eta \rightarrow 0^+} \Im t_E(z)} & z:=\lambda -i\eta \\
&= \lambda \frac{\lim_{\eta \rightarrow 0^+} \Im[S_W(t)t_E(z)]}{\lim_{\eta \rightarrow 0^+} \Im t_E(z)}
\end{aligned}
$$

This is true for any multiplicative noise process where the noise has the S-transform described by $S_W(z)$ and the estimated properties of $E$ are gleaned from the sample covariance matrix.

Returning to our original assumptions, if $W=W_q$ is a Wishart matrix, then $S_{W_q}=(1+qt)^{-1}$. In the bulk region $\lambda_- < \lambda < \lambda_+$ of the eigenvalue spectrum, $t=t_E(z)$ is complex with nonzero imaginary part. 

Next, observet that  $\Im\bigg[\frac{t}{1+qt}\bigg]=\Im\bigg[\frac{t(1+qt^*)}{|1+qt|^2}\bigg]=\Im\bigg[\frac{t+q|t|^2}{|1+qt|^2}\bigg]=\frac{1}{|1+qt|^2}\Im t$

Plugging this in:

$$
\begin{aligned}
\xi(\lambda) &= \lambda \frac{\lim_{\eta \rightarrow 0^+} \Im[\frac{t}{1+qt}]}{\lim_{\eta \rightarrow 0^+} \Im t} \\
&= \frac{\lambda}{|1+qt_E(\lambda-i\eta)|^2}\bigg|_{\eta \rightarrow 0^+}\\
&= \frac{\lambda}{|1+-q+q\mathfrak g_E(\lambda-i\eta)|^2}\bigg|_{\eta \rightarrow 0^+}
\end{aligned} 
$$

That's it.