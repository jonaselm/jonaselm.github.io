[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Progress Site",
    "section": "",
    "text": "I’ll add more structure to this site as I collect more results than I can fit in this format.\nThe links above provide general background on my problem setting, current progress, and some additional questions (and hopefully their answers) that might not be perfectly aligned with that path.\n\nResearch Plan\n\nFind optimal estimator for underlying dependence\n\nSubordinated Covariance Estimator\nSubordinated Correlation Estimator\nRestricted Expectation-Maximization\n\nInvestigate potential benefits of a median-of-means approach for heavy tails and small observations\nInvestigate high-dimensionality adjustments\nApplications"
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "Since I realize that this problem setting may be a little unfamiliar, I’m including a brief background on \\(\\alpha\\)-stable random variables and the research question.\nI’m interested in estimating the dependence of a random vector when covariance may not exist and the dimension of the vector may be high relative to the number of observations. We’re principally interested in a symmetric, positive semi-definite covariance matrix-like object \\(\\Sigma\\) or its corresponding correlation matrix, since:\nIn financial markets, distributions of returns (and log-returns, which are used interchangeably and often without distinction) often have heavy tails. This leads to the relaxation of the finite variance assumption. Even if a process has finite variance, my conjecture is that an estimator that admits heavy tails may be better (e.g. more efficient) in finite/small samples.\nIn the literature, correlation and covariance matrices are used interchangably, since the correlation matrix is just a covariance matrix of an alternative dataset where the data have been standardized."
  },
  {
    "objectID": "background.html#stable-distributions-and-heavy-tailed-random-vectors",
    "href": "background.html#stable-distributions-and-heavy-tailed-random-vectors",
    "title": "Background",
    "section": "Stable Distributions and Heavy-Tailed Random Vectors",
    "text": "Stable Distributions and Heavy-Tailed Random Vectors\n\n\n\n\n\n\nWhy Stable?\n\n\n\nFor the why behind using stable distributions to connect to theoretical results, see the page Why Stable?\n\n\nIf we relax the assumption that variance is finite, then a reasonable model for returns is the stable distribution, which has no closed form pdf in general but has a complicated characteristic function where the dependence is fully defined by the spectral measure (typically denoted \\(\\Gamma\\)), a finite measure on the unit circle \\(\\mathbb S^d\\).\nStable distributions are a reasonable choice because they are infinitely divisible, and by the Generalized Central Limit Theorem of Gnedenko and Kolmogorov (1954), if sums of i.i.d. random variables converge to a distribution, it must be stable. This mirrors the justifications often used for the normal distribution in financial models.\nIn fact, the normal distribution is a special case when the characteristic exponent \\(\\alpha = 2\\). It is the only case where variance is finite. The two other closed form pdfs of the stable family are the Cauchy (\\(\\alpha = 1\\)) and the Lévy (\\(\\alpha = 1/2\\)).\nStable distributions are admittedly confusing because of the complicated characteristic function and infinite variance, as well as the fact that some terms (e.g. sub-Gaussian) mean something totally different in the stable distribution context than in the broader statistics context.\nLikewise there are nearly a dozen known parameterizations of the stable distribution, which makes it hard to distinguish what an author means when they describe a result.\nA special case is the sub-Gaussian (aka subordinated Gaussian) symmetric \\(\\alpha\\)-stable distribution (to avoid confusion with sub-Gaussian tails, I’ll note this as \\(\\alpha\\)-SG(\\(\\Sigma\\)), as some texts do), which for a random vector X can be written:\n\\[\nX = \\sqrt W G\n\\]\nWhere W \\(\\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers, and \\(G \\sim N(0, \\Sigma)\\), a multivariate normal random vector independent of W.\nThe proof of this representation can be found here.\nIn this sub-Gaussian random vector, the spectral measure \\(\\Gamma\\) is fully determined by \\(\\Sigma\\). In fact, \\(\\Gamma\\) symmetric (e.g. antipodal points have equal measure), and continuous. While it’s a bit counter-intuitive, even in the isotropic case where \\(\\Sigma = I_d\\), the identity matrix, the components of X are not independent. Indeed, the idea of orthogonality isn’t defined (although some generalizations of this idea are), since X is not defined on a (pre-)Hilbert space when \\(\\alpha &lt; 2\\). This actually turns out to be a desirable characteristic for a couple of reasons. For one, since tail dependence is asymptotically zero in the Gaussian case, a contributor to the failure of the Gaussian copula model for derivative pricing during the 2008 financial crisis. This model errs on the side of overestimating risk.\nWhen \\(X \\sim\\) \\(\\alpha\\)-SG(\\(\\sigma^2\\)), it has the following simplified, more familiar characteristic function:\n\\[\n\\mathbb E[e^{iu^\\text T X}] = exp\\bigg\\{-\\frac12 |u^\\text T \\Sigma u|^{\\alpha/2}\\bigg\\}\n\\]\nWhen \\(\\alpha =2\\), this is the Gaussian CF.\nWe want the best estimate of \\(\\Sigma\\) or its corresponding correlation matrix.\nThe best reference on multivariate stable distributions is Stable Non-Gaussian Random Processes by Samoradnitsky and Taqqu (1994), which contains virtually all known theoretical results as of its publication (despite the book’s 30th anniversary, not much has changed). For a more modern treatment that’s limited to univariate stable random variables, John Nolan’s aptly-named Univariate Stable Distributions: Models for Heavy Tailed Data from 2020 is recommended.\nOne important note is that parameterizations of stable distributions are very inconsistent in the literature."
  },
  {
    "objectID": "background.html#high-dimensional-dependence-estimation",
    "href": "background.html#high-dimensional-dependence-estimation",
    "title": "Background",
    "section": "High-Dimensional Dependence Estimation",
    "text": "High-Dimensional Dependence Estimation\nThe other piece of the research question puzzle is “high-dimensional” data. By high-dimensional, I mean that the dimension of the data is high relative to the number of observations; so 30 dimensions is quite high-dimensional when you only have 35 observations to estimate something like a covariance matrix from.\nThe current state-of-the-art approach used in finance is the Optimal Rotationally-Invariant Estimator (RIE), which uses results from Random Matrix Theory to find the optimal way to “clean” the eigenvalues of a sample covariance matrix C so that the squared Frobenius distance \\(||\\Sigma - C||_F^2\\) is minimized, even without being able to observe the true \\(\\Sigma\\), provided that the data are at least asymptotically normal.\nInitially, my plan was to explore how this recipe might change if the underlying data do not have finite variance (e.g. finding \\(\\Sigma\\) from a sub-Gaussian \\(\\alpha\\)-stable random vector described above). More recently, I’ve found some research that suggests that if \\(1 &lt; \\alpha \\leq 2\\), then the universality rules apply asymptotically and this same RIE formulation is appropriate to use for a Lévy matrix. If true, this would provide a theoretical justification for using this approach for heavy-tailed data."
  },
  {
    "objectID": "background.html#current-estimator",
    "href": "background.html#current-estimator",
    "title": "Background",
    "section": "Current Estimator",
    "text": "Current Estimator\nMy candidate estimator, Median Oracle, takes a very simple heuristic approach to estimating the quasi-correlation from a matrix \\(X\\) of returns. Assume \\(X\\) is a \\(t\\times n\\) matrix consisting of \\(t\\) independent observations of a market of \\(n\\) assets, and a sub-sample size \\(d_in\\):\n\n(Optional_ Rescale the columns of \\(X\\) to be mean-zero with unit variance, such that we’re estimating \\(\\Sigma\\) as a correlation matrix.\nFor \\(i=1,..., B\\):\n\nDraw \\(b \\sim Unif[1,t-d_in)\\)\nAt each \\(b\\), subsample \\(X_b = X_{b:(b+d_in-1)}, \\cdot\\)\nCompute the sample covariance matrix \\(\\Sigma_b = \\frac{1}{d_{in}-1}X_b^\\text T X_b\\).\nCompute the Rotationally Invariant Estimator (RIE) adjustment to the eigenvalues \\(\\hat\\lambda_{1}^{(1)}, ..., \\hat\\lambda_{i}^{(n)}\\) of \\(\\Sigma_b\\), and store them in \\(Q \\in \\mathbb R^{B \\times n}\\)\n\nFor for each eigenvalue \\(\\lambda^{(j)}\\) of the covariance matrix, \\(j = 1, ..., n\\), estimate \\(\\hat \\lambda^{(j)} = \\text{median}(\\hat\\lambda_{i}^{(j)})\\), the median across all \\(B\\) samples.\nReconstruct the covariance matrix using \\(\\hat\\Lambda = \\text{diag}(\\hat \\Lambda^{(n)},...,\\hat \\lambda^{(1)})\\) and the eigenvectors of the full-sample covariance matrix \\(\\Sigma = V^\\text{T} \\Lambda V\\).\nRescale by forecasted asset-wise standard deviation to get a final estimate of the underlying covariance matrix.\n\nTaking the sample covariance and correlations of a dataset assumed to be generated by an infinite-variance process seems crazy initially, but in fact the eigenvalue spectrum of these subsampled covariance matrices are well defined distributions."
  },
  {
    "objectID": "open_questions.html",
    "href": "open_questions.html",
    "title": "Some Questions (And Answers)",
    "section": "",
    "text": "Can we generalize the equivalence property of Fractional Lower-Order Moments (FLOMs) to other distributions where covariance exists?\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nThis becomes obvious from the definition of the characteristic function. Let \\(X = \\mu + \\gamma Z\\) be a random variable with mean \\(\\mu\\) (note that \\(\\mathbb EX \\nless \\infty\\) is not actually required. If X is stable with \\(\\alpha \\leq 1\\), we replace mean with location and it still works - but X need not be a location scale family), and characteristic function \\(\\varphi_X(t)\\). Then for any p for which \\(\\varphi_X(t)\\) is differentiable at \\(t=0\\), the p-th fractional moment exists (Laue, 1980) and can be factored to some constant that depends only on the distribution parameters \\(\\mathbf \\theta\\) times the scale parameter survives raised to the p-th power thanks to the product rule:\n\\[\n\\begin{aligned}\n\\mathbb E(|X-\\mu|^p) & = \\mathbb E(\\gamma Z_+^p)+\\mathbb E (\\gamma Z_-^p)\\\\\n& = (-i)^p \\bigg(\\frac{d^p}{dt^p}\\varphi_{\\gamma Z_+}(t)+\\frac{d^p}{dt^p}\\varphi_{\\gamma Z_-}(t)\\bigg)\\bigg |_{t=0} \\\\\n& = (-i)^p\\frac{d^p}{dt^p}\\bigg(\\varphi_{Z_+}(\\gamma t)+\\varphi_{Z_-}(\\gamma t)\\bigg)\\bigg |_{t=0} \\\\\n& = \\gamma^p c(\\mathbf \\theta)\n\\end{aligned}\n\\]\nWhen the second moment exists, note that \\(\\gamma^2\\) is at worst a rescaled parameterization of the variance of the distribution, since:\n\\[\n\\gamma^2 c(\\mathbf \\theta) = \\mathbb E(|X-\\mu|^2) = \\text{Var}(X) \\implies \\gamma^2 \\propto \\text{Var}(X)\n\\]\nThis result is helpful for a couple of reasons.\n\nIf we’re comparing random varables that differ only by a scale parameter (e.g. otherwise, they share \\(\\mathbf \\theta\\)), we can get a good estimate of “correlation” because these constants will cancel.\nIf we apply FLOMs to arbitrary random variables with finite variance, the p-th root of the FLOM will be proportionate to the actual standard deviation. This makes this estimator robust to model misspecification (think lognormal distribution)\n\n\n\n\n\nWhat p is actually optimal, given some \\(\\theta\\).\n\nDoes p actually depend on \\(\\alpha\\)? Can we choose some clever \\(\\theta\\) that will turn the Taylor series approximation into an exact result?\n\nTo what extent does the resulting correlation matrix result in eigenvalue regularization compared to a sample covariance matrix? What does that regularization function look like?\nIs RIE appropriate to use on sample covariance estimates derived from stable data?\n\n(Two approaches: universality when \\(\\alpha &gt;1\\), \\(\\Sigma\\) as a covariance matrix of a Gaussian vector)"
  },
  {
    "objectID": "progress.html",
    "href": "progress.html",
    "title": "Current Progress",
    "section": "",
    "text": "I’ve split the problem into two pieces: getting an “optimal” estimate of the covariance/correlation matrix underlying a stable-sub-Gaussian random vector, and adjusting for high-dimensionality. It’s a little hand-wavy, but I think this is reasonable since the RIE is a monotonically increasing function of the eigenvalues of the input. So, by the invariance of quantiles under monotone transformations, the median RIE estimate at each eigenvalue should be the same as the RIE of the median eigenvalue estimate."
  },
  {
    "objectID": "progress.html#estimating-sigma",
    "href": "progress.html#estimating-sigma",
    "title": "Current Progress",
    "section": "Estimating \\(\\Sigma\\)",
    "text": "Estimating \\(\\Sigma\\)\nWe want to estimate the covariance matrix \\(\\Sigma\\) of the underlying Gaussian vector that defines the dependence structure of our stable random variable \\(X\\).\nKodia and Garel propose a signed symmetric covariation coefficient:\n\\[\n\\text{scov} (X_1, X_2) = \\kappa_{(X_1, X_2)}\\bigg | \\frac{[X_1, X_2]_\\alpha [X_2, X_1]_\\alpha}{||X_1||_\\alpha ||X_2||_\\alpha} \\bigg|^{1/2}\n\\]\nHere, \\([X_1, X_2]_\\alpha\\) and \\(||X_1||_\\alpha\\) are the covariation of \\(X_1\\) on \\(X_2\\) and the covariation norm of \\(X_1\\), respectively, and \\(\\kappa\\) represents the “agreement” of the signs of the covariation terms.\nWhen \\(1 &lt; \\alpha &lt; 2\\), this can equivalently be represented using Fractional Lower Order Moments (FLOMs):\n\\[\nr = \\frac{(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p}}{2(\\mathbb E |X_1|^p\\mathbb E |X_2|^p)^{1/p}}\n\\]\nIn both cases, when \\(X\\) is a sub-Gaussian random vector with underlying Gaussian vector \\(G\\), these quantities coincide with the correlation coefficient between \\(G_1\\) and \\(G_2\\).\nKodia and Garel show that straightforward estimators based on these quantities are strongly consistent estimators of the Gaussian correlation matrix.\n\nBias of the FLOM Estimator\nNote that FLOMs aren’t unbiased estimators of scale/dependence unless \\(p=2\\).\nProposition 1 on p.32 of Nikias and Shao is:\n\\[\n\\mathbb E(|X|^p) = C(p,\\alpha)\\gamma^{p/\\alpha}\n\\]\nwhere \\(C(p,\\alpha)\\) is a known constant that depends only on p and the tail exponent of X and is used below, and \\(\\gamma\\) is the scale parameter of X. Note that the authors use a non-typical parameterization for \\(\\gamma\\) but this result holds in general for any distribution – rough/hand-wavy proof here.\nOne nice thing is that if we’re estimating a correlation-type quantity like scov and r above, these bias terms cancel out in the numerator and denominator, and we’re left only with the scale terms.\n\n\nVariance of the FLOM Estimator\nOne area that (surprisingly) has received little attention in the literature is the variance of these FLOM estimators. While the variance of \\(X\\) may be infinite, we can estimate its scale and dependence using FLOMs with finite variance.\nIt’s known that for \\(p&lt;\\alpha, \\mathbb E |X|^p &lt; \\infty\\). Therefore, the variance of \\(|X|^p\\) exists for \\(p&lt;\\alpha/2\\). Several papers estimate sample FLOM variance using Monte Carlo simulations, but I haven’t seen analytical representations in any papers. In this case, it’s a straightforward application of the expected value of the FLOM:\n\\[\n\\begin{aligned}\n\\text{Var}(\\frac{1}{N}\\sum_{i=1}^N |X|^p) & = N^{-2} \\text{Var}(\\sum_{i=1}^N |X|^p) \\\\\n&= N^{-1} [\\mathbb E(|X|^{2p}) - \\mathbb E(|X|^p)^2] \\\\\n&= N^{-1} [C(2p,\\alpha)\\gamma^{2p/\\alpha}-(C(p, \\alpha)\\gamma^{p/\\alpha})^2]\\\\\n&= \\frac{2^{2p+2}\\gamma^{2p/\\alpha}}{N \\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\}\n\\end{aligned}\n\\]\nWhat’s interesting is that simulation results are problematically misleading when \\(X\\) is nearly Gaussian:\n\nThe chart above simulates 10,000 samples to estimate the standard error of the FLOM estimator as a function of p at various values of \\(\\alpha\\). When p gets near the infinite variance regime, simulations will materially underestimate the variability of the estimator, especially when \\(\\alpha \\uparrow 2\\). Heuristically, this is because tails are only a little heavier than the Gaussian case, and you’re much less likely to get extreme realizations that traverse the full support of the distribution via simulation unless your sample size is massive. In these examples, the simulation approach requires tens of millions of simulations to get close to the analyical standard deviation of the estimator.\nI think this is an incredibly interesting aside that I haven’t seen covered in the literature so far.\nThis is relevant because some papers have suggested choosing optimal p to be \\(p \\lesssim \\alpha/2\\) (e.g. right on the boundary of finite variance).\nThe variance of the FLOM is monotonically increasing in p, and some have suggested making p as small as possible. The difficulty is that \\(\\lim_{p \\rightarrow 0} C(p, \\alpha) = \\infty\\). This problem doesn’t exist in the correlation estimator case.\n\n\nVariance of the Correlation Estimators\n\n\n\n\n\n\nImportant\n\n\n\nNote: The correlation-type estimators are being abandoned since they perform far worse than original Median Oracle despite some nice theoretical properties.\n\n\nWhen our estimator is a correlation-type estimator like scov and r above, any sample estimate is bounded on \\([-1,1]\\) and all moments of the estimator itself are finite for any p – even \\(p&gt;\\alpha\\)!\nLike with raw FLOMs, Monte Carlo has been used to estimate variance of these estimators in some papers, but only with relatively small simulation sizes and without comment on potential errors in the calculation of an optimal p.\nSince we can induce finite variance on the components of these estimators, it seems reasonable to invoke the delta method to approximate their asymptotic variance using Taylor series expansions to get a better estimate of a variance-minimizing choice of p.\nMy derivation of the correlation-type estimator variance approximation can be found here.\nUnfortunately, this variance approximation doesn’t work very nicely in practice for moderate-sized simulations:\n\nAnd worse, plugging it into the current Median Oracle algorithm in place of sample covariance results in worse estimates. I think this is happening because throwing away the magnitude information for a correlation-type estimator results in worse estimates. The good news is that an estimator based on the numerator of r should fix this problem. The bad news is that it reintroduces infinite estimator variance."
  },
  {
    "objectID": "corr_variance_derivation.html",
    "href": "corr_variance_derivation.html",
    "title": "Deriving Correlation Estimator Variance",
    "section": "",
    "text": "Starting with couple preliminaries that will be useful to derive an analyticaly tractable approximation for the variance of these correlation estimators, \\(\\hat r\\) and \\(\\hat{ \\text{scov}}\\) based on the sample FLOM estimators.\n\n\nFirst, deriving the expected value for W \\(\\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers. We can use the fact that we can decompose a \\(\\alpha\\)-SG(\\(\\sigma^2\\)) random variable into the product of the square root of W and normal random variable \\(G \\sim N(0,\\sigma^2)\\) independent of W:\n\\[\n\\begin{aligned}\n\\mathbb E(|X|^p) &= \\mathbb E(|W^{1/2}G|^p) \\\\\n&= \\mathbb E(W^{p/2}|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\mathbb E(|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\sigma^p 2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n&= \\mathbb E(W^{p/2}) (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n\\\\ \\text{So we get:} \\\\\n\\mathbb E(W^{p/2}) &= \\frac{\\mathbb E(|X|^p)}{ (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}\\\\\n&=  2^{-p}\\gamma^{-p}\\frac{\\sqrt \\pi}{\\Gamma(\\frac{1+p}{2})}\\times{\\frac{\\gamma^p 2^{p+1}\\Gamma(\\frac{p+1}{2})\\Gamma(-p/\\alpha)}{\\alpha \\sqrt \\pi \\Gamma(-p/2)}} \\\\\n&= \\frac{2\\Gamma(-p/\\alpha)}{\\alpha\\Gamma(-p/2)}, & p&lt;\\alpha/2\n\\end{aligned}\n\\]\nThis uses the well-known results for normal absolute moments, stable FLOMs, and a change of variables from the standard deviation of G to the scale parameter \\(\\gamma\\) of X.\n\n\n\nWhen \\(p&lt;\\alpha/2\\), \\(\\text{Cov}(|X_1|^p, |X_2|^p)\\) is finite and can be written in terms of the hypergeometric function for \\(-1&lt;p&lt;\\alpha/2\\):\n\\[\n\\begin{aligned}\n\\text{Cov}(|X_1|^p, |X_2|^p) &= \\mathbb E(|X_1|^p |X_2|^p) - \\mathbb E (|X_1|^p) \\mathbb E (|X_2|^p)\\\\\n&= \\mathbb E(|X_1 X_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\mathbb E(|\\sqrt W G_1 \\sqrt W G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p) \\mathbb E(|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^p \\sigma_1^p \\sigma_2^p}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^{p+1} \\gamma_1^{p/\\alpha} \\gamma_2^{p/\\alpha}}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\n\\end{aligned}\n\\]\nTherefore, letting \\(G_3 = G_1-G_2 \\implies X_3 = X_1-X_2\\), with \\(G_i\\) a mean-zero normal random variable correlated with the other two:\n\\[\n\\begin{aligned}\n\\text{Cov}(G_3, G_1) &= \\text{Cov}(G_1 - G_2, G_1)  \\\\\n&= \\text{Cov}(G_1, G_1)-\\text{Cov}(G_2, G_1)\\\\\n&= \\text{Var}(G_1)-\\text{Cov}(G_2, G_1) \\\\\n&= \\sigma_1^2 - \\rho \\sigma_1\\sigma_2 \\\\\n\\text{Thus, we have:}\\\\\n\\text{Cov}(G_3, G_2) &= \\text{Cov}(G_1 - G_2, G_2) \\\\\n&= \\rho \\sigma_1\\sigma_2 - \\sigma_2^2 \\\\\n\\text{Var}(G_3) &= \\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\n\\end{aligned}\n\\]\nThis implies that the correlation coefficients for \\(G_3\\) are:\n\\[\n\\begin{aligned}\n\\rho_{3,1} \\equiv \\frac{\\sigma_1^2 - \\rho \\sigma_1\\sigma_2}{\\sigma_1 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}} \\\\\n\\rho_{3,2} \\equiv \\frac{\\rho \\sigma_1\\sigma_2 - \\sigma_2^2 }{\\sigma_2 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}}\n\\end{aligned}\n\\]\nThis gives us \\(\\text{Cov}(|X_1-X_2|^p, |X_i|^p), i=1,2\\) for stable random variables \\(X_i\\) by using \\(\\rho_{3,i}\\) as the final parameter in the hypergeometric function above.\nWe can use all of this to get an approximation for the variance of our estimators when \\(p&lt;\\alpha/2\\)."
  },
  {
    "objectID": "corr_variance_derivation.html#preliminaries",
    "href": "corr_variance_derivation.html#preliminaries",
    "title": "Deriving Correlation Estimator Variance",
    "section": "",
    "text": "Starting with couple preliminaries that will be useful to derive an analyticaly tractable approximation for the variance of these correlation estimators, \\(\\hat r\\) and \\(\\hat{ \\text{scov}}\\) based on the sample FLOM estimators.\n\n\nFirst, deriving the expected value for W \\(\\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers. We can use the fact that we can decompose a \\(\\alpha\\)-SG(\\(\\sigma^2\\)) random variable into the product of the square root of W and normal random variable \\(G \\sim N(0,\\sigma^2)\\) independent of W:\n\\[\n\\begin{aligned}\n\\mathbb E(|X|^p) &= \\mathbb E(|W^{1/2}G|^p) \\\\\n&= \\mathbb E(W^{p/2}|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\mathbb E(|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\sigma^p 2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n&= \\mathbb E(W^{p/2}) (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n\\\\ \\text{So we get:} \\\\\n\\mathbb E(W^{p/2}) &= \\frac{\\mathbb E(|X|^p)}{ (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}\\\\\n&=  2^{-p}\\gamma^{-p}\\frac{\\sqrt \\pi}{\\Gamma(\\frac{1+p}{2})}\\times{\\frac{\\gamma^p 2^{p+1}\\Gamma(\\frac{p+1}{2})\\Gamma(-p/\\alpha)}{\\alpha \\sqrt \\pi \\Gamma(-p/2)}} \\\\\n&= \\frac{2\\Gamma(-p/\\alpha)}{\\alpha\\Gamma(-p/2)}, & p&lt;\\alpha/2\n\\end{aligned}\n\\]\nThis uses the well-known results for normal absolute moments, stable FLOMs, and a change of variables from the standard deviation of G to the scale parameter \\(\\gamma\\) of X.\n\n\n\nWhen \\(p&lt;\\alpha/2\\), \\(\\text{Cov}(|X_1|^p, |X_2|^p)\\) is finite and can be written in terms of the hypergeometric function for \\(-1&lt;p&lt;\\alpha/2\\):\n\\[\n\\begin{aligned}\n\\text{Cov}(|X_1|^p, |X_2|^p) &= \\mathbb E(|X_1|^p |X_2|^p) - \\mathbb E (|X_1|^p) \\mathbb E (|X_2|^p)\\\\\n&= \\mathbb E(|X_1 X_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\mathbb E(|\\sqrt W G_1 \\sqrt W G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p) \\mathbb E(|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^p \\sigma_1^p \\sigma_2^p}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^{p+1} \\gamma_1^{p/\\alpha} \\gamma_2^{p/\\alpha}}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\n\\end{aligned}\n\\]\nTherefore, letting \\(G_3 = G_1-G_2 \\implies X_3 = X_1-X_2\\), with \\(G_i\\) a mean-zero normal random variable correlated with the other two:\n\\[\n\\begin{aligned}\n\\text{Cov}(G_3, G_1) &= \\text{Cov}(G_1 - G_2, G_1)  \\\\\n&= \\text{Cov}(G_1, G_1)-\\text{Cov}(G_2, G_1)\\\\\n&= \\text{Var}(G_1)-\\text{Cov}(G_2, G_1) \\\\\n&= \\sigma_1^2 - \\rho \\sigma_1\\sigma_2 \\\\\n\\text{Thus, we have:}\\\\\n\\text{Cov}(G_3, G_2) &= \\text{Cov}(G_1 - G_2, G_2) \\\\\n&= \\rho \\sigma_1\\sigma_2 - \\sigma_2^2 \\\\\n\\text{Var}(G_3) &= \\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\n\\end{aligned}\n\\]\nThis implies that the correlation coefficients for \\(G_3\\) are:\n\\[\n\\begin{aligned}\n\\rho_{3,1} \\equiv \\frac{\\sigma_1^2 - \\rho \\sigma_1\\sigma_2}{\\sigma_1 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}} \\\\\n\\rho_{3,2} \\equiv \\frac{\\rho \\sigma_1\\sigma_2 - \\sigma_2^2 }{\\sigma_2 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}}\n\\end{aligned}\n\\]\nThis gives us \\(\\text{Cov}(|X_1-X_2|^p, |X_i|^p), i=1,2\\) for stable random variables \\(X_i\\) by using \\(\\rho_{3,i}\\) as the final parameter in the hypergeometric function above.\nWe can use all of this to get an approximation for the variance of our estimators when \\(p&lt;\\alpha/2\\)."
  },
  {
    "objectID": "corr_variance_derivation.html#estimator-variance-for-r",
    "href": "corr_variance_derivation.html#estimator-variance-for-r",
    "title": "Deriving Correlation Estimator Variance",
    "section": "Estimator Variance for r",
    "text": "Estimator Variance for r\n\n\n\n\n\n\nImportant\n\n\n\nThis estimator below has been abandoned, but the preliminaries above are still important so I’m keeping this page active.\n\n\nSince \\(\\hat r = \\frac{(\\sum_{i=1}^N |X_{1i}|^p)^{2/p}+(\\sum_{i=1}^N |X_{2i}|^p)^{2/p}-(\\sum_{i=1}^N |X_{1i}- X_{2i}|^p)^{2/p}}{2(\\sum_{i=1}^N |X_{1i}|^p \\sum_{i=1}^N |X_{2i}|^p)^{1/p}}\\), we can set \\(Y_{ji} = |X_{ji}|^p, Z_j = \\sum_{i=1}^N Y_{ji}\\) and replace this with:\n\\[\n\\begin{aligned}\n\\hat r &= \\frac{(\\sum_{i=1}^N Y_{1i})^{2/p}+(\\sum_{i=1}^N Y_{2i})^{2/p}-(\\sum_{i=1}^N Y_{3i})^{2/p}}{2(\\sum_{i=1}^N Y_{1i} \\sum_{i=1}^N Y_{2i})^{1/p}} \\\\\n& = \\frac{Z_1^{2/p}+Z_2^{2/p}-Z_3^{2/p}}{2(Z_1Z_2)^{1/p}} \\\\\n\\text{We can define the epectation as:} \\\\\n\\zeta_i \\equiv \\mathbb EZ_i = N C(\\alpha, p) \\gamma_i^{p/\\alpha}\n\\end{aligned}\n\\]\nThe second-order Taylor expansion of \\(\\mathbb E [\\hat r]\\) around \\((\\zeta_1, \\zeta_2, \\zeta_3)\\) is:\n\\[\n\\begin{aligned}\n\\mathbb E [\\hat r] \\equiv \\mathbb E [f(Z_1,Z_2,Z_3)] &\\approx \\mathbb E \\bigg\\{f(\\zeta_1, \\zeta_2, \\zeta_3) + \\sum_{i=1}^3 \\frac{\\partial f}{\\partial Z_i}\\bigg |_{(\\zeta_1, \\zeta_2, \\zeta_3)} (Z_i - \\zeta_i) + \\frac 1 2 \\sum_{i=1}^3\\sum_{j=1}^3 \\frac{\\partial^2 f}{\\partial Z_i\\partial Z_j}\\bigg |_{(\\zeta_1, \\zeta_2, \\zeta_3)} (Z_i - \\zeta_i)(Z_j - \\zeta_j)\\bigg\\} \\\\\n&=\\frac{\\zeta_1^{2/p}+\\zeta_2^{2/p}-\\zeta_3^{2/p}}{2(\\zeta_1 \\zeta_2)^{1/p}} +\\frac 1 2 \\bigg(\\frac{\\partial^2 f}{\\partial Z_1^2}\\text{Var}(Z_1)+\\frac{\\partial^2 f}{\\partial Z_2^2}\\text{Var}(Z_2)+\\frac{\\partial^2 f}{\\partial Z_3^2}\\text{Var}(Z_3)+ \\\\ & (\\frac{\\partial^2 f}{\\partial Z_1\\partial Z_2} + \\frac{\\partial^2 f}{\\partial Z_2\\partial Z_1})\\text{Cov}(Z_1, Z_2)+(\\frac{\\partial^2 f}{\\partial Z_1\\partial Z_3} + \\frac{\\partial^2 f}{\\partial Z_3\\partial Z_1})\\text{Cov}(Z_1, Z_3)+ \\\\ &(\\frac{\\partial^2 f}{\\partial Z_2\\partial Z_3} + \\frac{\\partial^2 f}{\\partial Z_3\\partial Z_2})\\text{Cov}(Z_2, Z_3)\\bigg) \\\\\n& =\n\\end{aligned}\n\\]\nA quick and dirty evaluation of all higher-order terms shows both the moments and the derivatives go to zero as \\(N \\rightarrow \\infty\\), so this estimator should quickly become unbiased as the sample size increases.\nUsing the first-order Taylor expansion (to keep things less crazy) for the variance, and the fact that for large N, \\(\\mathbb E[\\hat r] \\approx f(\\zeta_1, \\zeta_2, \\zeta_3)\\):\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat r)& \\approx \\mathbb E\\bigg[\\bigg(f(Z_1,Z_2,Z_3)-f(\\zeta_1, \\zeta_2, \\zeta_3) \\bigg)^2\\bigg]\n\\\\ &\n= \\mathbb E\\bigg[\\bigg(f(Z_1,Z_2,Z_3)-f(\\zeta_1, \\zeta_2, \\zeta_3) + \\frac{\\partial f}{\\partial Z_1}(Z_1 - \\zeta_1) + \\frac{\\partial f}{\\partial Z_2}(Z_2 - \\zeta_2) +\n\\frac{\\partial f}{\\partial Z_3}(Z_3 - \\zeta_3) - f(Z_1,Z_2,Z_3)\n\\bigg)^2\\bigg] \\\\\n&= \\mathbb E \\bigg[\n    \\bigg(\\frac{\\partial f}{\\partial Z_1}\\bigg)^2(Z_1 - \\zeta_1)^2 +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg)^2(Z_2 - \\zeta_2)^2 +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg)^2(Z_2 - \\zeta_2)^2 +\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_3}(Z_3 - \\zeta_3)(Z_1 - \\zeta_1) + \\\\ &\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_2}(Z_2 - \\zeta_2)(Z_1 - \\zeta_1) +  2\\frac{\\partial f}{\\partial Z_2}\\frac{\\partial f}{\\partial Z_3}(Z_3 - \\zeta_3)(Z_2 - \\zeta_2)\n    \\bigg] \\\\\n    &= \\bigg(\\frac{\\partial f}{\\partial Z_1}\\bigg)^2\\text{Var}(Z_1) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg)^2\\text{Var}(Z_2) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_3}\\bigg)^2\\text{Var}(Z_3) +\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_2}\\text{Cov}(Z_1, Z_2) + \\\\ &\n     2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_3}\\text{Cov}(Z_1, Z_3) +  2\\frac{\\partial f}{\\partial Z_2}\\frac{\\partial f}{\\partial Z_3}\\text{Cov}(Z_2, Z_3) \\\\\n&= \\frac{(\\zeta_1^{2/p}-\\zeta_2^{2/p}+\\zeta_3^{2/p})^2}{4p^2\\zeta_1^2(\\zeta_1 \\zeta_2)^{2/p}}\\text{Var}(Z_1)+\\frac{(\\zeta_2^{2/p}-\\zeta_1^{2/p}+\\zeta_3^{2/p})^2}{4p^2\\zeta_2^2(\\zeta_1 \\zeta_2)^{2/p}}\\text{Var}(Z_2)+\\frac{\\zeta_3^{\\frac 4p -2}}{p^2(\\zeta_1 \\zeta_2)^{2/p}}\\text{Var}(Z_3)+\\\\ &\n2\\frac{(\\zeta_3^{4/p}+(\\zeta_1^{2/p}-\\zeta_2^{2/p})^2)}{4p^2(\\zeta_1\\zeta_2)^{\\frac{2+p}{p}}}\\text{Cov}(Z_1, Z_2)-\\dots\n\\end{aligned}\n\\]\nInitially, I started writing out this expansion to see if I could get any of the known terms to cancel or find a pattern that could lead to an exact solutionm, but unfortunately neither case seems to hold. I calculated this variance approximation numerically, and compared it to simulations with less than compelling results."
  },
  {
    "objectID": "progress.html#the-subordinated-covariance-estimator",
    "href": "progress.html#the-subordinated-covariance-estimator",
    "title": "Current Progress",
    "section": "The Subordinated Covariance Estimator",
    "text": "The Subordinated Covariance Estimator\nIf we just take the rescaled numerator of r, we get the following:\n\\[\n\\begin{aligned}\n&(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p} \\\\\n&= (C(p,\\alpha)\\gamma_1^p)^{2/p}+(C(p,\\alpha)\\gamma_2^p)^{2/p}-(C(p,\\alpha)\\gamma_3^p)^{2/p}\n\\\\\n& = C(p,\\alpha)^{2/p}\\bigg[\\gamma_1^2 + \\gamma_2^2 -\\gamma_3^2 \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -\\frac{\\sigma_3^2}{2}  \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -(\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2}-\\frac{2\\rho_{12}\\sigma_1\\sigma_2}{2}) \\bigg] \\\\\n&= C(p,\\alpha)^{2/p}\\rho_{12}\\sigma_1\\sigma_2\n\\end{aligned}\n\\]\nThe subordinated covariance estimator is defined as \\(\\hat s\\) below:\n\\[\n\\begin{aligned}\n\\hat s &= \\frac{(\\frac{1}{N}\\sum_{i=1}^N |X_{1i}|^p)^{2/p} + (\\frac{1}{N}\\sum_{i=1}^N |X_{2i}|^p)^{2/p} -(\\frac{1}{N}\\sum_{i=1}^N |X_{1i} - X_{2i}|^p)^{2/p}}{C(p,\\alpha)^{2/p}}\n\\end{aligned}\n\\]\nOne immediate benefit of this estimator is that, when \\(p=2\\), this estimator is exactly the sample covariance (which is an input into the Median Oracle estimator). This provides a very reasonable path to improving the performance of Median Oracle - when \\(\\alpha&lt;2\\), the sample covariance is a biased estimator of the subordinated covariance, and \\(p=2\\) may not be optimial anymore.\nThe problem, though, is that this formulation likely won’t have finite moments due to the elements being raised to the second power.\nI’m continuing to explore potential estimators of this type.\n[Last Updated on July 24, 2024]"
  },
  {
    "objectID": "sce_derivation.html",
    "href": "sce_derivation.html",
    "title": "Subordinated Covariance Estimator Derivation",
    "section": "",
    "text": "We can use the same preliminaries and random variable setup from this page to get approximations for the expected value and variance of what I’m calling the Subordinated Covariance Estimator, \\(\\hat s\\).\nFirst, it’s not immediately obvious that the moments of this estimator are finite. Luckily, we can see that if \\(p &lt; \\alpha &lt;2\\) for the non-Gaussian case, or \\(p \\leq 2\\) for the Gaussian \\(\\alpha =2\\) case, since \\(f(x) = x^\\frac{2}{p}\\) is concave when \\(\\frac 2 p &gt; 1\\)\nUsing a second-order Taylor expansion around the means for the expectation, and setting \\(Y_{ji} = |X_{ji}|^p, Z_j = \\sum_{i=1}^N Y_{ji}\\), we get “simplified” univariate expansions compared to the correlation-type estimators that approximate the covariance of the subordinated Gaussian vector:\n\\[\n\\begin{aligned}\n\\mathbb E(\\hat s) &= \\mathbb E \\frac{(\\frac{1}{N}\\sum_{i=1}^N |X_{1i}|^p)^{2/p} + (\\frac{1}{N}\\sum_{i=1}^N |X_{2i}|^p)^{2/p} -(\\frac{1}{N}\\sum_{i=1}^N |X_{1i} - X_{2i}|^p)^{2/p}}{C(p,\\alpha)^{2/p}} \\\\\n&= N^{-2/p}C(p,\\alpha)^{-2/p}\\mathbb E\\bigg[(\\sum_{i=1}^N |X_{1i}|^p)^{2/p}\\bigg]+ \\mathbb E\\bigg[(\\sum_{i=1}^N |X_{2i}|^p)^{2/p}\\bigg]-\\mathbb E\\bigg[(\\sum_{i=1}^N |X_{1i} - X_{1i}|^p)^{2/p}\\bigg] \\\\\n&\\approx N^{-2/p}C(p,\\alpha)^{-2/p} \\bigg[ \\zeta_1^{2/p}+\\zeta_2^{2/p}-\\zeta_3^{2/p}-\n    \\sum_{i=1}^3\\frac{(p-2)\\zeta_i^{\\frac{2}{p}-2}}{p^2}\\text{Var}(Z_i)\n\\bigg]\\\\\n&= N^{-2/p}C(p,\\alpha)^{-2/p} \\bigg[ \\{NC(p,\\alpha)\\}^{2/p}(\\gamma_1^2+\\gamma_2^2-\\gamma_3^2)- \\\\\n&\\frac{(p-2)\\{NC(p,\\alpha)\\}^{\\frac{2}{p}-2}}{p^2} \\frac{2^{2p+2}}{N \\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\} \\bigg(\\gamma_1^{2p+\\frac{2}{p}-2} + \\gamma_2^{2p+\\frac{2}{p}-2} - \\gamma_3^{2p+\\frac{2}{p}-2}\\bigg)\\bigg] \\\\\n&= (\\gamma_1^2+\\gamma_2^2-\\gamma_3^2) - \\frac{(p-2)}{N^3\\{pC(p,\\alpha)\\}^{2}} \\frac{2^{2p+2}}{\\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\} \\\\ &\\times\\bigg(\\gamma_1^{2p+\\frac{2}{p}-2} + \\gamma_2^{2p+\\frac{2}{p}-2} - \\gamma_3^{2p+\\frac{2}{p}-2}\\bigg) \\\\\n& = \\text{Cov}(G_1,G_2) -\n\\frac{(p-2)}{N^3\\{pC(p,\\alpha)\\}^{2}} \\frac{2^{2p+2}}{\\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\} \\\\ &\\times\\bigg(\\gamma_1^{2p+\\frac{2}{p}-2} + \\gamma_2^{2p+\\frac{2}{p}-2} - \\gamma_3^{2p+\\frac{2}{p}-2}\\bigg)\n\\end{aligned}\n\\]\nThe result above uses the expansion of the rescaled numerator of r from the progress page to move from the stable scale parameters to the covariance of the Gaussian vector.\nMoving on to the variance of \\(\\hat s\\), with \\(f(Z_1, Z_1, Z_1) = (C(p, \\alpha)N)^{-2/p}[ Z_1^{2/p}+Z_2^{2/p}-Z_3^{2/p}]\\):\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat s) &\\approx  \n\\bigg(\\frac{\\partial f}{\\partial Z_1}\\bigg|_{\\zeta}\\bigg)^2\\text{Var}(Z_1) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg|_{\\zeta}\\bigg)^2\\text{Var}(Z_2) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_3}\\bigg|_{\\zeta}\\bigg)^2\\text{Var}(Z_3) +\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_2}\\bigg|_{\\zeta}\\text{Cov}(Z_1, Z_2) + \\\\ &\n     2\\frac{\\partial f}{\\partial Z_1}\\bigg|_{\\zeta}\\frac{\\partial f}{\\partial Z_3}\\bigg|_{\\zeta}\\text{Cov}(Z_1, Z_3) +  2\\frac{\\partial f}{\\partial Z_2}\\frac{\\partial f}{\\partial Z_3}\\bigg|_{\\zeta}\\text{Cov}(Z_2, Z_3) \\\\\n&= C(p, \\alpha)^{-2}\\bigg[\\frac{4}{p^2} \\bigg\\{\\gamma_1^{(4p-2)p}\\text{Var}(Z_1)+\\gamma_2^{(4p-2)p}\\text{Var}(Z_2)+\\gamma_3^{(4p-2)p}\\text{Var}(Z_3)\\bigg\\}+\\\\ &\\frac{8}{p^2}\\bigg\\{(\\gamma_1^p)^{2/p-1}(\\gamma_2^p)^{2/p-1}\\text{Cov}(Z_1, Z_2)-(\\gamma_1^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(Z_1, Z_3)-(\\gamma_2^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(Z_2, Z_3)\\bigg\\}\n\\bigg]\\\\\n&= N^{-1}C(p, \\alpha)^{-2}\\bigg[\\frac{4}{p^2} \\bigg\\{\\gamma_1^{(4p-2)p}\\text{Var}(|X_1|^p)+\\gamma_2^{(4p-2)p}\\text{Var}(|X_2|^p)+\\gamma_3^{(4p-2)p}\\text{Var}(|X_3|^p)\\bigg\\}+\\\\\n&\\frac{8}{p^2}\\bigg\\{(\\gamma_1^p)^{2/p-1}(\\gamma_2^p)^{2/p-1}\\text{Cov}(|X_1|^p, |X_2|^p)-(\\gamma_1^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(|X_1|^p, |X_3|^p)-\\\\ &(\\gamma_2^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(|X_2|^p, |X_3|^p)\\bigg\\}\n\\bigg]\\\\\n\\end{aligned}\n\\]\nThere is some additional simplification that happens with the variance terms (they’re all identical except for a \\(\\gamma_i^{2p}\\)), but the expression becomes very complicated very quickly, and doesn’t have any obvious simplifications or tricks to make it more interpretable (unlike the expected value). At this point may as well be calculated numerically."
  },
  {
    "objectID": "index.html#research-plan",
    "href": "index.html#research-plan",
    "title": "Research Progress Site",
    "section": "Research Plan",
    "text": "Research Plan\n\nFind optimal estimator for underlying dependence\nInvestigate potential benefits of a median-of-means approach for heavy tails and small observations\nInvestigate high-dimensionality adjustments\nApplications"
  },
  {
    "objectID": "progress.html#the-subordinated-covariance-estimators",
    "href": "progress.html#the-subordinated-covariance-estimators",
    "title": "Current Progress",
    "section": "The Subordinated Covariance Estimators",
    "text": "The Subordinated Covariance Estimators\n\n\n\n\n\n\nImportant\n\n\n\nNote: This estimator is also being put on ice due to infinite moments. I’m continuing to investigate more estimators of this type to see if I can get something that reverts to sample covariance as a special case when the data are Gaussian, but has better moment properties. Alternatives to estimator variance might also be helpful.\n\n\nIf we just take the rescaled numerator of r, we get the following:\n\\[\n\\begin{aligned}\n&(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p} \\\\\n&= (C(p,\\alpha)\\gamma_1^p)^{2/p}+(C(p,\\alpha)\\gamma_2^p)^{2/p}-(C(p,\\alpha)\\gamma_3^p)^{2/p}\n\\\\\n& = C(p,\\alpha)^{2/p}\\bigg[\\gamma_1^2 + \\gamma_2^2 -\\gamma_3^2 \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -\\frac{\\sigma_3^2}{2}  \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -(\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2}-\\frac{2\\rho_{12}\\sigma_1\\sigma_2}{2}) \\bigg] \\\\\n&= C(p,\\alpha)^{2/p}\\rho_{12}\\sigma_1\\sigma_2\n\\end{aligned}\n\\]\nThe subordinated covariance estimator is defined as \\(\\hat s\\) below:\n\\[\n\\begin{aligned}\n\\hat s &= \\frac{(\\frac{1}{N}\\sum_{i=1}^N |X_{1i}|^p)^{2/p} + (\\frac{1}{N}\\sum_{i=1}^N |X_{2i}|^p)^{2/p} -(\\frac{1}{N}\\sum_{i=1}^N |X_{1i} - X_{2i}|^p)^{2/p}}{C(p,\\alpha)^{2/p}}\n\\end{aligned}\n\\]\nOne immediate benefit of this estimator is that, when \\(p=2\\), this estimator is exactly the sample covariance (which is an input into the Median Oracle estimator). This provides a very reasonable path to improving the performance of Median Oracle - when \\(\\alpha&lt;2\\), the sample covariance is a biased estimator of the subordinated covariance, and \\(p=2\\) may not be optimial anymore.\nThe problem, though, is that this formulation cannot have finite moments (this follows from Jensen’s inequality; we need \\(p&gt;2\\) to be finite for finite moments, which is a contradiction.)\nI’m continuing to explore potential estimators of this type."
  },
  {
    "objectID": "why_stable.html",
    "href": "why_stable.html",
    "title": "Why Stable?",
    "section": "",
    "text": "Note\n\n\n\nThis page tries to explain the benefits of a stable model conversationally. I think a more formal, example-filled, adequately sourced version of this discussion would be useful early in my dissertation. But hopefully, this version is helpful for understanding my desire to use the stable model!\nThe question has come up – why use an alpha-stable distribution to model investment returns in the first place?\nIt’s worth noting that my goal isn’t necessarily to build an estimator that only works on stable distributions, but rather to build one that doesn’t immediately break if the distribution of the data is heavy tailed. Working with the stable distribution gives us something to anchor to for theoretical results.\nEven when variance exists, my conjecture is that in the presence of heavy tails, a model that can accomodate infinite variance probably does better at measuring dependence in finite samples.\n(Or to paraphrase a comment I read on the internet: in the real world, a slowly converging integral is no better than one that doesn’t converge at all.)"
  },
  {
    "objectID": "why_stable.html#why-stable",
    "href": "why_stable.html#why-stable",
    "title": "Why Stable?",
    "section": "Why Stable?",
    "text": "Why Stable?\nStable distributions generalize the normal distribution, which has typically been used commonly in finance, allowing for heavier tails – and shocks or jumps.\nArguably, for any stochastic additive process with countably infinite increments (e.g. like log-returns), if “sufficiently large” summands aren’t suitably well-behaved, we have an argument for using a stable distribution. More concretely, when the central limit theorem provides a justification for using the normal distribution throughout mathematical finance, when the variance of those increments is infinite or the tails of the increments are large enough that their sums have slow convergence to the Gaussian regime, a stable distribution makes more sense.\nBy the Generalized CLT of Gnedenko and Kolmogorov (1954), the sums of any i.i.d. random variables must be alpha-stable in distribution.\n\n\n\nFrom Meucci’s “Risk and Asset Allocation”, stable distributions in the taxonomy of distributions; N denotes the normal.\n\n\nFinancial returns have been observed to be strongly non-Gaussian for nearly a century, and in the early 1960s, the stable distribution was suggested as a model for financial returns by Benoit Mandlebrot. Around that same time, significant research was done in extending Gaussian assumptions in asset pricing models to the more realistic stable distribution by future Nobel laureate Eugene Fama. More recently, books like Stable Paretian Models in Finance have extended more contemporary financial models to being driven by stable distributions.\n\nWhy Aren’t Stable Models More Common in Finance?\nDespite significant theory and justification for using stable distributions, they’re not commonly used in practice. One reason for that is that they’re less accessible to non-mathematicians than simpler Gaussian models, while also implying less explainability than more sophisticated models that only work in the rearview mirror (e.g. ex post, there is more noise).\nI’d speculate that another reason has to do with timing. When Harry Markowitz introduced Modern Portfolio Theory in the 1950s, he brought the idea that variance should be used as a measure of investment risk, and presented a framework for portfolio contruction that implied Gaussian returns - the mean-variance approach pioneered by Markowitz at this time only guarantees optimality if the distribution of returns is fully parameterized by its mean and variance, which forms a complete sufficient statistic for the Gaussian. This made the normal distribution a more appealing choice during this time.\nBy the time enough computational power was available to deal with stable distributions (and, for instance, calculate their PDFs and estimate parameters using MLE), that computational power could be used instead for nonparametric/empirical approaches, and many practitioners burned by failures of the Gaussian model were turned off by parameteric approaches. In some sense, stable distributions missed the boat despite arguably being a better model than either (empirical distributions in particular are problematic because of the “unseen” tail that’s impossible to estimate for events that haven’t happened yet).\nThe idea of using standard deviation to define investment volatility (and risk) has likely been one of the biggest challenges to seeing stable models get used more widely. Some have argued that infinite variance implies infinite risk, which is not correct.\nIn fact, stable distributions have well-defined scale parameters that behave just like standard deviation in the Gaussian case – as I’ll go into below, they are exactly related to standard deviation in the \\(\\alpha\\)-SG(\\(\\Sigma\\)) case.\nThese scale parameters have direct applications to portfolio optimization when simple returns are stable, and more importantly, they describe the decreasing marginal benefit to diversification that we see in the real world when returns come from heavy-tailed distributions (See Nolan, 2020, p. 35)."
  },
  {
    "objectID": "why_stable.html#why-sub-gaussian",
    "href": "why_stable.html#why-sub-gaussian",
    "title": "Why Stable?",
    "section": "Why Sub-Gaussian",
    "text": "Why Sub-Gaussian\nWhen thinking about stable distributions as a generalization of the Gaussian, the \\(\\alpha\\)-SG(\\(\\Sigma\\)) class of stable distributions can be thought of as a minimal relaxation: they share all the dependence structure of a Gaussian, but have the ability to present heavier tails and jumps.\nStable distrbutions lend themselves well to portfolio construction when they model simple returns; the dispersion matrix \\(\\Sigma\\) has been shown to be a coherent risk metric that can be used to build a portfolio when an agent is averse to risk. (See Kring, et al., 2009)\nWhile more complicated dependence structures are possible with a stable distribution by lifting the symmetry assumption, I think the tradeoff lies in the ability to estimate those parameters well enough as well as the futility of trying to accomodate those dependence structures in a portfolio whose returns are an affine transformation of the underlying return vector. In other words, the juice isn’t worth the squeeze for a more complex model."
  },
  {
    "objectID": "why_stable.html#why-infinite-variance",
    "href": "why_stable.html#why-infinite-variance",
    "title": "Why Stable?",
    "section": "Why Infinite Variance",
    "text": "Why Infinite Variance\nInfinite variance itself isn’t necessarily a desirable attribute for our model distribution. Rather, it is a side effect of having a target distribution that can accomodate realistic jumps/shocks/outliers.\nWhile it’s true that with a stable distribution we have positive probability of seeing an arbitrarily large realization, the same is true of the normal distribution too!\nSo, this question is less “why infinite variance?” and more “why not infinite variance?”\nUltimately, infinite variance says more about a distribution’s speed of tail decay than it does about the likelihood of an extreme event in general. I’ll attempt to address some possible objections to infinite variance models below:\nInfinite variance does not say anything about the variability of a distribution\nWhile variance can be a useful metric to compare the variability of distributions, once it’s infinite, it’s no longer meaningful to compare.\nAs an example, take the Cauchy distribution below, a symmetric distribution which has infinite integer moments:\n\nCompare this to a \\(N(10^8,1)\\) distribution, and taking random deviates from each, it’s clear that the Cauchy’s central tendency or typical realizations are not “higher” in any sense. It’s is simply no longer a meaningful quantity (no pun intended) because the integral fails to converge.\nSimilarly, the stable random variable \\(X \\sim S_{1.9}(0.01,0,0)\\) with 99.9985% of its probability density on [-1,1] has infinite variance. When the second moment doesn’t converge, that’s really only a commentary on the speed of tail decay relative to the expectation being integrated.\nInfinite variance models are a realistic choice for investment returns.\nEmpirically, investment returns are heteroskedastic. This has led to many more complicated models to explain the behavior of prices (regime models, stochastic volatility, etc.).\nBut looking at even a very simple Lèvy flight driven by a stable distribution, the empirical rolling standard deviation is quite well modeled by stable returns:\n\n\n\nFrom my Stable Bands paper (2023); Rolling 20-day standard deviation of S&P 500 returns in red vs. the same from an infinite variance stable distribution fit to S&P returns in blue.\n\n\nFinite variance does not solve the practical challenges of heavy tails\nWhen we allow the second moment to exist in a heavy-tailed distribution, we do regain some nice theoretical properties but, in practice, results will largely be indistinguishable from the infinite variance regime.\nFor example even if variance is finite, convergence of the integral (and by extension a sample-based estimate of it) may be so slow that finite-sample estimates are effectively stochastic. This is almost more problematic because it creates a tendency to be overconfident in higher-moment estimates.\nWith the stable model, the \\(\\alpha\\) parameter can be treated like a confidence metric for moment estimates; for instance, \\(\\alpha\\) near 2 suggests that the mean will likely be well behaved and reliably estimated in small samples.\nInfinite variance does not imply infinite or undefined variability.\nWhile variance may be undefined, other measures of variability, such as mean absolute deviation will exist when \\(\\alpha&gt;1\\). Likewise, median absolute deviation will always exist. Arguably, even if variance did exist as well, one of these more robust measures of variability may be a more appropriate/interpretable quantity in a heavy-tailed application like modeling financial returns.\nLikewise, while theoretical variance may be undefined for a stable distribution, sample variance from a realization of that stable distribution will always exist and controlling it is a common investment objective in portfolio construction.\nIn that case, it’s straightforward that the minimum stable scale portfolio will also be the portfolio with the smallest expected median realized standard deviation. Similar extensions can be made for other objectives.\nMoreover, by appreciating the fact that target metrics may not converge in expectation, it’s possible to treat them more realistically - e.g. by building a portfolio that has a 90% probability of not exceeding an undesirable sample standard deviation.\nInfinite variance does not mean that tail events are more likely.\nIt’s true that under some regularity conditions (e.g. unimodal and monotonically decreasing tails) a stable random variable will have higher variability (however you choose to measure it) than, say, a normal random variable with the same scale parameter. But that setup artificially handicaps the stable random variable.\nWith some of the variability “handled” by a lower \\(\\alpha\\), the stable random variable fit to the same data as a Gaussian will have a lower scale parameter.\nOne way to see this is that by choosing a Gaussian with the probability of an “extreme event” p, I can choose a scale parameter for a stable random variable with arbitrary choice of \\(\\alpha\\) such that the probability of the same extreme event is less than p.\nFor this reason, infinite variance distributions are more reasonable than normal distributions for modeling exceptionally low-volatility assets that endure rare, but proportionally meaningful price shocks.\nAn unbounded distribution is a reasonable choice for investment returns.\nBecause this is also a characteristic of the Gaussian, this question doesn’t get addressed often.\nThere is no economic theory I’m aware of that would justify hard bounds on investment returns. Anything is possible, however vanishingly small the probability of a quadrillion-percent move. In fact, because of the fact that investment returns are heavy-tailed imposing a hard upper limit (and getting it wrong) will have drastic consequences on the interpretation of the model."
  },
  {
    "objectID": "EM_approach.html",
    "href": "EM_approach.html",
    "title": "Restricted EM Derivation",
    "section": "",
    "text": "Given our \\(\\alpha\\)-SG(\\(\\Sigma\\)) vector \\(X = \\sqrt W G + \\boldsymbol \\mu\\), with underlying latent stable subordinator W, and Gaussian vector G, note that \\(X|W=w \\sim N(\\boldsymbol \\mu,w\\Sigma)\\) we can write our complete data likelihood function as:\n\\[\n\\begin{aligned}\n\\mathcal L_c(\\Theta) &\\equiv f(\\Theta | \\textbf x, w) \\\\\n& = f_{X|w}(\\Theta, \\textbf x|w)f_W(\\Theta)\n\\end{aligned}\n\\]\nWhere \\(\\Theta = (\\theta, \\alpha)^\\text T\\), \\(\\theta = (\\mu, \\Sigma)\\). So the log likelihood for the observed data is:\n\\[\n\\begin{aligned}\n\\ell_c(\\Theta|\\textbf x, w) &= \\sum_{i=1}^N \\log f_{X|w}(\\theta, x_i|w) + \\sum_{i=1}^N \\log f_W(\\alpha, w_i ) \\\\\n& = C - \\frac n 2 \\log |\\Sigma| - \\frac 1 2 \\sum_{i=1}^n \\frac{(\\textbf x_i - \\boldsymbol\\mu)^\\text T \\Sigma ^{-1}(\\textbf x_i - \\boldsymbol\\mu)}{w_i}+\\sum_{i=1}^n \\log f_W(\\alpha, w_i )\n\\end{aligned}\n\\]\nAs in Teimouri et. al., the conditional expectation of the complete data log-likelihood is:\n\\[\n\\begin{aligned}\nQ(\\Theta|\\Theta^{(t)}) = C - \\frac n 2 \\log |\\Sigma| - \\frac 1 2 \\sum_{i=1}^n (\\textbf x_i - \\boldsymbol\\mu)^\\text T \\Sigma ^{-1}(\\textbf x_i - \\boldsymbol\\mu)e_i^{(t,s)}+\\sum_{i=1}^n \\mathbb E [\\log f_W(\\alpha, w_i ) | \\textbf x_i, \\Theta^{(t)} ]\n\\end{aligned}\n\\]\nHere, the CM step involves the posterior mean of 1/W, denoted \\(e_i^{(t,s)}\\) in the Teimouri paper. The paper provides an approximation of this value, but doesn’t derive all the intermediate steps, which are at least helpful to be aware of. Using the fact that \\(X|W=w \\sim N(\\boldsymbol \\mu,w\\Sigma)\\)\n\\[\n\\begin{aligned}\ne_i^{(t,s)} \\equiv \\mathbb E[W^{-1} | \\textbf x_i, \\boldsymbol \\mu^{(t)}, \\Sigma^{(s)}, \\alpha^{(s)}] &= \\int_0^\\infty w^{-1} f_{W|\\textbf X}(w|\\textbf x)dw \\\\\n& = \\int_0^\\infty w^{-1} \\frac{f_{\\textbf X, W}(\\textbf x,w)}{f_{\\textbf X}(x)}dw \\\\\n& = \\frac {1}{f_\\textbf X(x)}\\int_0^\\infty w^{-1} f_{\\textbf X, W}(\\textbf x,w)dw \\\\\n& =   \\frac{\\int_0^\\infty w^{-1}f_W(w)f_{\\textbf X|W}(\\textbf x|w)dw}{\\int_0^\\infty f_W(w)f_{\\textbf X|W}(\\mathbf x|w)dw} \\\\\n&=  \\frac{\\int_0^\\infty w^{-1}f_W(w)f_{\\textbf X|W}(\\textbf x|w)dw}{\\int_0^\\infty f_W(w)f_{\\textbf X|W}(\\mathbf x|w)dw} \\\\\n&= \\frac{\\int_0^\\infty w^{-d/2-1}f_W(w|\\alpha^{(t)})\\exp\\{-\\frac{(\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})}{2w}\\}dw}{\\int_0^\\infty w^{-d/2} f_W(w|\\alpha^{(t)})\\exp\\{-\\frac{(\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})}{2w}\\}dw} \\\\\n& \\approx \\frac{\\sum_{i=1}^M w_i^{-d/2-1}\\exp\\{-\\frac{(\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})}{2w_i}\\}}{\\sum_{i=1}^M w_i^{-d/2}\\exp\\{-\\frac{(\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})}{2w_i}\\}}\n\\end{aligned}\n\\]\nWhere \\(w_i \\sim S_{\\alpha^{(t)}/2}(\\cos \\frac{\\pi \\alpha}{4}^{2/\\alpha}, 1, 0)\\) for large M. Alternatively, subject to some straightforward regularity conditions, Teimouri et al. suggest swapping the Monte Carlo integration with a series representation for an almost exact evaluation of \\(f_W(.|\\alpha^{(t)})\\).\nSo we can use the following (admittedly horrible) representation:\n\\[\n\\begin{aligned}\ne_i^{(t,s)}\n&= \\frac{\\int_0^\\infty w^{-d/2-1}f_W(w|\\alpha^{(t)})\\exp\\{-\\frac{(\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})}{2w}\\}dw}{\\int_0^\\infty w^{-d/2} f_W(w|\\alpha^{(t)})\\exp\\{-\\frac{(\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})}{2w}\\}dw} \\\\\n& \\approx \\frac{\\sum_{j=1}^k \\frac{(-1)^j}{\\Gamma(j+1)}\\Gamma(\\frac{j\\alpha^{(t)}+2}{2})\\Gamma(\\frac{j\\alpha^{(t)}+d+2}{2})\\sin(\\frac{j \\pi \\alpha^{(t)}}{2})(D_i^{(t)})^{-\\frac{j\\alpha^{(t)}+d+2}{2}}}{\\sum_{j=1}^k \\frac{(-1)^j}{\\Gamma(j+1)}\\Gamma(\\frac{j\\alpha^{(t)}+2}{2})\\Gamma(\\frac{j\\alpha^{(t)}+d}{2})\\sin(\\frac{j \\pi \\alpha^{(t)}}{2})(D_i^{(t)})^{-\\frac{j\\alpha^{(t)}+d}{2}}}\n\\end{aligned}\n\\]\nwhere \\(D_i^{(t)} = (\\textbf x - \\boldsymbol\\mu^{(t)})^\\text T {(\\Sigma^{(t)})} ^{-1}(\\textbf x - \\boldsymbol\\mu^{(t)})\\), \\(k = [\\min(168, 168/\\alpha^{(t)})]\\), and the following condition is met (e.g. \\(D_i\\) is sufficiently large):\n\\[\nD_i^{(t)} \\in \\bigg( \\bigg[\\frac{\\Gamma(\\frac{k\\alpha^{(t)}+\\alpha^{(t)}+2}{2})\\Gamma(\\frac{k\\alpha^{(t)}+\\alpha^{(t)}+2+d}{2})}{(k+1)\\Gamma(\\frac{k\\alpha^{(t)}+2}{2})\\Gamma(\\frac{k\\alpha^{(t)}+2+d}{2})}\\bigg]^{2/\\alpha}, \\infty\\bigg)\n\\]\nDespite being unpleasant to write in Latex, these expressions and conditions are extremely quick to compute/simulate/etc.\nFinally, returning to the complete data log likelihood, there’s the issue of the second term, which doesn’t have a closed form or clear path to maximization. To simplify the process, we use the fact that the \\(\\alpha\\) parameter is identical across all of the marginals and a fast univariate numerical MLE already implemented in several packages to quickly estimate the MLE for this parameter and treat it as fixed. We can either do this for an arbitrary marginal or sample of marginals, or if the dimensonality is small enough, by estimating all marginal \\(\\alpha\\) parameters and then averaging them.\nFrom there, we can perform conditional EM as follows:\nE-step: Estimate the posterior mean \\(e_i^{(t,t)} = \\mathbb E[W^{-1} | \\textbf x_i, \\boldsymbol \\mu^{(t)}, \\Sigma^{(t)}, \\alpha_{MLE}]\\) as described above.\nM-step: Update \\(\\theta = (\\boldsymbol \\mu, \\Sigma)\\) as:\n\\(\\boldsymbol \\mu^{(t+1)} = \\frac{\\sum_{i=1}^n \\textbf x_i e_i^{(t,t)}}{\\sum_{i=1}^n e_i^{(t,t)}}\\), \\(\\Sigma^{(t+1)} =  \\sum_{i=1}^n (\\textbf x_i - \\boldsymbol\\mu^{(t)}) (\\textbf x_i - \\boldsymbol\\mu^{(t)})^\\text T e_i^{(t,t)}/\\sum_{i=1}^n e_i^{(t,t)}\\)\nRepeat until estimates converge.\nBesides fixing \\(\\alpha\\) with the MLE, another departure from Teimouri et al. is that we don’t actually care about the estimated values of \\(\\theta\\) themselves. Instead, we only want to retain the vector \\(\\hat{\\textbf e}\\), since the data are then approximately normal and we can use either the sample covariance matrix (the true MLE of \\(\\Sigma\\)), or an adjustment for dimensionality that outperforms it.\nAs a reminder, note that we’re estimating the covariance matrix of the normal vector, not the dispersion matrix of the stable vector \\(X\\). The two may differ by a scaling constant of 1/2 depending on the way they’re parameterized."
  },
  {
    "objectID": "progress.html#subordinated-correlation-estimator",
    "href": "progress.html#subordinated-correlation-estimator",
    "title": "Current Progress",
    "section": "Subordinated Correlation Estimator",
    "text": "Subordinated Correlation Estimator\nMy derivation of the FLOM covariance leads to (what I believe is) a novel estimator for the correlation coefficient of the subordinated Gaussian vector.\nUsing the fact that: \\[\n\\begin{aligned}\n\\mathbb E(|X_1 X_2|^p) &= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^{p+1} \\gamma_1^{p/} \\gamma_2^{p}}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)\n\\end{aligned}\n\\]\nThis requires that \\(p &lt; \\alpha/2\\). From there, we can get the following estimator for the underlying correlation between \\(G_1\\) and \\(G_2\\):\n\\[\n\\begin{aligned}\n\\hat \\rho = \\text{sign}\\bigg( \\frac 1 N \\sum_{i=1}^N \\text{sign}(X_{1i} X_{2i}) \\bigg) \\bigg| {_2F_{1p}}^{-1}\\bigg( \\frac{\\alpha \\pi \\Gamma(-p)\\sum_{i=1}^N|X_{1i} X_{2i}|^p}{N2^{p+2}(\\gamma_1 \\gamma_2)^p\\Gamma(\\frac{p+1}{2})^2}\\bigg)\\bigg|^{1/2}\n\\end{aligned}\n\\]\nHere, I use \\({_2F_{1p}}^{-1}(\\cdot)\\) to denote the inverse of the hypergeometric function \\({}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)\\), such that \\({_2F_{1p}}^{-1}({}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2))=\\rho^2\\). While a bit nasty and not ideal for analyzing the behavior of this estimator, this can be implemented numerically without much trouble.\nA quick examination of the hypergeometric function on the relevant domain shows that it’s really not cause for concern despite lacking a closed form:\n\nIt’s worth taking a second to justify taking the mean of the sign function, since \\(\\mathbb E X_1 X_2 \\nless \\infty\\). Note that \\(\\text{sign}(XY) \\in \\{-1, 1\\}\\), so the random variable is bounded and all moments are finite. Equivalently, \\(\\mathbb E[\\text{sign}(XY)] = P(XY &gt; 0)- P(XY &lt; 0) \\in (-1,1)\\), which always exists.\nBecause of our expectation definition above, this estimator will converge pointwise in probability to \\(\\rho\\) by the law of large numbers. Better yet, it will have finite variance when \\(p &lt; \\alpha/2\\), which is required to get the subordinator to have finite expectation. This is a somewhat stronger claim for finite variance than the correlation estimator candidate offered above (which was made simply on the basis of it being bounded). So it might have better small-sample performance.\nThe benefits of this estimator are that it has finite variance and it should be extrmeley computationally cheap.\nThere are a few of problems with this estimator: First we can’t make any guarantees about convergence in expectation. Second, I don’t really expect it to perform incredibly well because of the need to estimate other parameters as inputs. Third, it doesn’t fix the issue of correlation-only estimates having worse performance in Median Oracle. The latter is fixed by recombining it with estimates of \\(\\alpha, \\sigma_1, \\sigma_2\\), which can be made quickly and accurately without regard for the dimensionality of the problem (e.g. using MLE).\nI tested this estimator via simulation on a range of different true correlation values, assuming other parameters \\(\\alpha, \\gamma_1, \\gamma_2\\) are known. I ran this on a Databricks cluster running an i3.4xlarge node with 16 cores. This simulation took approximately 3 days to run.\n\nThe plot above shows the convergence of the estimated correlation to the true correlation (red horizontal line). Each linetype represents a different choice of p. A couple things are clear from the results of the simulation: the estimator does show good convergence to the true correlation value, but not for all values of p, not consistently across all true correlation values, and not likely better than MLE (MLE simulations were done separately, but haven’t been incoporated into these plots).\n\nThis estimator is interesting, and may be useful for some cases where other parameters are known, or if speed is more important than accuracy. Most importantly, it actually performs quite well in regimes of interest for real-world equity returns (between 0.2 and 0.8) However the fact that it must be constructed elementwise is a strike against it compared to other options, as is the fact that it still requires estimates of \\(\\alpha\\) and each marginal distribution’s scale. Also, the wrong choice of p takes this from a possibly competitive estimator to a terrible one, which may be a problem in practice when true parameter values are unknown.\n\nOne important desirable property is that there does seem to be an optimal p in the sense of variance mimization across the range of \\(\\alpha\\) values. This suggests that the problem of choosing p may not be as big as it seems. For p=0.4, for instance, variance is guaranteed to be finite for all relevant cases (since we assume \\(\\alpha&gt;1\\) already), and estimator performance should be reasonably good.\nStill do to: - Incorporate MLE estimates for comparison - Test if there is a fixed best p when defined in terms of \\(\\alpha\\)."
  },
  {
    "objectID": "progress.html#restricted-expectation-maximization-approach",
    "href": "progress.html#restricted-expectation-maximization-approach",
    "title": "Current Progress",
    "section": "Restricted Expectation-Maximization Approach",
    "text": "Restricted Expectation-Maximization Approach\nMy original research proposal suggested estimating and then conditioning on the stable subordinator in the \\(\\alpha\\)-SG(\\(\\Sigma\\)) random variable. A natural approach to this is expectation-maximization, but I was resistant to this at first because if the MLE is accessible then it seems like we should obviously prefer it to EM, which only hopes to converge to the MLE. And since Median Oracle appears to do better than the MLE in the high-dimensional setting, there’s no point to this approach. But on deeper consideration, this may yield better estimates than EM in the high-dimensional setting.\nIf we estimate and then condition on the realized latent variable W (the maximally skewed stable random variable), we can get \\(X|W=w \\sim N(0,w\\Sigma)\\). Moreover since the estimation of each \\(w_i\\) happens in the cross-section, I’d expect the estimation error of these variables to decrease as the dimensionality increases, whereas the error of \\(\\Sigma_{EM}\\) will of course increase - while estimating \\(w_i^{(t)}\\) relies on the estimate \\(\\Sigma^{(t-1)}\\), it may be less impacted by a “bad” estimate, especially if we ultimately only care about correlations (consider the single-observation case). Taking the \\(X|W\\) and plugging this into the closed form MLE for \\(\\Sigma\\) given Gaussian data should give us a better estimate. Better still, adjusting this MLE for high dimensionality using the RIE should yield an even better estimate.\nLikewise, this should be provable using asymptotic efficiency of MLE.\nEM for stable random vectors isn’t straightforward, and an approach was only first offered in 2018 by Teimouri et. al. . This approach requires a conditional maximization (CM) step, then a conditional maximization of the likelihood (CML) step, which itself involves a separate stochastic EM to approximate the parameters of a related Weibull distribution. This is required because the authors are trying to jointly estimate all parameters via EM. Since we’re only interested in \\(\\Sigma\\), we can actually simplify \\(\\alpha\\)-SG(\\(\\Sigma\\)) EM significantly, both computationally and conceptually, by restricting our parameter search to not include \\(\\alpha\\), totally eliminating the CML step, and making the process equivalent to a simple Gaussian EM with a conditional maximization step for W.\nMy approach to stable EM is detailed here."
  },
  {
    "objectID": "copula_transform.html",
    "href": "copula_transform.html",
    "title": "Linked Copula Transform Estimator",
    "section": "",
    "text": "A copula is a multivariate CDF that encodes the dependence of a random vector. They provide a way to uncouple the distribution of the marginals from the dependence structure. Because the copulae of X and G in our problem formulation are both parameterized by \\(\\textbf R\\), the correlation matrix that corresponds to \\(\\Sigma\\), the covariance matrix of G/dispersion matrix of X, the MLE of this parameter from the copula distribution itself may be more easily computed than that of the stable vector. (Note: This is probably not true)\nI realized that we don’t actually need to calculate the MLE for the copula – since we have the estimated marginals of the stable vector X, we can “push forward” from our empirical copula to Gaussian marginals, since their parameters are now fully specified (the standard deviation of any marginal \\(i\\) is \\(\\sigma_i = \\sqrt 2 \\gamma_i\\), where \\(\\gamma_i\\) is the corresponding stable scale). From there, we can calculate the sample covariance matrix of this final distribution to get our MLE of \\(\\Sigma\\).\nClearly, this doesn’t work in both directions. If \\(\\Sigma = I_d\\), then the multivariate Gaussian has independent marginals, and an independence copula which has uniform (flat) density. Since an elliptical stable distribution can’t have independent components, the resulting stable vector will be “star shaped”, not spherical.\nPutting a stable copula on Gaussian marginals, the resulting distribution no longer has an elliptical dependence structure due to the higher tail dependence imposed by the stable copula. Even so, this new distribution shares the covariance matrix of \\(G\\), and is an approximate-MLE method in the sense that the new (stable copula + Gaussian marginals) distribution is approximately multivariate Gaussian, deviating more as \\(\\alpha \\rightarrow 0\\) deforms the level curves further from an ellipse. (Proofs of these facts below)"
  },
  {
    "objectID": "progress.html#normalized-sample-covariance-matrix-approach-nscm",
    "href": "progress.html#normalized-sample-covariance-matrix-approach-nscm",
    "title": "Current Progress",
    "section": "Normalized Sample Covariance Matrix Approach (NSCM)",
    "text": "Normalized Sample Covariance Matrix Approach (NSCM)\nThe idea was to estimate the correlation matrix with NSCM (by inverting its expected eigenvalues), and then rescaling via EM. Need to add more details on the procedure, since it’s pretty cool."
  },
  {
    "objectID": "progress.html#empricical-copula-transform",
    "href": "progress.html#empricical-copula-transform",
    "title": "Current Progress",
    "section": "Empricical Copula Transform",
    "text": "Empricical Copula Transform\nSince univariate stable MLE works well (fast and very accurate), one idea I had is to recreate the full dispersion matrix via lower-dimensional sub-matrices. These can be used to estimate W (as in EM above). Taking this idea to the limit (e.g. the marginals) yields this new idea…\nIn testing, this new approach works incredibly well, besting direct MLE calculations both in terms of error and time (by a lot).\nResults and detailed procedures here here: Linked Copula Transform Estimator.\n[Last Updated on November 30, 2024]"
  },
  {
    "objectID": "copula_transform.html#orignal-idea",
    "href": "copula_transform.html#orignal-idea",
    "title": "Linked Copula Transform Estimator",
    "section": "",
    "text": "A copula is a multivariate CDF that encodes the dependence of a random vector. They provide a way to uncouple the distribution of the marginals from the dependence structure. Because the copulae of X and G in our problem formulation are both parameterized by \\(\\textbf R\\), the correlation matrix that corresponds to \\(\\Sigma\\), the covariance matrix of G/dispersion matrix of X, the MLE of this parameter from the copula distribution itself may be more easily computed than that of the stable vector. (Note: This is probably not true)\nI realized that we don’t actually need to calculate the MLE for the copula – since we have the estimated marginals of the stable vector X, we can “push forward” from our empirical copula to Gaussian marginals, since their parameters are now fully specified (the standard deviation of any marginal \\(i\\) is \\(\\sigma_i = \\sqrt 2 \\gamma_i\\), where \\(\\gamma_i\\) is the corresponding stable scale). From there, we can calculate the sample covariance matrix of this final distribution to get our MLE of \\(\\Sigma\\).\nClearly, this doesn’t work in both directions. If \\(\\Sigma = I_d\\), then the multivariate Gaussian has independent marginals, and an independence copula which has uniform (flat) density. Since an elliptical stable distribution can’t have independent components, the resulting stable vector will be “star shaped”, not spherical.\nPutting a stable copula on Gaussian marginals, the resulting distribution no longer has an elliptical dependence structure due to the higher tail dependence imposed by the stable copula. Even so, this new distribution shares the covariance matrix of \\(G\\), and is an approximate-MLE method in the sense that the new (stable copula + Gaussian marginals) distribution is approximately multivariate Gaussian, deviating more as \\(\\alpha \\rightarrow 0\\) deforms the level curves further from an ellipse. (Proofs of these facts below)"
  },
  {
    "objectID": "copula_transform.html#procedure",
    "href": "copula_transform.html#procedure",
    "title": "Linked Copula Transform Estimator",
    "section": "Procedure",
    "text": "Procedure\nThe procedure for what I’m calling the Linked Copula Transform Estimator is as follows. The marginal distributions of our \\(X\\) and \\(G\\) vectors are linked in the sense that knowing the parameters of \\(X\\) fully specifies the Gaussian marginals: the scale paramter is related by \\(\\sigma = \\sqrt 2 \\gamma\\), and the location parameter is shared.\nThe copula of \\(X\\) is a d-dimensional CDF on \\([0,1]^d\\) with uniform marginals that captures all of the dependence (but not the scale) between \\(X\\) and \\(G\\). The random vector\n\\((U_1, \\dots, U_d) = (F_{X_1}(x_1), \\dots, F_{X_d}(x_d))\\)\nconverts realizations \\(x\\) to a sample from the copula of \\(F_{\\textbf X}\\).\nIn the case of elliptically symmetric stable distributions (including the Gaussian), the copula encodes (and is parameterized by) the shape of the correlation matrix defined by \\((\\sqrt{\\text{diag}(\\Sigma)})^{-1}\\Sigma(\\sqrt{\\text{diag}(\\Sigma)})^{-1}\\). Since \\(X\\) has a stable copula, it encodes the same elliptical shape as the Gaussian copula of G. The procedure to estiamte this covariance matrix is as follows:\n\nEstimate each marginal distribution via numerical univarite MLE. This is quick and accurate and can be made even faster through paralellization.\nTransform each observation \\(X_i\\) into a uniform vector \\(U_i\\) drawn from the copula distribution of \\(X\\) by pushing it through its CDF (which is again available numerically)\nTransform each observation \\(U_i\\) into \\(\\tilde G_i\\) by pushing each marginal through the appropriately scaled inverse Gaussian CDF/erf: \\(\\tilde G_{ij} = 2\\gamma_j\\text{erf}^{-1}(2*U_{ij}-1)\\).\nThe resulting data matrix \\(\\tilde G\\) now has Gaussian marginals corresponding to those of \\(X\\), with a the empirical copula of \\(X\\). This distribution is approximately multivariate normal and \\(\\Sigma\\) can now be estimated via MLE in closed form as the sample covariance matrix.\n\nTo summarize in one step:\n\\(\\hat{\\Sigma} = \\frac{1}{n-1} \\tilde{\\mathbf{G}}^{\\text{T}} \\tilde{\\mathbf{G}}\\).\nWhere \\(\\tilde G_j = \\sqrt 2 \\gamma_j \\Phi^{-1}(\\hat F_{X_{j}}(X_{j}))\\), \\(j = 1, \\dots, d\\) is the \\(j\\)-th column of the approximately-Gaussian sample matrix \\(\\tilde G\\), with:\n\n\\(\\hat F_{X_{j}}\\) the \\(\\alpha\\)-stable distribution function of marginal \\(j\\) with parameters determined via direct MLE,\n\\(\\Phi^{-1}\\) the inverse standard normal CDF,\n\\(\\gamma_j\\) the scale parameter of \\(\\hat F_{X_{j}}\\),\nand \\(\\tilde G_j\\) denoting the \\(j\\)-th column of the approximately-Gaussian sample matrix \\(\\tilde G\\).\n\n(Note that \\(\\tilde G\\) is mean zero by construction.)"
  },
  {
    "objectID": "copula_transform.html#some-more-formal-results",
    "href": "copula_transform.html#some-more-formal-results",
    "title": "Linked Copula Transform Estimator",
    "section": "Some More Formal Results",
    "text": "Some More Formal Results\nLemma: The stable copula applied to any (fixed) stable marginals with \\(\\alpha_1 \\neq \\alpha_2\\) cannot be ellptically symmetric.\nProof: This is immediate from the Jacobian of the transformation. Since J is diagonal, it acts on the coordinate axes, but these are fixed by the marginal distribution parameters. Elliptical symmetry can only be maintained then if \\(J(X) \\propto I_d\\) for all values of \\(X\\). Clearly, this only happens if the two marginals share the same distribution up to scale and translation.\nLemma: \\(G\\) and \\(\\tilde G\\) share the same covariance matrix \\(\\Sigma\\), the dispersion matrix of the stable vector \\(X\\).\nProof: To prove."
  },
  {
    "objectID": "copula_transform.html#more-things",
    "href": "copula_transform.html#more-things",
    "title": "Linked Copula Transform Estimator",
    "section": "More Things",
    "text": "More Things\n\nThe deformation (as measured by the Jacobian) seems related to the KL-divergence of the starting and ending marginals. We might be doing the transformation that minimizes the KL-divergence between \\(X\\) and \\(\\tilde G\\) while preserving both Gaussian marginals and covariance?\nCan we build a correction to force the dependence to be elliptical (getting the actual MLE)\nNormalizing Flows could be an interesting way to make the final distribution more elliptical (and thus more efficient) by learning the transform to get from \\(\\tilde G \\rightarrow G\\)."
  },
  {
    "objectID": "copula_transform.html#some-practical-results",
    "href": "copula_transform.html#some-practical-results",
    "title": "Linked Copula Transform Estimator",
    "section": "Some Practical Results",
    "text": "Some Practical Results\nI tested the LCTE across the following paramters:\nn_samples = [100, 200, 500, 1000, 2500, 5000] \ndimensions = [2, 5, 10, 50, 75, 150, 250, 300] \nalphas = [0.5, 1.0, 1.5, 1.9] \nThe LTCE was benchmarked against direct numerical MLE implemented in the stable R package via a python wrapper. I believe that this implementation of stable MLE is the current state of the art. I measured normalized error - \\(||\\hat \\Sigma - \\Sigma||/||\\Sigma||\\) - for both the LTCE and the direct MLE, against both the true covariance matrix of \\(G\\) as well as the sample covariance matrix calculated on the “latent” G, which is stored as part of the test class. This test was time-consuming because of the extremely slow estimation of the direct MLE approach in high dimensions. To save time, I only performed a single estimate at each unique set of parameters, but reporting a statistic of many such estimates seems worthwhile. Since there doesn’t seem to be a dramatic difference across the choice of n_samples, I averaged these results in the plots below.\nLooking at estimator performance, the LTCE seems to reliably outperform the direct MLE by approximately 10% - 20%.\n\nSome likely reasons for this performance are below.\nEven more significant are the computational advantages of the LTCE. Because costly numerical integration only happens in the univariate marginals, it’s computationally inexpensive and scales linearly with the dimension. It’s clear that this approach dramatically outperforms direct MLE for dimensions larger than about 5:"
  },
  {
    "objectID": "copula_transform.html#why-this-works",
    "href": "copula_transform.html#why-this-works",
    "title": "Linked Copula Transform Estimator",
    "section": "Why This Works",
    "text": "Why This Works\nThe speed improvements are obvious, but I think the LTCE achieves lower estimator error because even though the tail dependence results in a non-elliptical dependence structure (e.g. it’s a deformed elliptical, similar to the t-copula case), \\(\\tilde G\\) and \\(G\\) share a covariance matrix (proof below). Thus, the sample covariance matrix of \\(\\tilde G\\) is an unbiased estimator of \\(\\Sigma\\).\nBetter yet, in finite samples, this estimator is more efficient than the stable MLE due to the simple fact that the variance of the stable MLE is only guaranteed to be asymptotically finite, whereas the Linked Copula Transform Estimator has finite variance for any \\(n\\), since the transformation via the stable CDF induces finite support to the sample, and the transform back to Gaussian random variables doesn’t violate this. As a consequence, the LCTE has finite variance even if the marginal parameter estimates are misspecified.\nMoreover, since the stable copula is empirical rather than parameteric, relatively smaller samples are unlikely to see extreme events under which the deforemations of the elliptical dependence structure are worst. In other words, the transformed sample is likely to be much more elliptical in shape than it appears analytically (e.g. via the Jacobian below), resulting in better estimates.\nThinking about what’s actually going on using the Jacobian of the transformation in 2 dimensions:\n\\[\nJ =\n\\begin{bmatrix}\n\\gamma_1\\frac{f_{X_1}(X_1)}{\\phi(\\Phi^{-1}(F_{X_1}(X_1)))} & 0 \\\\[2ex]\n0 & \\gamma_2\\frac{f_{X_2}(X_2)}{\\phi(\\Phi^{-1}(F_{X_2}(X_2)))}\n\\end{bmatrix}\n\\]\nClearly, we have a nonlinear transformation happening in the principal axes, so shape is only maintained if the Jacobian is a multiple of the identity matrix. (I think) this can only happen when we start with a Gaussian… but obviously, the Jacobian is small when \\(\\alpha\\) is near 2. One consequence of this is that we’re closer to an elliptical dependence structure (and thus \\(\\tilde G\\) is closer to a multivariate normal) when \\(\\alpha\\) is close to 2. As \\(\\alpha \\rightarrow 2\\) I believe LCTE converges to the MLE for \\(\\Sigma\\).\nAlso, note that our new dependence structure maintains its symmetry about the principal axes, since it’s clear that \\(J(X) = J(-X)\\). Actually, the Jacobian is completely invariant to the sign of any component, preserving radial symmetry of the distribution despite the non-elliptical deformation."
  },
  {
    "objectID": "progress.html#empirical-copula-transform",
    "href": "progress.html#empirical-copula-transform",
    "title": "Current Progress",
    "section": "Empirical Copula Transform",
    "text": "Empirical Copula Transform\nSince univariate stable MLE works well (fast and very accurate), one idea I had is to recreate the full dispersion matrix via lower-dimensional sub-matrices. These can be used to estimate W (as in EM above). Taking this idea to the limit (e.g. the marginals) yields this new idea…\nIn testing, this new approach works incredibly well, besting direct MLE calculations both in terms of error and time (by a lot).\nResults and detailed procedures here here: Linked Copula Transform Estimator.\n[Last Updated on December 2, 2024]"
  },
  {
    "objectID": "copula_transform.html#references",
    "href": "copula_transform.html#references",
    "title": "Linked Copula Transform Estimator",
    "section": "References",
    "text": "References\nRoger B. Nelsen (1993) Some concepts of bivariate symmetry, Journal of Nonparametric Statistics, 3:1, 95-101, DOI: 10.1080/10485259308832574"
  },
  {
    "objectID": "progress.html#linked-copula-transform-estimator-lcte",
    "href": "progress.html#linked-copula-transform-estimator-lcte",
    "title": "Current Progress",
    "section": "Linked Copula Transform Estimator (LCTE)",
    "text": "Linked Copula Transform Estimator (LCTE)\nSince univariate stable MLE works well (fast and very accurate), one idea I had is to recreate the full dispersion matrix via lower-dimensional sub-matrices. These can be used to estimate W (as in EM above). Taking this idea to the limit (e.g. the marginals) yields this new idea…\nIn testing, this new approach works incredibly well, besting direct MLE calculations both in terms of error and time (by a lot).\nResults and detailed procedures here here: Linked Copula Transform Estimator."
  },
  {
    "objectID": "progress.html#normalized-lcte",
    "href": "progress.html#normalized-lcte",
    "title": "Current Progress",
    "section": "Normalized LCTE",
    "text": "Normalized LCTE\n[Last Updated on January 6, 2024]"
  },
  {
    "objectID": "RIE.html",
    "href": "RIE.html",
    "title": "Optimal RIE Derivation",
    "section": "",
    "text": "This comes from “A First Course in Random Matrix Theory”, with some more simple details worked out and specifically for the multiplicative case (e.g. a covariance matrix)… This explanation is hopefully self-contained and doesn’t rely on too much RMT knowledge.\n\nPreliminaries\nSuppose we have a sample data matrix \\(X \\in \\mathbb R^{T \\times N}\\), with \\(X = Y\\Sigma^{1/2}\\), where \\(Y\\) is a TxN matrix of standard Gaussian entries, and \\(\\Sigma\\) is the true covariance matrix of \\(X\\).\nThen \\(W_q = \\frac{1}{T}Y ^ \\text T Y\\) is a wishart matrix with aspect ratio \\(q=N/T\\), and the sample covariance matrix follows the model \\(E = \\frac{1}{T}X^\\text T X = \\Sigma ^{1/2} {W_q \\Sigma}^{1/2}\\).\nWe want to estiamte \\(\\Sigma\\) optimally.\n\n\nBasic Setup\nLeveraging its eigendecomposition, we can also writ the sample covariance matrix as the average of rank-1 matrices: \\(E = \\sum_{i=1}^N \\lambda_i v_iv_i^{\\text T}\\). We want a Rotationally Invariant Estimator (RIE) of \\(\\hat\\Sigma = \\Xi(E)\\), such that \\(\\Xi(OEO^{\\text T})=O\\Xi(E)O^{\\text T}\\) – e.g. if the sample covariance matrix is rotated by some matrix O, then our estimation of \\(\\Sigma\\) will also be rotated the same way. The intuition (p.302) is that we don’t have priors on the eigenvectors of \\(\\Sigma\\), so the sample eigenvectors provide at least as good a prior as any other choice we have. This means that our estimator has the same eigenvectors as \\(E\\):\n\\(\\Xi(E) = \\sum_{i=1}^N \\xi_i v_iv_i^{\\text T}\\), and the goal now becomes to find the adjustment to the empirical eigenvalues \\(\\lambda_i\\) of \\(\\Sigma\\) to get the corresponding “optimally adjusted” eigenvalues \\(\\xi_i\\).\n\n\nOracle Estimator\nHere, “optimal” means we want to minimize squared Frobenius error (there are many practical reasons to choose this error for financial applications). So, the optimal choice of eigenvalues \\(\\xi_i\\) minimizes:\n\\[\n\\begin{aligned}\n||\\Sigma - \\hat \\Sigma||_F^2 &= \\text{Tr}(\\Xi(E) - \\Sigma)^2 \\\\\n& = \\sum_{i=1}^N v_i^{\\text T}(\\Xi(E) - \\Sigma)v_i &(*) \\\\\n&= \\sum_{i=1}^N v_i^{\\text T}(\\Xi(E)^2 - 2\\Xi(E)\\Sigma + \\Sigma^2)v_i \\\\\n&= \\sum_{i=1}^N \\bigg( v_i^{\\text T} \\Xi(E)^2 v_i - 2v_i^{\\text T}\\Xi(E)\\Sigma v_i +v_i^{\\text T} \\Sigma^2v_i \\bigg) \\\\\n&= \\sum_{i=1}^N \\bigg( v_i^{\\text T} V\\Omega^2V^{\\text T} v_i - 2v_i^{\\text T} V\\Omega^2V^{\\text T}\\Sigma v_i +v_i^{\\text T} \\Sigma^2v_i \\bigg) \\\\\n&= \\sum_{i=1}^N \\bigg( \\xi_i^2 - 2\\xi_i v_i^{\\text T}\\Sigma v_i + v_i^{\\text T} \\Sigma^2v_i \\bigg)\n\\end{aligned}\n\\]\nWhere (*) holds as a property of a trace projected onto an orthonormal basis, and \\(\\Omega = \\text{diag}(\\xi_1, \\dots, \\xi_n)\\), the eigenvalues of the estimator.\nMaximizing this quantity:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\xi_k} \\bigg( \\xi_k^2 - 2\\xi_k v_k^{\\text T}\\Sigma v_k + v_k^{\\text T} \\Sigma^2v_k \\bigg) &=  \n2\\xi_k - 2 v_k\\Sigma v_k^{\\text T}\\\\\n\\implies & \\xi_k^* = v_k\\Sigma v_k^{\\text T}\n\\end{aligned}\n\\]\nThis \\(\\xi_k^*\\) is the Oracle Estimator for the true eigenvalues of \\(\\Sigma\\) given \\(E\\). Of course, this isn’t helpful since we don’t know the true \\(\\Sigma\\) in the first place. This is where Random Matrix Theory comes in – specifically, it’s how we recast our dependence on \\(\\Sigma\\) as a representation that depends only on our realization \\(E\\) (the “large-dimension miracle”).\n\n\nAdding a Dash of RMT: Eigenvector Overlaps\nRewriting the estimator by expanding it over the eigenvectors of \\(\\Sigma\\):\n\\[\n\\begin{aligned}\n\\xi_k &= v_k\\Sigma v_k^{\\text T} \\\\\n&=  v_k (\\sum_{i=1}^N \\mu_i u_i u_i^{\\text T})v_k^{\\text T} \\\\\n&= \\sum_{i=1}^N \\mu_i v_k u_i u_i^{\\text T}v_k^{\\text T} \\\\\n&= \\sum_{i=1}^N \\mu_i (u_i^{\\text T}v_k)^2 \\\\\n&\\underset{N \\rightarrow \\infty}{\\longrightarrow} \\int \\mu \\Phi(\\lambda_k, \\mu) \\rho_\\Sigma(\\mu) d\\mu\n\\end{aligned}\n\\]\nWhere above, \\(\\Phi(\\lambda_k, \\mu_j) = N\\mathbb E[(v_i^{\\text T}u_k)^2]\\), with expectation taken over different realizations of the randomness. (See p.298 for an alternative interpretation). This expectation is necessary because unlike the eigenvalues of the sample covariance matrix, the eigenvectors will continue to flucuate when N goes to infinity, never reaching any kind of deterministic limit.\nNote from the above that if it just so happens that our sample covariance matrix \\(E\\) has the same eigenvectors as \\(\\Sigma\\), then \\((u_i^{\\text T}v_k)^2=1\\), and the optimal estimate for \\(\\xi_k\\) is just the average eigenvalue of \\(\\Sigma\\). In general, it depends on the squared “overlaps” between the eigenvectors of the sample covariance matrix and the true covariance matrix.\nThese overlaps are related to the Resolvent of the sample covariance matrix as follows:\n\\[\n\\begin{aligned}\nG_E(z) &= (zI_d - E)^{-1} \\\\\n&= (zI_d - V\\Lambda V^{\\text T})^{-1}\\\\\n&= (zI_d - V\\Lambda V^{\\text T})^{-1}\\\\\n&= (V(z-\\Lambda)V^{\\text T})^{-1}\\\\\n&= V(z-\\Lambda)^{-1}V^{\\text T} \\\\\n&= \\sum_{i=1}^N\\frac{v_iv_i^{\\text T}}{z-\\lambda_i}\\\\\n\\text{ So we get: }\\\\\nu^{\\text T}G_E(z)u &= \\sum_{i=1}^N\\frac{u^{\\text T}v_iv_i^{\\text T}u}{z-\\lambda_i}\\\\\n&= \\sum_{i=1}^N\\frac{(u^{\\text T}v_i)^2}{z-\\lambda_i}\n\\end{aligned}\n\\]\nFor any \\(u \\in \\mathbb R^N\\) and (typically, but not necessarily small) \\(z \\in \\mathbb C\\).\nThe result above means that the projection of the Resolvent matrix onto a true eigenvector \\(u\\) provides the weighted sum of squared overlaps (aka directional consistency) between the true and sample covariance matrices’ eigenvectors, where weighting is inversely proportional to the spectral distances \\(z-\\lambda_i\\) using only the sample eigenvalues (e.g. those from the noisy matrix). This acts as a sort of bridge between the eigendecomposition of our noisy matrix \\(E\\) and \\(\\Sigma\\).\nWhen our u is a unit vector (e.g. a vector extracted from our orthonormal basis of \\(\\Sigma\\)), observe that for all \\(i\\) we have \\((u^{\\text T}v_i)^2\\leq 1\\) and \\(\\sum_{i=1}^N(u^{\\text T}v_i)^2=1\\).\nLooking carefully at our new sum, we notice that the projection of the Resolvent matrix results in something resembling the Stieltjes transform, defined as \\(\\mathfrak g_E(z)=\\frac{1}{N}\\text{Tr}(G_E(z))=\\frac{1}{N}\\sum_{i=1}\\frac{1}{z-\\lambda_i}\\). While the Stieltjes transform \\(\\mathfrak g_E(z)\\) averages over all eigenvalues with equal weight 1/N, the projection onto \\(u\\) gives a weighted average where weights are the squared overlaps. So, we give more weight to directions where the eigenvectors match up well.\nNow, we can use the Stieltjes inversion formula on this weighted term:\n\\[\n\\begin{aligned}\n\\Im \\{\\ u^{\\text T}G_E(\\lambda_i - i\\eta)u \\}\\ &\\approx \\pi \\rho_E(\\lambda_i)\\Phi(\\lambda_k, \\mu_j) \\\\\n&\\implies \\Phi(\\lambda_k, \\mu_j) \\approx \\frac{\\Im \\{\\ u^{\\text T}G_E(\\lambda_i - i\\eta)u \\}\\ }{\\pi\\rho_E(\\lambda_i)}\n\\end{aligned}\n\\]\nIn other words, we get the spectral density of E, with multiplicative scaling factors that encode the alignment of the eigenvectors. For example, if our eigenvectors are perfectly aligned, we give full weight to the sample eigenvalues. If they’re somehow orthogonal, the sample eigenvalue contributes zero weight to the density.\nUnfortunately, we still need to know the true eigenvectors in order to prject the Resolvent on to them…\n\n\nAnother Dash of RMT: Removing Dependence on \\(\\Sigma\\)\nUsing the stuff we derived above, we can now rewrite our adjusted eigenvalues by plugging the Stieltjes representation of the projection into the limiting case:\n\\[\n\\begin{aligned}\n\\xi_k = \\sum_{i=1}^N \\mu_i (u_i^{\\text T}v_k)^2\n&\\longrightarrow \\lim_{\\eta \\rightarrow 0^+} \\sum_{i=1}^N \\mu_i \\frac{\\Im \\{\\ u_i^{\\text T}G_E(\\lambda_i - i\\eta)u_i \\}\\ }{\\pi\\rho_E(\\lambda_i)} \\\\\n&= \\frac{1}{\\pi\\rho_E(\\lambda_k)} \\lim_{\\eta \\rightarrow 0^+} \\Im \\bigg\\{\\ \\sum_{i=1}^N  \\bigg(u_i^{\\text T}\\mu_i G_E(\\lambda_i - i\\eta)u_i \\bigg)\\bigg\\}\\ \\\\\n&= \\frac{1}{\\pi\\rho_E(\\lambda_k)}\\lim_{\\eta \\rightarrow 0^+} \\Im\\bigg[\\text{Tr}(\\Sigma G_E(\\lambda_i - i\\eta)) \\bigg]\\\\\n&=\\frac{1}{\\pi\\rho_E(\\lambda_k)}\\lim_{\\eta \\rightarrow 0^+} \\Im \\bigg[\\tau(\\Sigma G_E(\\lambda_i - i\\eta))\\bigg]\n\\end{aligned}\n\\]\nHere, the last line is an informal convention where the normalized trace is used by convention in place of the trace in the high-dimensional asymptotic setting to ensure convergence to a number, the second-to-last line comes from the fact that if we expand the trace by the eigenbasis of \\(\\Sigma\\), we get Sigma back…\nOkay, here’s where the magic happens: Continuing using the subordination equation (19.30), we write \\(G_E(z) = Y(z)G_\\Sigma(Z(z))\\), where in the multiplicative case, \\(Y(z) = Z(z)/z\\).\n\n\n\n\n\n\nNote\n\n\n\nNote, if we don’t want to take it at face value that this subordination relationship is true, let’s derive it…\nSince our original setup involves multiplying \\(E = \\Sigma ^{1/2} {W_q \\Sigma}^{1/2}\\), we can use the S-transform for freely convoluted matrices (remember, \\(\\Sigma\\) is a deterministic matrix, so it is automatically mutually free w/r/t any random matrix): \\(S_E(t) = S_\\Sigma(t) S_{W_q}(t)\\).\nFor a Wishart matrix \\(W_q\\), the S-transform is explicitly known: \\(S_{W_q}(t)=\\frac{1}{1+qt}\\). So now we have \\(S_E(t) = S_\\Sigma(t) \\frac{1}{1+qt}\\).\nNext, we leverage the t-transform, which is related to the Stieltjes transform \\(\\mathfrak{g}_E(z)\\), of \\(E\\) by: \\(t(z)=z\\mathfrak g_E(z)-1\\). The S-transform is (by definition): \\(S_\\Sigma(t)= \\frac{1+t_\\Sigma(z)}{t_\\Sigma(z)}\\frac{z}{t_\\Sigma(z)}\\). Plugging this in:\n\\(S_E(t) = \\frac{1+t_E(z)}{t_E(z)}\\frac{z}{t_E(z)} = \\frac{1+t_\\Sigma(z)}{t_\\Sigma(z)}\\frac{z}{t_\\Sigma(z)} \\frac{1}{1+qt_E(z)}\\).\nSimplifying the equality involving the right two terms, this becomes: \\(t_E(z) = t_\\Sigma\\bigg( \\frac{t_E(z)}{1+qt_E(z)}\\bigg)\\). Plugging back in the definition of the t-transform:\n\\(z\\mathfrak g_E(z)-1 = Z\\mathfrak g_\\Sigma(Z)-1\\), where \\(Z=\\frac{t_E(z)}{1+qt_E(z)}\\) (from the argument in \\(t_\\Sigma\\) above).\nSo \\(\\mathfrak g_E(z) = \\frac{1}{z}\\frac{t_E(z)}{1+qt_E(z)}\\mathfrak g_\\Sigma(Z)\\). Letting \\(Y(z)=Z(z)/z=\\frac{1}{z}\\frac{t_E(z)}{1+qt_E(z)}\\), we finally get \\(\\mathfrak g_E(z) = Y(z)\\mathfrak g_\\Sigma(Z(z))\\).This is weaker than the original claim from the book that the resolvent matrices are related by this subordination relationship, but it’s sufficient to prove the next part.\n\n\nPlugging this into the expression above:\n\\[\n\\begin{aligned}\n\\tau(\\Sigma G_E(z))=Y\\tau(\\Sigma G_\\Sigma(Z))&=Y\\tau\\bigg((\\Sigma-Z\\mathbf I_d + Z \\mathbf I_d)(Z \\mathbf I_d - C)^{-1}\\bigg) \\\\\n&=YZ\\mathfrak g_\\Sigma(Z)-Y \\\\\n&= Z\\mathfrak g_E(z) -Y(z)\n\\end{aligned}\n\\]\nThis is an expression that does not expliticly depend on \\(\\Sigma\\) anymore.\nThis means we have a general form for the multiplicative RIE eigenvalues that doesn’t depend on anything more than the multiplicative form of the matrix \\(E\\) and freeness:\n\\[\n\\begin{aligned}\n\\xi_k &= \\frac{1}{\\pi\\rho_E(\\lambda_k)}\\lim_{\\eta \\rightarrow 0^+} Z(z_k)\\mathfrak g_E(z_k) -Y(z_k), & z_k := \\lambda_k-i\\eta\n\\end{aligned}\n\\]\nNow, all of the quantities on the right hand side can be estimated from data. For our specific case of Wishart noise, it’s appealing to use the subordination relation of (13.46) using the T-matrix, which is an alternative form of the resolvent, and not to be confused with the T transform: \\(T_E(z) = zG_E(z)-I\\). The T-transform has its own subordination relation for multiplication of this form, making it a little easier to deal with due to the leading \\(z\\) term, and a wonder why we didn’t start with that in the first place… \\(T_E(z)=T_\\Sigma[zS_W(t_E(z))]\\). Rewriting (19.29) using this:\n\\[\n\\begin{aligned}\n\\xi_k &=\\frac{1}{\\pi\\rho_E(\\lambda_k)}\\lim_{\\eta \\rightarrow 0^+} \\Im \\bigg[\\tau(\\Sigma G_E(\\lambda_i - i\\eta))\\bigg] \\\\\n&= \\frac{1}{\\pi\\rho_E(\\lambda_k)}\\lim_{\\eta \\rightarrow 0^+} \\Im \\bigg[\\tau(\\Sigma \\frac{1}{z}T_\\Sigma[zS_W(t_E(z))])\\bigg]\\\\\n&=\\frac{\\lim_{\\eta \\rightarrow 0^+} \\Im \\bigg[\\tau(\\Sigma T_\\Sigma[zS_W(t_E(z))])\\bigg]}{\\lim_{\\eta \\rightarrow 0^+} \\Im t_E(z)} & z:=\\lambda -i\\eta\n\\end{aligned}\n\\]\nHopefully you’ve been reading the book carefully at this point, since this derivation is left out but uses the not-so-obvious special T-version of the Sokhotski-Plemelj formula: \\(\\lim_{\\eta \\rightarrow 0^+} \\Im t(x-i\\eta) = \\pi x \\rho(x)\\) found in (11.90)\nSimplifying the numerator, using the fact that \\(T_\\Sigma(z)=\\Sigma G_\\Sigma(z) =\\Sigma(zI-\\Sigma)^{-1}\\) (11.91):\n\\[\n\\begin{aligned}\n\\tau(\\Sigma T_\\Sigma[zS_W(t_E(z))]) &= \\tau(\\Sigma^2(zS_W(t)I-\\Sigma)^{-1})\\\\\n&= \\tau(\\Sigma(\\Sigma - zS_W(t)I+zS_W(t)I)(zS_W(t)I-\\Sigma)^{-1})\\\\\n&= \\tau(\\Sigma)+zS_W(t)t_\\Sigma(zS_W(t)) \\\\\n&= \\tau(\\Sigma)+zS_W(t)t_E(z)\\\\\n&\\implies \\Im[\\tau(\\Sigma)+zS_W(t)t_E(z)] = \\Im[zS_W(t)t_E(z)]\n\\end{aligned}\n\\]\nFINALLY, putting all of this together:\n\\[\n\\begin{aligned}\n\\xi_k\n&=\\frac{\\lim_{\\eta \\rightarrow 0^+} \\Im \\bigg[\\tau(\\Sigma T_\\Sigma[zS_W(t_E(z))])\\bigg]}{\\lim_{\\eta \\rightarrow 0^+} \\Im t_E(z)} & z:=\\lambda -i\\eta \\\\\n&= \\lambda \\frac{\\lim_{\\eta \\rightarrow 0^+} \\Im[S_W(t)t_E(z)]}{\\lim_{\\eta \\rightarrow 0^+} \\Im t_E(z)}\n\\end{aligned}\n\\]\nThis is true for any multiplicative noise process where the noise has the S-transform described by \\(S_W(z)\\) and the estimated properties of \\(E\\) are gleaned from the sample covariance matrix.\nReturning to our original assumptions, if \\(W=W_q\\) is a Wishart matrix, then \\(S_{W_q}=(1+qt)^{-1}\\). In the bulk region \\(\\lambda_- &lt; \\lambda &lt; \\lambda_+\\) of the eigenvalue spectrum, \\(t=t_E(z)\\) is complex with nonzero imaginary part.\nNext, observet that \\(\\Im\\bigg[\\frac{t}{1+qt}\\bigg]=\\Im\\bigg[\\frac{t(1+qt^*)}{|1+qt|^2}\\bigg]=\\Im\\bigg[\\frac{t+q|t|^2}{|1+qt|^2}\\bigg]=\\frac{1}{|1+qt|^2}\\Im t\\)\nPlugging this in:\n\\[\n\\begin{aligned}\n\\xi(\\lambda) &= \\lambda \\frac{\\lim_{\\eta \\rightarrow 0^+} \\Im[\\frac{t}{1+qt}]}{\\lim_{\\eta \\rightarrow 0^+} \\Im t} \\\\\n&= \\frac{\\lambda}{|1+qt_E(\\lambda-i\\eta)|^2}\\bigg|_{\\eta \\rightarrow 0^+}\\\\\n&= \\frac{\\lambda}{|1+-q+q\\lambda\\mathfrak g_E(\\lambda-i\\eta)|^2}\\bigg|_{\\eta \\rightarrow 0^+}\n\\end{aligned}\n\\]\nThat’s it."
  },
  {
    "objectID": "sub_gaussian.html",
    "href": "sub_gaussian.html",
    "title": "Sub-Gaussian Stable Distribution Derivation",
    "section": "",
    "text": "This comes from the book “Stable Non-Gaussian Random Processes” by Samorodnitsky and Taqqu.\nProposition 2.5.2: The sub-Gaussian \\(\\alpha\\)-stable random vector \\(X\\) defined as \\(X = (A^{1/2}G_1, A^{1/2}G_2, \\dots, A^{1/2}G_d)\\) with zero-mean Gaussian vector \\(G = (G_1, G_2, \\dots, G_d)\\) and \\(A \\sim S_{\\alpha/2}((\\cos\\frac{\\pi \\alpha}{4})^{2/\\alpha}, 1, 0)\\) independent of \\(G\\), has characteristic function:\n\\[\n\\mathbb E[\\exp\\{i\\theta^{\\text T}X\\}]=\\exp\\left(-\\bigg|\\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d \\theta_i \\theta_j R_{ij}\\bigg|^{\\alpha/2}\\right) = \\exp(-(\\frac{1}{2}\\theta^{\\text T}\\Sigma\\theta)^{\\alpha/2})\n\\]\nProof:\nFirst, note that by construction, the Laplace transform of \\(A\\), \\(\\mathbb E \\exp(-t A) = \\exp(-t^{\\alpha/2})\\) for \\(t \\geq 0\\). (See Prop 1.2.12 for why this is true, and the more general Laplace transform as a function of the scale parameter).\nThen this is seen easily by conditioning the characteristic function of the proposed random variable on A:\n\\[\n\\begin{aligned}\n\\mathbb E[\\exp\\{i\\theta^{\\text T}X\\}] &= \\mathbb  E[\\exp\\{i\\theta^{\\text T}A^{1/2}G\\}] \\\\\n&= \\mathbb E_A\\mathbb E_G \\exp[i\\theta^{\\text T}A^{1/2}G|A] &\\text{(Conditioning on A)}\\\\\n&= \\mathbb E_A\\exp[-\\frac{1}{2}A\\theta^{\\text T}\\Sigma\\theta] &\\text{(Gaussian CF)}\\\\\n&= \\exp(-(\\frac{1}{2}\\theta^{\\text T}\\Sigma\\theta)^{\\alpha/2}) &\\text{(Laplace transform of } A)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "normalized_rescaling.html",
    "href": "normalized_rescaling.html",
    "title": "Normalized Projective Estimator",
    "section": "",
    "text": "Procedure\nThe procedure for the normalized projective estimator is as follows. Given a matrix of (stable) observations \\(\\mathbf X = [X_{*,1}, \\dots, X_{*,d}] \\in \\mathbb R^{n \\times d}\\) with each row representing a distinct realization and each of the \\(d\\) columns a dimension of the dataset:\n\nEstimate marginal parameters of each column \\(X_{*,j}\\) \\(j=1, \\dots, d\\) via univariate stable MLE.\nCenter each margin by subtracting its location paramter estimate: \\(X_{*,j}^{(c)} = X_{*,j} - \\mu_j^{MLE}\\)\nNormalize each observation by its Euclidean norm: \\(\\tilde X_{i,*} = X_{i,*}^{(c)}/\\|X_{i,*}^{(c)}\\|\\), \\(i = 1, \\dots, n\\)\nRescale the data by the square root of the sum of the latent marginal variances (aka the the sum of the squared scale paramters in the stable parameterization scaled by \\(\\sqrt 2\\) to match conventional Gaussian parameterization). \\(\\tilde{\\mathbf G} = \\sqrt d \\bar \\sigma\\tilde{ \\mathbf  X}\\), where \\(\\bar \\sigma = \\bigg(\\frac{2}{d}\\sum_{j=1}^d  {\\gamma_j^{MLE}}^2\\bigg)^{1/2}\\).\nThe dispersion matrix estimate is the sample covariance matrix of \\(\\tilde{\\mathbf G}\\): \\(\\hat \\Sigma = \\frac{1}{n}\\tilde{\\mathbf G}^{\\top}\\tilde{\\mathbf G}\\)\n\nNote that this doesn’t rely on the fact that \\(X\\) is stable, and would hold for any so-called “Gaussian scale mixture/scale mixture of normals/normal variance-mean mixture”, including a multivariate t distribution and generalized hyperbolic distribution, as long as we can estimate marginal scale and location paramters. These might provide some interesting opportunities to compare this estimator against more common and nicely-behaved estimators with closed forms, etc.\nIn special cases (like if we can already assume \\(\\mu\\)=0, and want to estimate the correlation matrix so that \\(\\bar \\sigma = 1\\)), we don’t even need to estimate the marginal distributions.\n\n\nBenchmarking vs. Rotated MLE\nI tested this estimator against prior aproaches as well as the “Rotated MLE” approach of estimating eigenvectors using the normalized sample covariance matrix and then rotating to the eigenbasis to estimate the eigenvalues along the coordinate axes via stable MLE. The parameters used were:\nn_samples = [100, 200, 500, 1000, 2500, 5000] \ndimensions = [2, 5, 10, 50, 75, 150, 250, 300] \nalphas = [0.5, 1.0, 1.5, 1.9] \nInterestingly, these two approaches perform extremely similarly:\n\nBoth approaches provide a huge improvement over state-of-the-art MLE. And both show convergence to the latent Gaussian sample covariance matrix:\n\nThe Normalized Projective approach does seem to edge out Rotated MLE in most cases, however. Comparing the two approaches, the eigenvectors of the two estimates will always be identical (since they’re both based on normalization), so the differences boil down to eigenvalue estimation. Stable MLE is a bit of a black box, so it’s possible that if error in scale estimates is increasing in the magnitude of the scale but very nonlinear, doing estimation along the principal axes makes these estimates more error-prone because the error increase in the largest eigenvalue estimate is more than the error reduction in the smallest. Since the projective method estimates scale along the original coordinate axes, these are arbitrary but perhaps lower error. I’m not convinced this is actually what’s going on since I’d expect this edge to disappear as \\(\\alpha\\) increases.\nNevertheless, this led to the idea – “Rotated Projective” – of rotating to a new basis with approximately equal scale in all coordinate directions (this should be the minimum error estimate). Since the projective approach only needs to estimate \\(\\text{Tr}(\\Sigma)\\), which is invariant under rotations, this approach should work reasonably well, even if the rotation isn’t optimal.\nIn simulation, the Rotated Projective Estimator does seem to edge out the others (at least in total error), but not by much (this advantage is so small that on the plots above, it covers the projective approach). The benefits of this approach seem to disappear in the spiked covariance model.\nWhen the covariance matrix is spiked, the projective method is disadvantaged when the largest eigenvalues are dominant relative to the total variance of the data. The workaround is that we can work in a higher ambient dimension and only retain the relevant submatrix of the larger full-dimensional covariance matrix.\nTo test this, I used the covariance matrix from our internal U.S. equity factor model, and combined it with estimated factor exposures and specific risk with 6,422 dimensions (factors plus individual securities). Because specific risk is uncorrelated by assumption and amounts to approximately 60% of any security’s total variance, working in a higher dimensional ambient space guarantees that we diminish the effects of larger eigenfactors, and get concentration of measure to our projected Gaussian. We want to retain only some subset of this ambient dimension (e.g. the factors themselves). The results of some simple/early testing are below, and are consistent with the uniform case:\n\nAs expected, we also get convergence to the latent Gaussian sample covariance matrix even when n is small relative to d:\n\nAs the sample size increases, the performance gap gets closed relative to the projective method. Likewise, rotated MLE outperforms the projective method when the covariance matrix is spiked but the ambient dimension is low. Practically speaking, these aren’t regimes we’re operating in, but these are the limitations of this approach. We may want to talk about both projective and rotated MLE, since both give convergence guarantees to the latent Gaussian SCM but rotated MLE may be preferable if we’re working in relatively low dimensions with many observations.\nWe may also be able to create artificial observations that help acheive the concentration of measure effects that make the projection method work. The key is to have linearly independent components with known moments. If we can do some nonlinear transformation to make new columns with linearly independent noise, these random variables can be arbitrary as long as they’re subgaussian.\n\n\nExplanation\nLet {\\(X^{(d)}\\)} be a sequence of centered random vectors where \\(X^{(d)} \\in \\mathbb R^d\\), having corresponding dispersion matrices {\\(\\Sigma^{(d)}\\)}, \\(d \\geq 1\\) such that:\n\nThe leading principal \\(k \\times k\\) submatrix of \\(\\Sigma^{(d)}\\) equals \\(\\Sigma^{(k)}\\) for all \\(k \\leq d\\).\nFor any d, \\(\\Sigma^{(d)} \\in \\mathbb R^{d \\times d}\\) is well-behaved in the sense that it is positive definite and its eigenvalues \\(\\lambda_i\\) \\(i=1, \\dots d\\) are uniformly bounded so that there exist constants \\(m, M &gt;0\\) independent of \\(d\\) such that \\(m \\leq \\lambda_i \\leq M\\) for all \\(i\\). Additionally, assume that the average trace converges to a fixed constant as \\(d \\rightarrow \\infty\\).\n\nUsing the subordinator representation, we can write \\(X=\\sqrt W G\\) in the usual way with \\(W \\in \\mathbb R^+\\) a maximally skewed positive \\(\\alpha/2\\)-stable subordinator and \\(G \\sim N(0, \\Sigma)\\) independent of W.\nFrom the eigendecomposition of \\(\\Sigma\\), we can write \\(\\Sigma = O\\Lambda O^\\top = O\\Lambda^{1/2}\\Lambda^{1/2} O^\\top\\). Call \\(\\Sigma^{1/2} :=O\\Lambda^{1/2}\\). Let \\(Z \\sim N(0, I_d)\\) so that \\(G = \\Sigma^{1/2}Z\\).\nThen we can write:\n\\[\n\\begin{aligned}\n\\frac{X}{\\|X\\|} = \\frac{\\sqrt WG}{\\|\\sqrt WG\\|} = \\frac{\\sqrt WG}{\\|\\sqrt W\\Sigma^{1/2}Z\\|} &= \\frac{\\sqrt WG}{\\sqrt W\\|O\\Lambda^{1/2}Z\\|}\\\\\n&= \\frac{G}{\\|\\Lambda^{1/2}Z\\|}\n\\end{aligned}\n\\]\nDefine the normalized trace from (Potters & Bouchaud, 2020) as \\(\\tau(\\mathbf A) := \\frac{1}{d}\\mathbb E[\\text{Tr}(\\mathbf A)]\\). Then for a covariance matrix as described above, \\(\\tau(\\Sigma)\\) is finite and \\(m \\leq \\tau(\\Sigma) \\leq M\\). Note that we can also interpret the normalized trace as the average variance of the components of \\(G\\), \\(\\bar{\\sigma}^2\\) from the procedure described above.\nLooking more closely at the norm from the expression above:\n\\[\n\\begin{aligned}\n\\frac{1}{d}\\|\\Lambda^{1/2}Z\\|^2 & = \\frac{1}{d} \\sum_{i=1}^d \\lambda_i Z_i^2 \\\\\n&= \\frac{1}{d}\\sum_{i=1}^d (\\lambda_i Z_i^2 - \\lambda_i + \\lambda_i)\\\\\n&= \\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)+ \\frac{1}{d}\\sum_{i=1}^d \\lambda_i \\\\\n&=\\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)+ \\frac{1}{d}\\text{Tr}(\\Sigma) \\\\\n&= \\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)+ \\bar{\\sigma}^2\\\\\n&\\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} 0+ \\bar{\\sigma}^2\\\\\n\\end{aligned}\n\\]\nThe argument that \\(\\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)\\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} 0\\) follows from Kolmogorov’s Criterion for the SLLN with independent but non-identically distributed random variables, since the sufficient conditions \\(Y_i:= \\lambda_i(Z_i^2 - 1)\\), \\(\\mathbb E Y_i=0\\) and \\(\\sum_i^\\infty \\text{Var}(Y_i)/i^2 = \\sum_i^\\infty 2\\lambda_i^2/i^2 \\leq \\sum_i^\\infty 2M^2/i^2 =2M^2\\frac{\\pi^2}{6}&lt;\\infty\\) are met.\nTaking square roots and applying the continuous mapping theorem, we get \\(\\frac{1}{d}\\|\\Lambda^{1/2}Z\\|^2 \\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} \\bar{\\sigma}^2 \\implies \\frac{\\|\\Lambda^{1/2}Z\\|}{\\sqrt d \\bar{\\sigma}}\\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} 1\\).\nNote for any fixed indices \\(i,j\\), the bivariate vector elliptical vector \\((X_i, X_j)\\) doesn’t depend on \\(d\\), nor does the corresponding latent Gaussian bivariate vector \\((G_i, G_j)\\) – (the same is true for arbitrary fixed dimension \\(k\\)). In other words, the components are the marginal distributions, and don’t change by adding more dimensions to the vector.\nPutting this all together, we get:\n\\[\n\\begin{aligned}\n\\sqrt d \\bar \\sigma \\frac{(X_i, X_j)}{\\|X\\|} =\\sqrt d  \\bar \\sigma \\frac{(G_i, G_j)}{\\|\\Lambda^{1/2}Z\\|}  \\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} (G_i, G_j)\n\\end{aligned}\n\\]\nNow, consider that for the full vector \\(G\\), we can write: \\[\n\\mathbb E [GG^\\top] =\n\\begin{pmatrix}\n\\mathbb{E}[G_1^2] & \\mathbb{E}[G_1G_2] & \\cdots & \\mathbb{E}[G_1G_d] \\\\\n\\mathbb{E}[G_2G_1] & \\mathbb{E}[G_2^2] & \\cdots & \\mathbb{E}[G_2G_d] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}[G_dG_1] & \\mathbb{E}[G_dG_2] & \\cdots & \\mathbb{E}[G_d^2]\n\\end{pmatrix}\n\\]\nSo that we can recover \\(\\mathbb E [GG^\\top] =\\Sigma\\) without ever looking at more than two components of \\(G\\) at a time. While the matrix \\(\\Sigma\\) clearly grows with \\(d\\), each entry is independent of the ambient dimension.\nNote that \\((\\frac{Z_i}{\\|Z\\|})^2\\) \\(i=1, \\dots, d\\) is uniformly integrable, since in the uncorrelated case, a uniform random variable on the sphere \\(\\mathbb S^{d-1}\\) must satisfy \\(\\sum_{i=1}^d (\\frac{Z_i}{\\|Z\\|})^2=1\\) and by symmetry \\(\\mathbb E(\\frac{Z_i}{\\|Z\\|})^2=\\frac{1}{d}\\). So \\(\\mathbb E(\\sqrt d \\frac{Z_i}{\\|Z\\|})^2=1\\) and doesn’t depend on d. Since the eigenvalues of the covariance matrix are uniformly bounded and independent of \\(d\\), \\((\\frac{\\sqrt d X_i}{\\|X\\|})^2\\overset{\\text{a.s.}}{=}(\\frac{\\sqrt d G_i}{\\|G\\|})^2\\) are also uniformly integrable.\nThen for any fixed \\(i,j \\leq d\\), \\((\\frac{\\sqrt d X_i}{\\|X\\|}\\frac{\\sqrt d X_j}{\\|X\\|})\\) is also uniformly integrable (since it’s bounded above by \\(\\max\\{(\\frac{\\sqrt d X_i}{\\|X\\|})^2,(\\frac{\\sqrt d X_j}{\\|X\\|})^2\\}\\), and the following interchange of limits and expectation is warranted. : \\[\n\\lim_{d\\to\\infty} \\mathbb{E}\\!\\left[d\\,\\bar{\\sigma}^2\\,\\frac{X_i}{\\|X\\|}\\frac{X_j}{\\|X\\|}\\right] = \\mathbb{E}\\!\\left[ \\bar{\\sigma}^2\\,\\lim_{d\\to\\infty}d\\,\\frac{X_i}{\\|X\\|}\\frac{X_j}{\\|X\\|}\\right] = \\mathbb{E}[G_iG_j] = \\Sigma_{i,j}\n\\]\nSince this quantity doesn’t depend on \\(d\\), this shows that \\[\nd\\,\\bar{\\sigma}^2\\,\\mathbb{E}\\!\\left[\\frac{X}{\\|X\\|}\\frac{X}{\\|X\\|}^\\top\\right] \\to \\mathbb{E}[GG^\\top] = \\Sigma\n\\]\nin the sense that every finite block converges entrywise.\n\n\nError Bounds\nThe fact that the denominator in the normalized vector simplifies to a Gaussian vector norm (almost surely) means that we should be able to apply standard quadratic form conentration bounds (e.g. Hanson-Wright) to determine the impact of this Gaussian approximation in finite dimensions (e.g. how close can we get to recovering the sample covariance matrix of the latent Gaussian vector).\nIf we’re interested in how much error this approximation adds to the process, a reasonable thing to look at might be the probability of a large distance between the estimator versus the latent Gaussian sample covariance matrix, \\(\\Sigma_G\\) (if it were observable). This simplifies to:\n\\[\n\\begin{aligned}\n\\mathbb P\\bigg(\\bigg \\| d\\bar\\sigma^2 \\frac{XX^\\top}{\\|X\\|^2} - GG^\\top\\bigg \\|_F \\geq t \\bigg) & = \\mathbb P\\bigg(\\bigg \\| d\\bar\\sigma^2 \\frac{GG^\\top}{\\|G\\|^2} - GG^\\top\\bigg \\|_F \\geq t \\bigg) \\\\\n&= \\mathbb P\\bigg(\\bigg \\| GG^\\top \\bigg( \\frac{d\\bar\\sigma^2}{\\|G\\|^2} - 1 \\bigg) \\bigg \\|_F \\geq t \\bigg) \\\\\n&= \\mathbb P\\bigg( \\| GG^\\top \\|_F \\bigg| \\frac{d\\bar\\sigma^2}{\\|G\\|^2} - 1 \\bigg|  \\geq t \\bigg) \\\\\n&= \\mathbb P\\bigg( \\| G \\|^2 \\bigg| \\frac{d\\bar\\sigma^2}{\\|G\\|^2} - 1 \\bigg|  \\geq t \\bigg) \\\\\n&= \\mathbb P( | \\|G\\|^2 - d \\bar\\sigma^2 |  \\geq t ) \\\\\n&= \\mathbb P( | \\|\\Sigma^{1/2}Z\\|^2 - d \\bar\\sigma^2 |  \\geq t ) \\\\\n&= \\mathbb P( | (\\Sigma^{1/2}Z)^\\top(\\Sigma^{1/2}Z) - d \\bar\\sigma^2 |  \\geq t ) \\\\\n&= \\mathbb P( | Z^\\top \\Sigma Z - d \\bar\\sigma^2 |  \\geq t ) \\\\\n&\\leq 2\\exp\\bigg[-c \\min\\bigg(\\frac{t^2}{\\|\\Sigma\\|_F^2}, \\frac{t}{\\|\\Sigma\\|_{\\text{op}}}\\bigg)\\bigg]\n\\end{aligned}\n\\]\nfor some constant \\(c&gt;0\\). The last line is a direct plug of the Hanson-Wright inequality.\n\n\nEfficiency\nThe projective estimator becomes a superefficient estimator of the dispersion matrix of a stable random vector when \\(d \\rightarrow \\infty\\). This is because the Gaussian has the lowest CRLB for its scale parameter in the class of stable distributions:\n\nThis isn’t actually an obvious result, since the Gaussian was thought by many to be the largest of all Cramér-Rao bounds (CRB) for many classes of similarly parameterized distributions (as claimed in B.3.26 in Spectral Analysis of Signals by Stoica and Moses), a fact disproven in Abeida and Delmas (2019). For instance, this is also a quality of the multivariate-t when variance is finite, and potentially extends to other distributions. It’s not obvious how to extend this analytically to stable distributions, since the second moments needed for the subordinator (aka modular variate) aren’t met.\nIn Dulek (2017), the author derives characteristic function-based lower bounds for Fisher information in the stable case. This could provide an analtyical argument for the superefficiency of this estimator.\nThe normalized projective estimator attains the Cramér-Rao Lower Bound (CRLB) for the underlying Gaussian covariance matrix estimate in the limit. And since the Gaussian has the lower CRLB in the class of elliptical distributions parameterized by a shape matrix, this estimator is superefficient for our stable dispersion matrix estimate.\nWhile the distribution of \\(d\\bar\\sigma^2 X_iX_j/\\|X\\|^2\\) doesn’t have closed form moments, the variance of this estimator can be analyzed via its Taylor expansion.\n\nGaussian CRB\nAssume like in our assumptions above that \\(G \\sim N(0, \\Sigma)\\). Then the Slepian-Bangs formula gives the (i,j)th element of the Fisher Information Matrix for the Gaussian as \\([F]_{i,j} = N\\cdot \\text{Tr}\\{{\\Sigma}^{-1}\\frac{\\partial{\\Sigma}}{\\partial \\theta_i}{\\Sigma}^{-1}\\frac{\\partial{\\Sigma}}{\\partial \\theta_j}\\}\\).\nLooking elementwise, the CRB for the (i,j)-th entry of the covariance matrix is \\(\\text{CRB}(\\Sigma_{ij}) = \\frac 1 n [\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2]\\)\n\n\nProjective Estimator\n[Note: Need to change X to G to be consistent with the earlier notation]\nCall the estimate of Using a Talor series expansion for the expectation and variance of the ratio \\(\\hat\\Sigma_{ij} = d\\hat\\sigma^2 X_iX_j/\\|X\\|^2\\) around \\((\\mathbb E(d\\hat\\sigma^2 G_iG_j), \\mathbb E(\\|G\\|^2))=(d\\bar\\sigma^2\\text{Cov}(G_i,G_j), d\\bar\\sigma^2)\\):\n\\[\n\\begin{aligned}\n\\mathbb E (d\\bar\\sigma^2 X_iX_j/\\|X\\|^2) &=\\mathbb E (d\\bar\\sigma^2 G_iG_j/\\|G\\|^2) = \\mathbb E \\bigg [  \\bigg] \\\\\n&= \\Sigma_{ij}+B_{ij}\n\\end{aligned}\n\\]\nWhere \\(B_{ij} =  \\mathcal O(\\frac{1}{d^\\beta \\sqrt{n}})\\) with \\(0 &lt; \\beta \\leq \\frac{1}{2}\\).\nFor the variance:\n\\[\n\\begin{aligned}\n\\text{Var}(d\\bar\\sigma^2 G_iG_j/\\|G\\|^2) &= \\\\\n& = \\Sigma_{ii}\\Sigma_{jj}+\\Sigma{ij}^2\n\\end{aligned}\n\\]\nUsing this, and defining \\(\\Omega\\) as the CRLB matrix. Choose \\(m=m(d)\\) so that \\(\\frac{m(d)}{d} &lt; \\mathcal O(1/d^2)\\).\nThen assuming:\n\nFinite variance of univariate stable scale MLEs (DuMouchel’s criteria)\nWe are estimating the covariance matrix of a sub-vector, \\(X^{(m)}\\) where \\(i,j \\leq m\\)\n\\(\\mathcal O(n) =\\mathcal O(m)\\)\n\n\\[\n\\begin{aligned}\n\\mathbb E \\|\\hat \\Sigma^{(m)} - \\Sigma^{(m)}  \\|_F^2 &= \\mathbb E \\bigg \\| d\\hat\\sigma^2 \\frac{X^{(m)} X^{(m)\\top}}{\\|X\\|^2} - \\Sigma^{(k)}  \\bigg\\|_F^2 \\\\\n&= \\mathbb E \\bigg \\| d\\hat\\sigma^2 \\frac{G^{(m)}G^{(m)\\top}}{\\|G\\|^2} - \\Sigma \\bigg\\|_F^2 \\\\\n&= \\mathbb E \\sum_{i,j} (d\\hat\\sigma^2 G_iG_j/\\|G\\|^2 - \\Sigma_{ij})^2 \\\\\n&= \\sum_{i,j} \\text{Var}(d\\hat\\sigma^2 G_iG_j/\\|G\\|^2)+ \\sum_{i,j} \\mathbb E (\\Sigma_{ij} -\\hat \\Sigma_{ij})^2\\\\\n&= \\sum_{i,j} \\text{Var}(d\\hat\\sigma^2 G_iG_j/\\|G\\|^2)+\\sum_{i,j}\\mathcal O (\\frac{1}{d^{\\beta/2}}) \\\\\n&= \\sum_{i,j}\\text{Var}(G_iG_j) +\\sum_{i,j}\\mathcal O(\\frac{1}{d^\\beta \\sqrt{n}}) \\\\\n&= \\sum_{i,j}\\frac 1 n [\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2]+\\mathcal O(\\frac{1}{d^\\beta \\sqrt{n}}) \\\\\n&= \\text{Tr}(\\Omega) + \\mathcal O(\\frac{1}{d^\\beta \\sqrt{n}}) \\\\\n\\implies  \\frac{\\mathbb E \\|\\hat \\Sigma - \\Sigma \\|_F^2}{\\text{Tr}(\\Omega)} &= 1 + \\mathcal O(\\frac{1}{d^{1+\\beta } \\sqrt{n}}) \\\\\n\\end{aligned}\n\\]\nWhen the ambient dimension grows sufficiently faster than the estimation dimension, no unbiased estimator can have a strictly smaller total MSE in the limit.\n\nadd assumptions\nfix (a,b) expansion details for sigma hat"
  },
  {
    "objectID": "normalized_rescaling.html#bounds-on-error",
    "href": "normalized_rescaling.html#bounds-on-error",
    "title": "Normalized Rescaling Estimator",
    "section": "Bounds on Error",
    "text": "Bounds on Error"
  },
  {
    "objectID": "normalized_rescaling.html#also-sufficient-to-show-that-the-eigenvalues-converge-since-the-eigenvectors-are-alredy-the-same",
    "href": "normalized_rescaling.html#also-sufficient-to-show-that-the-eigenvalues-converge-since-the-eigenvectors-are-alredy-the-same",
    "title": "Normalized Rescaling Estimator",
    "section": "Also sufficient to show that the eigenvalues converge (since the eigenvectors are alredy the same)",
    "text": "Also sufficient to show that the eigenvalues converge (since the eigenvectors are alredy the same)"
  },
  {
    "objectID": "progress.html#normalized-projective-estimator",
    "href": "progress.html#normalized-projective-estimator",
    "title": "Current Progress",
    "section": "Normalized Projective Estimator",
    "text": "Normalized Projective Estimator\nDetails here: Normalized Projective Estimator\n[Last Updated on February 13, 2024]"
  },
  {
    "objectID": "projective_estimator.html",
    "href": "projective_estimator.html",
    "title": "Projective Estimator",
    "section": "",
    "text": "Data Model and Setup\nNote: The notation here is a little confusing – I want to distinguish between the estimation dimension and the ambient dimension, but there’s potentially a better way to do it without introducing so much notation.\nLet \\(\\{X^{(d)}\\}\\) be a sequence of centered random vectors where \\(X^{(d)} \\in \\mathbb R^d\\), having corresponding dispersion/shape matrices \\(\\{\\Sigma^{(d)}\\}\\), \\(d \\geq 1\\) such that:\n\nThe leading principal \\(k \\times k\\) submatrix of \\(\\Sigma^{(d)}\\) equals \\(\\Sigma^{(k)}\\) for all \\(k \\leq d\\).\nFor any \\(d\\), \\(\\Sigma^{(d)} \\in \\mathbb R^{d \\times d}\\) is well-behaved in the sense that it is positive definite and its eigenvalues \\(\\lambda_i\\), \\(i=1, \\dots, d\\) are uniformly bounded so that there exist two constants \\(m, M &gt;0\\) independent of \\(d\\) such that \\(m \\leq \\lambda_i \\leq M\\) for all \\(i\\). Additionally, assume that the average eigenvalue (or normalized trace in a random matrix theory context) converges to a fixed constant: \\(\\frac{1}{d}\\text{tr}(\\Sigma^{(d)})\\rightarrow \\tau&gt;0\\) as \\(d \\rightarrow \\infty\\).\n\nDefine \\(X=X_{[k]}\\) as the first \\(k\\) coordinates of \\(X^{(d)}\\). Then, as defined above for any \\(k \\leq d\\), the dispersion matrix of \\(X_{[k]}\\) is simply \\(\\Sigma^{(k)}\\), irrespective of the ambient dimension \\(d\\).\nAssume that each \\(X\\) has the familiar stochastic representation \\(X= \\sqrt{W}G\\), where \\(W \\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers, and \\(G \\sim N(0, \\Sigma^{(d)})\\), a multivariate normal random vector independent of \\(W\\) with covariance matrix \\(\\Sigma^{(d)}\\).\nUnder this setup, each \\(X \\sim \\alpha\\)-SG(\\(\\Sigma^{(d)}\\)) – an elliptical stable random vector centered at the origin, whose shape matrix is the covariance matrix of the latent Gaussian vector \\(G\\) (e.g. the characteristic function of \\(X\\) is \\(\\mathbb E[\\exp\\{i\\theta^{\\text T}X\\}]= \\exp(-(\\frac{1}{2}\\theta^{\\text T}\\Sigma^{(d)}\\theta)^{\\alpha/2})\\) (Samorodnitsky and Taqqu, Proposition 2.5.2))\nWe observe \\(n\\) such i.i.d. realized vectors via a data matrix \\(\\mathbf X \\in \\mathbb{R}^{n\\times d}\\) so that each row is one \\(d\\)-dimensional observation. We are interested in estimating the shape matrix of \\(X\\) for some \\(k \\leq d\\). In other words, our estimation dimension of interest may be smaller than the ambient dimension of our random vector. Without loss of generality, assume that the leading dimensions are the estimation dimensions.\n\n\n\n\n\n\nNote\n\n\n\nThis \\(X_i = \\sqrt{W}G\\) setup is a so-called normal variance mixture. In the literature, W may be referred to as a subordinator (stable distribution literature), or a texture or a 2nd-order modular variate (signal processing). The distribution of this random variable determines the distribution of \\(X\\). For example if \\(W\\) is inverse-gamma distributed rather than stable, \\(X\\) has a multivariate t distribution. This estimation approach works for any normal variance-mean mixture.\n\n\nSome examples of situations where this comes up include:\n\nInvesting: Estimating the dependence structure for a portfolio of \\(k=50\\) stocks that trade in a market of \\(d=6,500\\) total securities.\nGenomics: Estimating pathway-level dependence across \\(k=100\\) genes from genome-wide expression profiles across \\(d=20,000\\) genes.\n\nRather than discard these additional ambient dimensions (as is typical), because the same subordinator acts on each realization across the whole ambient space, we can use them to project an approximately Gaussian \\(k\\)-vector and get a better estimate of \\(\\Sigma^{(k)}\\) under the right conditions.\n\n\nEstimation Procedure\n\nEstimate marginal scale parameters \\(\\gamma_j^{MLE}\\), \\(j = 1, \\dots, d\\) via maximum likelihood.\n\nUse these marginal estimates to estimate the trace of the full ambient-dimensional latent covariance matrix: \\(\\widehat{\\text{tr}(\\Sigma^{(d)})} = d\\hat\\tau := 2\\sum_{j=1}^d  {\\gamma_j^{MLE}}^2\\) (Note: the 2 prefactor comes from differences between standard parameterizations of a Gaussian vs. general stable random vector used in the STABLE software).\nNormalize each row of \\(\\mathbf X\\) by its Euclidean norm: \\(\\tilde X_{i,*} = X_{i,*}/\\|X_{i,*}\\|\\), \\(i = 1, \\dots, n\\)\nRetain just the estimation dimensions: \\(\\tilde{\\mathbf G} :=  \\mathbf{\\tilde{X}_{[k]}} \\in \\mathbb R^{n \\times k}\\).\nThe dispersion matrix estimate is the sample covariance matrix of \\(\\tilde{\\mathbf G}\\), rescaled by the estimated trace of the full-ambient-dimension latent covariance matrix: \\(\\hat \\Sigma = \\frac{d\\hat\\tau}{n}\\tilde{\\mathbf G}^{\\top}\\tilde{\\mathbf G}\\)\n\n\n\nTheoretical Properties\nReturning to the representation \\(X = \\sqrt{W}G\\), the normalized stable vector \\(X\\) is equal to the normalized version of the corresponding latent Gaussian vector \\(G\\).\nTo see why, note that from the eigendecomposition of \\(\\Sigma\\), we can write \\(\\Sigma = O\\Lambda O^\\top = O\\Lambda^{1/2}\\Lambda^{1/2} O^\\top\\). Call \\(\\Sigma^{1/2} :=O\\Lambda^{1/2}\\). Let \\(Z \\sim N(0, I_d)\\) so that \\(G = \\Sigma^{1/2}Z\\).\nThen we can write:\n\\[\n\\begin{aligned}\n\\frac{X}{\\|X\\|} = \\frac{\\sqrt WG}{\\|\\sqrt WG\\|} = \\frac{\\sqrt WG}{\\|\\sqrt W\\Sigma^{1/2}Z\\|} &= \\frac{\\sqrt WG}{\\sqrt W\\|O\\Lambda^{1/2}Z\\|} = \\frac{G}{\\|\\Lambda^{1/2}Z\\|}\n\\end{aligned}\n\\]\nLooking more closely at the norm from the expression above:\n\\[\n\\begin{aligned}\n\\frac{1}{d}\\|\\Lambda^{1/2}Z\\|^2 & = \\frac{1}{d} \\sum_{i=1}^d \\lambda_i Z_i^2 \\\\\n&= \\frac{1}{d}\\sum_{i=1}^d (\\lambda_i Z_i^2 - \\lambda_i + \\lambda_i)\\\\\n&= \\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)+ \\frac{1}{d}\\sum_{i=1}^d \\lambda_i \\\\\n&=\\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)+ \\frac{1}{d}\\text{tr}(\\Sigma^{(d)}) \\\\\n&= \\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)+ \\tau\\\\\n&\\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} 0+ \\tau\\\n\\end{aligned}\n\\]\nThe argument that \\(\\frac{1}{d}\\sum_{i=1}^d \\lambda_i(Z_i^2 - 1)\\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} 0\\) follows from Kolmogorov’s Criterion for the SLLN with independent but non-identically distributed random variables, since the sufficient conditions \\(Y_i:= \\lambda_i(Z_i^2 - 1)\\), \\(\\mathbb E Y_i=0\\) and \\(\\sum_i^\\infty \\text{Var}(Y_i)/i^2 = \\sum_i^\\infty 2\\lambda_i^2/i^2 \\leq \\sum_i^\\infty 2M^2/i^2 =2M^2\\frac{\\pi^2}{6}&lt;\\infty\\) are met.\nTaking square roots and applying the continuous mapping theorem, we get \\(\\frac{1}{d}\\|\\Lambda^{1/2}Z\\|^2 \\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} \\tau \\implies \\frac{\\|\\Lambda^{1/2}Z\\|}{\\sqrt {d \\tau}}\\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} 1\\).\nThis means that for any fixed indices \\(i,j\\), we get:\n\\[\n\\begin{aligned}\n\\sqrt {d \\tau} \\frac{(X_i, X_j)}{\\|X\\|} =\\sqrt {d \\tau}\\frac{(G_i, G_j)}{\\|\\Lambda^{1/2}Z\\|}  \\underset{d \\rightarrow \\infty}{\\overset{\\text{a.s.}}{\\longrightarrow}} (G_i, G_j)\n\\end{aligned}\n\\]\nNow, consider that for the estimation vector \\(G':=G_{[k]}\\), we can write: \\[\n\\mathbb E [G'G'^\\top] =\n\\begin{pmatrix}\n\\mathbb{E}[G_1'^2] & \\mathbb{E}[G_1'G_2'] & \\cdots & \\mathbb{E}[G_1'G_k'] \\\\\n\\mathbb{E}[G_2'G_1'] & \\mathbb{E}[G_2'^2] & \\cdots & \\mathbb{E}[G_2'G_k'] \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\mathbb{E}[G_k'G_1'] & \\mathbb{E}[G_k'G_2'] & \\cdots & \\mathbb{E}[G_k'^2]\n\\end{pmatrix}\n\\]\nSo that we can recover \\(\\mathbb E [G'G'^\\top] =\\Sigma^{(k)}\\) without ever looking at more than two components of \\(G\\) at a time. While the matrix \\(\\Sigma^{(k)}\\) can be allowed to grow with \\(d\\) if we wish, each entry is independent of the ambient dimension.\nNote that \\((\\frac{Z_i}{\\|Z\\|})^2\\), \\(i=1, \\dots, d\\) is uniformly integrable, since in the uncorrelated case, a uniform random variable on the sphere \\(\\mathbb S^{d-1}\\) must satisfy \\(\\sum_{i=1}^d (\\frac{Z_i}{\\|Z\\|})^2=1\\) and by symmetry \\(\\mathbb E(\\frac{Z_i}{\\|Z\\|})^2=\\frac{1}{d}\\). So \\(\\mathbb E(\\sqrt d \\frac{Z_i}{\\|Z\\|})^2=1\\) and doesn’t depend on d. Since the eigenvalues of the covariance matrix are uniformly bounded and independent of \\(d\\), \\((\\frac{\\sqrt {d \\tau} X_i}{\\|X\\|})^2=(\\frac{\\sqrt {d \\tau} G_i}{\\|G\\|})^2\\) are also uniformly integrable.\nThen for any fixed \\(i,j \\leq d\\), \\((\\frac{\\sqrt {d \\tau} X_i}{\\|X\\|}\\frac{\\sqrt {d \\tau} X_j}{\\|X\\|})\\) is uniformly integrable (since it’s bounded above by \\(\\max\\{(\\frac{\\sqrt d X_i}{\\|X\\|})^2,(\\frac{\\sqrt d X_j}{\\|X\\|})^2\\}\\), which is itself uniformly integrable), and the following interchange of limits and expectation is warranted: \\[\n\\lim_{d\\to\\infty} \\mathbb{E}\\!\\left[d\\,\\tau\\,\\frac{X_i}{\\|X\\|}\\frac{X_j}{\\|X\\|}\\right] = \\mathbb{E}\\!\\left[ \\lim_{d\\to\\infty}d\\tau\\,\\frac{X_i}{\\|X\\|}\\frac{X_j}{\\|X\\|}\\right] = \\mathbb{E}[G_iG_j] = \\Sigma_{i,j}\n\\]\nSince this quantity doesn’t depend on \\(d\\), this shows that \\[\nd\\,\\tau\\,\\mathbb{E}\\!\\left[\\frac{X_{[k]}}{\\|X\\|}\\frac{X_{[k]}}{\\|X\\|}^\\top\\right] \\to \\mathbb{E}[G_{[k]}G_{[k]}^\\top] = \\Sigma^{(k)}\n\\]\nin the sense that every finite block converges entrywise.\n\nEfficiency\nIn the Gaussian case, the Slepian-Bangs formula gives the (i,j)th element of the Fisher Information Matrix for the Gaussian as \\([F]_{i,j} = n\\cdot \\text{tr}\\{{\\Sigma}^{-1}\\frac{\\partial{\\Sigma}}{\\partial \\theta_i}{\\Sigma}^{-1}\\frac{\\partial{\\Sigma}}{\\partial \\theta_j}\\}\\).\nThis leads to the familiar Cramér-Rao Bound (CRB) for the (i,j)th element of the inverse of this matrix: (\\(\\text{CRB}(\\Sigma_{ij}) = \\frac 1 n [\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2]\\).\nWhen \\(\\tau\\) is known (e.g. in special cases such as when we can assume equal variance for all dimensions) actual estimator we’re using in the Estimation Procedure section above is (an average of) an Angular Central Gaussian (ACG) random variables (see Tyler, 1987). When \\(\\tau\\) is not known and an estimate must be used, it’s more complicated and the distribution depends on the sample size. In either case, there is no closed form for the variance of an ACG variable of arbitrary dimension and a Taylor series expansion needs to be used.\nWe assume that DuMouchel’s criteria for the asymptotic normality of stable MLEs are satisfied, and that \\(n\\) is sufficiently large that the marginal scale estimators have finite variance. Beyond that, we don’t rely on any limiting arguments about the sample size.\nFor any \\(i,j \\leq k\\):\n\\[\n\\begin{aligned}\n\\mathbb E(d\\hat\\tau G_iG_j) &= \\mathbb E(d\\hat\\tau G_iG_j - d\\tau G_iG_j +  d\\tau G_iG_j ) \\\\\n&= \\mathbb E(d\\tau G_iG_j + d(\\hat\\tau - \\tau) G_iG_j) \\\\\n&= d\\tau\\text{Cov}(G_i,G_j) + \\mathbb E[d(\\hat\\tau - \\tau) G_iG_j] \\\\\n&\\leq d\\tau\\Sigma_{ij} +\\sqrt{\\text{Var}(d\\hat\\tau)} \\sqrt{\\mathbb E[(G_iG_j)^2]} \\\\\n&= d\\tau\\Sigma_{ij} + \\sqrt{\\mathcal O (d^{1-\\beta})} \\sqrt{\\mathcal O(1)} \\\\\n&= d\\tau\\Sigma_{ij} + \\mathcal O (d^{(1-\\beta)/2})\n\\end{aligned}\n\\]\nWhere \\(0 &lt; \\beta \\leq 1\\) is a term that depends on the correlation of the marginal MLEs (since \\(\\tau\\) is the average of finite variance random variables that cannot be perfectly correlated). So the bias term is \\(\\mathcal O (d^{(1-\\beta)/2})\\) or smaller.\nUsing a Taylor series expansion for the expectation and variance of the ratio \\(\\hat\\Sigma_{ij} = d\\hat\\tau X_iX_j/\\|X\\|^2\\) around \\((\\mathbb E(d\\hat\\tau G_iG_j), \\mathbb E(\\|G\\|^2))=(d\\tau\\Sigma_{i,j}+ \\mathcal O (d^{(1-\\beta)/2}), d\\tau)\\), starting with the moments:\n\\[\n\\begin{aligned}\n\\mathbb E (d\\hat\\tau X_iX_j/\\|X\\|^2) =\\mathbb E (d\\hat\\tau G_iG_j/\\|G\\|^2) &= \\mathbb E [ d\\hat\\tau G_iG_j/\\|G\\|^2 +  d\\tau G_iG_j/\\|G\\|^2 - d\\tau G_iG_j/\\|G\\|^2] \\\\\n&= \\mathbb E [ d\\tau G_iG_j/\\|G\\|^2] + \\mathbb E [d(\\hat\\tau - \\tau) G_iG_j/\\|G\\|^2] \\\\\n&= \\frac{\\mathbb E(d\\hat\\tau G_iG_j)}{\\mathbb E(\\|G\\|^2)}+\\mathcal O (\\frac{1}{d})+ \\mathbb E [d(\\hat\\tau - \\tau) G_iG_j/\\|G\\|^2] \\\\\n&\\leq \\frac{\\mathbb E(d\\hat\\tau G_iG_j)}{\\mathbb E(\\|G\\|^2)}+\\mathcal O (\\frac{1}{d})+ \\sqrt{\\text{Var}(\\hat\\tau)}\\sqrt{\\mathbb E [(dG_iG_j/\\|G\\|^2)^2]}\\\\\n&= \\frac{\\mathbb E(d\\hat\\tau G_iG_j)}{\\mathbb E(\\|G\\|^2)}+\\mathcal O (\\frac{1}{d})+ \\sqrt{\\mathcal O (d^{-\\beta})}\\sqrt{\\mathcal O(1)}\\\\\n&= \\frac{\\mathbb E(d\\hat\\tau G_iG_j)}{\\mathbb E(\\|G\\|^2)}+\\mathcal O (\\frac{1}{d^{\\beta/2}}) \\\\\n&= \\frac{d\\tau\\Sigma_{i,j}+ \\mathcal O (d^{(1-\\beta)/2})}{d\\tau} +\\mathcal O (\\frac{1}{d^{\\beta/2}}) \\\\\n&=  \\frac{d\\tau\\Sigma_{i,j}}{d\\tau} +  \\frac{\\mathcal O (d^{(1-\\beta)/2})}{d\\tau}+\\mathcal O (\\frac{1}{d^{\\beta/2}}) \\\\\n&= \\Sigma_{ij}+\\mathcal O (\\frac{1}{d^{\\beta/2}})\n\\end{aligned}\n\\]\nAnother building block:\n\\[\n\\begin{aligned}\n\\mathbb E [ (d\\hat\\tau G_iG_j/\\|G\\|^2)^2] &= \\mathbb E [ (d\\hat\\tau G_iG_j/\\|G\\|^2 +  d\\tau G_iG_j/\\|G\\|^2 - d\\tau G_iG_j/\\|G\\|^2)^2] \\\\\n&= \\mathbb E \\bigg[ \\bigg(d\\tau G_iG_j/\\|G\\|^2 +d(\\hat\\tau - \\tau) G_iG_j/\\|G\\|^2\\bigg)^2\\bigg] \\\\\n&=  \\mathbb E \\bigg[\\bigg(\\frac{d\\tau G_iG_j}{\\|G\\|^2}\\bigg)^2\\bigg] + \\mathbb E \\bigg[\\frac{2d^2\\tau(\\hat\\tau - \\tau)G_i^2G_j^2}{\\|G\\|^4} +\\bigg(\\frac{d(\\hat\\tau - \\tau) G_iG_j}{\\|G\\|^2}\\bigg)^2\\bigg] \\\\\n&= \\bigg[\\frac{d^2\\tau^2\\Sigma_{ij}^2}{d^2\\tau^2} + \\frac{d^2\\tau^2\\text{Var}(G_iG_j)}{d^2\\tau^2}+ \\mathcal O(\\frac 1 d )\\bigg] + \\mathbb E \\bigg[\\frac{2d^2\\tau(\\hat\\tau - \\tau)G_i^2G_j^2}{\\|G\\|^4} +\\bigg(\\frac{d(\\hat\\tau - \\tau) G_iG_j}{\\|G\\|^2}\\bigg)^2\\bigg] & (*) \\\\\n&= \\bigg[\\Sigma_{ij}^2+ (\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2) + \\mathcal O(\\frac 1 d )\\bigg] + \\mathbb E \\bigg[\\frac{2d^2\\tau(\\hat\\tau - \\tau)G_i^2G_j^2}{\\|G\\|^4} +\\bigg(\\frac{d(\\hat\\tau - \\tau) G_iG_j}{\\|G\\|^2}\\bigg)^2\\bigg] \\\\\n&= \\bigg[\\Sigma_{ij}^2+ (\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2) + \\mathcal O(\\frac 1 d )\\bigg] + \\mathbb E \\bigg[\\frac{2d^2\\tau(\\hat\\tau - \\tau)G_i^2G_j^2}{\\|G\\|^4}\\bigg] + \\mathbb E \\bigg[\\bigg(\\frac{d(\\hat\\tau - \\tau) G_iG_j}{\\|G\\|^2}\\bigg)^2\\bigg] \\\\\n&\\leq \\bigg[\\Sigma_{ij}^2+ (\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2) + \\mathcal O(\\frac 1 d )\\bigg] + \\sqrt{\\text{Var}(\\hat\\tau)}\\sqrt{\\mathbb E \\frac{4d^4\\tau G_i^4G_j^4}{\\|G\\|^8}} + \\sqrt{\\mathbb E(\\hat\\tau - \\tau)^4}\\sqrt{\\mathbb E\\frac{4d^4\\tau^2 G_i^4G_j^4}{\\|G\\|^8}} & (**)\\\\\n&= \\bigg[\\Sigma_{ij}^2+ (\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2) + \\mathcal O(\\frac 1 d )\\bigg] +\\sqrt{\\mathcal O(\\frac{1}{d^\\beta})}\\sqrt{\\mathcal O(1)}+\\sqrt{\\mathcal O(\\frac{1}{d^{2\\beta}})}\\sqrt{\\mathcal O(1)}\\\\\n&= 2\\Sigma_{ij}^2+ \\Sigma_{ii}\\Sigma_{jj} + \\mathcal O (\\frac{1}{d^{\\beta/2}})\n\\end{aligned}\n\\]\nAbove, (*) follows from another Taylor series expansion of the first bracketed term, and (**) comes from bounding the order of the expectations with respect to \\(d\\) using the Cauchy-Schwartz inequality.\nPutting these together for the variance:\n\\[\n\\begin{aligned}\n\\text{Var}(d\\hat\\tau G_iG_j/\\|G\\|^2) &= \\mathbb E \\bigg[ (d\\hat\\tau G_iG_j/\\|G\\|^2)^2\\bigg] - \\mathbb E \\bigg[d\\hat\\tau G_iG_j/\\|G\\|^2 \\bigg]^2\\\\\n& = \\bigg[2\\Sigma_{ij}^2+ \\Sigma_{ii}\\Sigma_{jj} + \\mathcal O (\\frac{1}{d^{\\beta/2}})\\bigg] - \\bigg[\\Sigma_{ij}+\\mathcal O (\\frac{1}{d^{\\beta/2}})\\bigg]^2  \\\\\n&= \\bigg[2\\Sigma_{ij}^2+ \\Sigma_{ii}\\Sigma_{jj} + \\mathcal O (\\frac{1}{d^{\\beta/2}})\\bigg] - \\bigg[\\Sigma_{ij}^2+\\mathcal O (\\frac{1}{d^{\\beta/2}})\\bigg]\\\\\n& = \\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2 + \\mathcal O (\\frac{1}{d^{\\beta/2}})\n\\end{aligned}\n\\]\nSo, for fixed indices \\(i,j\\), even if we fix the sample size \\(n\\), the variance of our estimator achieves the Gaussian CRB plus a bias term that vanishes as \\(d\\) increases. When we let \\(n\\) grow, by the independence of the samples, both terms in the variance above scale by \\(\\frac 1 n\\).\nChoose estimation dimension \\(k = k(d) = \\mathcal o(d^{\\beta/4})\\). Then, looking at the MSE of our estimator over the entire set of possible indices \\(i,j \\leq k\\) for any individual row of our estimation matrix \\(\\textbf X_{[k]}\\), and defining \\(\\Omega\\) as the CRLB matrix for \\(\\Sigma^{(k)}\\):\n\\[\n\\begin{aligned}\n\\mathbb E \\|\\hat \\Sigma^{(k)} - \\Sigma^{(k)}  \\|_F^2 &= \\mathbb E \\bigg \\| d\\hat\\tau \\frac{X_{[k]} X_{[k]}^{\\top}}{\\|X\\|^2} - \\Sigma^{(k)}  \\bigg\\|_F^2 \\\\\n&= \\mathbb E \\bigg \\| d\\hat\\tau \\frac{G_{[k]}G_{[k]}^{\\top}}{\\|G\\|^2} - \\Sigma^{(k)} \\bigg\\|_F^2 \\\\\n&= \\mathbb E \\sum_{i,j} (d\\hat\\tau G_iG_j/\\|G\\|^2 - \\Sigma_{ij})^2 \\\\\n&= \\sum_{i,j} \\text{Var}(d\\hat\\tau G_iG_j/\\|G\\|^2)+ \\sum_{i,j}(\\Sigma_{ij} - \\mathbb E\\hat \\Sigma_{ij})^2\\\\\n&= \\sum_{i,j} \\text{Var}(d\\hat\\tau G_iG_j/\\|G\\|^2)+\\sum_{i,j}\\mathcal O (\\frac{1}{ d^{\\beta}}) \\\\\n&= \\sum_{i,j}\\bigg[\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2 + \\mathcal O(\\frac{1}{d^{\\beta/2} })\\bigg] +\\sum_{i,j}\\mathcal O (\\frac{1}{ d^{\\beta}})  \\\\\n&= \\sum_{i,j}[\\Sigma_{ii}\\Sigma_{jj}+\\Sigma_{ij}^2]+\\mathcal O(\\frac{k^2}{d^{\\beta/2} }) \\\\\n&= \\text{tr}(\\Omega) + \\mathcal O(\\frac{1}{d^{\\beta/2} }) \\\\\n\\implies  \\frac{\\mathbb E \\|\\hat \\Sigma - \\Sigma \\|_F^2}{\\text{tr}(\\Omega)} &= 1 + \\mathcal O(\\frac{1}{d^{\\beta/2+1} }) \\\\\n\\end{aligned}\n\\]\nSo taking the limit as \\(d \\rightarrow \\infty\\), our estimator saturates the Gaussian global lower MSE bound as long as we choose \\(k = k(d) = \\mathcal o(d^{\\beta/4})\\) (e.g. \\(k\\) can be allowed to go to infinity along with \\(d\\), provided it grows no faster than \\(d^\\frac{\\beta}{4}\\)).\nWith \\(n\\) i.i.d. rows, the variance terms scale as \\(1/n\\), so averaging over multiple observations shrinks the additive Frobenius error by a factor of \\(1/n\\), but leaves the risk ratio above unchanged.\n\n\nAsymptotic Superefficiency\nBecause we saturate the Gaussian global lower MSE bound, this estimator is “superefficient” (with respect to the Frobenius norm metric above) under the right conditions.\nIn the limit as \\(d \\rightarrow \\infty\\), the projective achieves a lower global MSE than possible under the Cramér–Rao bound matrix for the original distribution. This doesn’t contradict the usual Cramér–Rao theory, because these optimal bounds are defined in terms of a particular distribution, and we are using a parameter-preserving transformation to an auxiliary distribution with a lower bound on MSE.\nOne side-effect of this is much lower model-misspecification risk.\nTo see this, consider the multivariate-t distribution. Besson & Abromovich (2018) extend the Slepian-Bangs formula for the Fisher Information Matrix (FIM) to the broader class of elliptically contoured distributions, and derive the formua for the FIM of the multivariate-t case.\n[Multivariate t proof]\n[Superefficiency of ACG?]\n[Alpha stable proof]\n\n\nFinite-Dimensional Superefficiency\nThe superefficiency results above do not require \\(d \\rightarrow \\infty\\).\nBy relying on a Gaussian vector norm for concentration of measure, we can apply the Hanson-Wright inequality to get a probabalistic bound on the efficiency gains relative to the original distribution. \\[\n\\begin{aligned}\n\\mathbb P\\bigg(\\bigg \\| d\\hat\\tau \\frac{XX^\\top}{\\|X\\|^2} - GG^\\top\\bigg \\|_F \\geq t \\bigg) & = \\mathbb P\\bigg(\\bigg \\| d\\hat\\tau\\frac{GG^\\top}{\\|G\\|^2} - GG^\\top\\bigg \\|_F \\geq t \\bigg) \\\\\n&= \\mathbb P\\bigg(\\bigg \\| GG^\\top \\bigg( \\frac{d\\hat\\tau}{\\|G\\|^2} - 1 \\bigg) \\bigg \\|_F \\geq t \\bigg) \\\\\n&= \\mathbb P\\bigg( \\| GG^\\top \\|_F \\bigg| \\frac{d\\bar\\sigma^2}{\\|G\\|^2} - 1 \\bigg|  \\geq t \\bigg) \\\\\n&= \\mathbb P\\bigg( \\| G \\|^2 \\bigg| \\frac{d\\hat\\tau}{\\|G\\|^2} - 1 \\bigg|  \\geq t \\bigg) \\\\\n&= \\mathbb P( | \\|G\\|^2 - d \\hat\\tau |  \\geq t ) \\\\\n&= \\mathbb P( | \\|\\Sigma^{1/2}Z\\|^2 - d \\hat\\tau|  \\geq t ) \\\\\n&= \\mathbb P( | (\\Sigma^{1/2}Z)^\\top(\\Sigma^{1/2}Z) - d \\hat\\tau |  \\geq t ) \\\\\n&= \\mathbb P( | Z^\\top \\Sigma Z - d \\hat\\tau |  \\geq t ) \\\\\n&\\leq 2\\exp\\bigg[-c \\min\\bigg(\\frac{t^2}{\\|\\Sigma\\|_F^2}, \\frac{t}{\\|\\Sigma\\|_{\\text{op}}}\\bigg)\\bigg]\n\\end{aligned}\n\\]\n\n\n\nSimulation Study"
  },
  {
    "objectID": "projective_estimator.html#efficiency",
    "href": "projective_estimator.html#efficiency",
    "title": "Projective Estimator",
    "section": "Efficiency",
    "text": "Efficiency"
  },
  {
    "objectID": "projective_estimator.html#asymptotic-super-efficiency",
    "href": "projective_estimator.html#asymptotic-super-efficiency",
    "title": "Projective Estimator",
    "section": "Asymptotic Super-Efficiency",
    "text": "Asymptotic Super-Efficiency"
  },
  {
    "objectID": "projective_estimator.html#finite-dimensional-superefficiency",
    "href": "projective_estimator.html#finite-dimensional-superefficiency",
    "title": "Projective Estimator",
    "section": "Finite-Dimensional Superefficiency",
    "text": "Finite-Dimensional Superefficiency\nWe can use the Hanson-Wright bound to show that above d min ⁡ d min ​\nthe probability that the projected estimator is super-efficient exceeds  1 − δ 1−δ.\n\nSimulation Study"
  }
]