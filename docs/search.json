[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Progress Site",
    "section": "",
    "text": "I’ll add more structure to this site as I collect more results than I can fit in this format.\nThe links above provide general background on my problem setting, current progress, and some additional questions (and hopefully their answers) that might not be perfectly aligned with that path.\n\nResearch Plan\n\nFind optimal estimator for underlying dependence\n\nSubordinated Covariance Estimator\nSubordinated Correlation Estimator\n\nInvestigate potential benefits of a median-of-means approach for heavy tails and small observations\nInvestigate high-dimensionality adjustments\nApplications"
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "Since I realize that this problem setting may be a little unfamiliar, I’m including a brief background on \\(\\alpha\\)-stable random variables and the research question.\nI’m interested in estimating the dependence of a random vector when covariance may not exist and the dimension of the vector may be high relative to the number of observations. We’re principally interested in a symmetric, positive semi-definite covariance matrix-like object \\(\\Sigma\\) or its corresponding correlation matrix, since:\nIn financial markets, distributions of returns (and log-returns, which are used interchangeably and often without distinction) often have heavy tails. This leads to the relaxation of the finite variance assumption. Even if a process has finite variance, my conjecture is that an estimator that admits heavy tails may be better (e.g. more efficient) in finite/small samples.\nIn the literature, correlation and covariance matrices are used interchangably, since the correlation matrix is just a covariance matrix of an alternative dataset where the data have been standardized."
  },
  {
    "objectID": "background.html#stable-distributions-and-heavy-tailed-random-vectors",
    "href": "background.html#stable-distributions-and-heavy-tailed-random-vectors",
    "title": "Background",
    "section": "Stable Distributions and Heavy-Tailed Random Vectors",
    "text": "Stable Distributions and Heavy-Tailed Random Vectors\n\n\n\n\n\n\nWhy Stable?\n\n\n\nFor the why behind using stable distributions to connect to theoretical results, see the page Why Stable?\n\n\nIf we relax the assumption that variance is finite, then a reasonable model for returns is the stable distribution, which has no closed form pdf in general but has a complicated characteristic function where the dependence is fully defined by the spectral measure (typically denoted \\(\\Gamma\\)), a finite measure on the unit circle \\(\\mathbb S^d\\).\nStable distributions are a reasonable choice because they are infinitely divisible, and by the Generalized Central Limit Theorem of Gnedenko and Kolmogorov (1954), if sums of i.i.d. random variables converge to a distribution, it must be stable. This mirrors the justifications often used for the normal distribution in financial models.\nIn fact, the normal distribution is a special case when the characteristic exponent \\(\\alpha = 2\\). It is the only case where variance is finite. The two other closed form pdfs of the stable family are the Cauchy (\\(\\alpha = 1\\)) and the Lévy (\\(\\alpha = 1/2\\)).\nStable distributions are admittedly confusing because of the complicated characteristic function and infinite variance, as well as the fact that some terms (e.g. sub-Gaussian) mean something totally different in the stable distribution context than in the broader statistics context.\nLikewise there are nearly a dozen known parameterizations of the stable distribution, which makes it hard to distinguish what an author means when they describe a result.\nA special case is the sub-Gaussian (aka subordinated Gaussian) symmetric \\(\\alpha\\)-stable distribution (to avoid confusion with sub-Gaussian tails, I’ll note this as \\(\\alpha\\)-SG(\\(\\Sigma\\)), as some texts do), which for a random vector X can be written:\n\\[\nX = \\sqrt W G\n\\]\nWhere W \\(\\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers, and \\(G \\sim N(0, \\Sigma)\\), a multivariate normal random vector independent of W.\nIn this sub-Gaussian random vector, the spectral measure \\(\\Gamma\\) is fully determined by \\(\\Sigma\\). In fact, \\(\\Gamma\\) symmetric (e.g. antipodal points have equal measure), and continuous. While it’s a bit counter-intuitive, even in the isotropic case where \\(\\Sigma = I_d\\), the identity matrix, the components of X are not independent. Indeed, the idea of orthogonality isn’t defined (although some generalizations of this idea are), since X is not defined on a (pre-)Hilbert space when \\(\\alpha &lt; 2\\). This actually turns out to be a desirable characteristic for a couple of reasons. For one, since tail dependence is asymptotically zero in the Gaussian case, a contributor to the failure of the Gaussian copula model for derivative pricing during the 2008 financial crisis. This model errs on the side of overestimating risk.\nWhen \\(X \\sim\\) \\(\\alpha\\)-SG(\\(\\sigma^2\\)), it has the following simplified, more familiar characteristic function:\n\\[\n\\mathbb E[e^{iu^\\text T X}] = exp\\bigg\\{-\\frac12 |u^\\text T \\Sigma u|^{\\alpha/2}\\bigg\\}\n\\]\nWhen \\(\\alpha =2\\), this is the Gaussian CF.\nWe want the best estimate of \\(\\Sigma\\) or its corresponding correlation matrix.\nThe best reference on multivariate stable distributions is Stable Non-Gaussian Random Processes by Samoradnitsky and Taqqu (1994), which contains virtually all known theoretical results as of its publication (despite the book’s 30th anniversary, not much has changed). For a more modern treatment that’s limited to univariate stable random variables, John Nolan’s aptly-named Univariate Stable Distributions: Models for Heavy Tailed Data from 2020 is recommended.\nOne important note is that parameterizations of stable distributions are very inconsistent in the literature."
  },
  {
    "objectID": "background.html#high-dimensional-dependence-estimation",
    "href": "background.html#high-dimensional-dependence-estimation",
    "title": "Background",
    "section": "High-Dimensional Dependence Estimation",
    "text": "High-Dimensional Dependence Estimation\nThe other piece of the research question puzzle is “high-dimensional” data. By high-dimensional, I mean that the dimension of the data is high relative to the number of observations; so 30 dimensions is quite high-dimensional when you only have 35 observations to estimate something like a covariance matrix from.\nThe current state-of-the-art approach used in finance is the Optimal Rotationally-Invariant Estimator (RIE), which uses results from Random Matrix Theory to find the optimal way to “clean” the eigenvalues of a sample covariance matrix C so that the squared Frobenius distance \\(||\\Sigma - C||_F^2\\) is minimized, even without being able to observe the true \\(\\Sigma\\), provided that the data are at least asymptotically normal.\nInitially, my plan was to explore how this recipe might change if the underlying data do not have finite variance (e.g. finding \\(\\Sigma\\) from a sub-Gaussian \\(\\alpha\\)-stable random vector described above). More recently, I’ve found some research that suggests that if \\(1 &lt; \\alpha \\leq 2\\), then the universality rules apply asymptotically and this same RIE formulation is appropriate to use for a Lévy matrix. If true, this would provide a theoretical justification for using this approach for heavy-tailed data."
  },
  {
    "objectID": "background.html#current-estimator",
    "href": "background.html#current-estimator",
    "title": "Background",
    "section": "Current Estimator",
    "text": "Current Estimator\nMy candidate estimator, Median Oracle, takes a very simple heuristic approach to estimating the quasi-correlation from a matrix \\(X\\) of returns. Assume \\(X\\) is a \\(t\\times n\\) matrix consisting of \\(t\\) independent observations of a market of \\(n\\) assets, and a sub-sample size \\(d_in\\):\n\n(Optional_ Rescale the columns of \\(X\\) to be mean-zero with unit variance, such that we’re estimating \\(\\Sigma\\) as a correlation matrix.\nFor \\(i=1,..., B\\):\n\nDraw \\(b \\sim Unif[1,t-d_in)\\)\nAt each \\(b\\), subsample \\(X_b = X_{b:(b+d_in-1)}, \\cdot\\)\nCompute the sample covariance matrix \\(\\Sigma_b = \\frac{1}{d_{in}-1}X_b^\\text T X_b\\).\nCompute the Rotationally Invariant Estimator (RIE) adjustment to the eigenvalues \\(\\hat\\lambda_{1}^{(1)}, ..., \\hat\\lambda_{i}^{(n)}\\) of \\(\\Sigma_b\\), and store them in \\(Q \\in \\mathbb R^{B \\times n}\\)\n\nFor for each eigenvalue \\(\\lambda^{(j)}\\) of the covariance matrix, \\(j = 1, ..., n\\), estimate \\(\\hat \\lambda^{(j)} = \\text{median}(\\hat\\lambda_{i}^{(j)})\\), the median across all \\(B\\) samples.\nReconstruct the covariance matrix using \\(\\hat\\Lambda = \\text{diag}(\\hat \\Lambda^{(n)},...,\\hat \\lambda^{(1)})\\) and the eigenvectors of the full-sample covariance matrix \\(\\Sigma = V^\\text{T} \\Lambda V\\).\nRescale by forecasted asset-wise standard deviation to get a final estimate of the underlying covariance matrix.\n\nTaking the sample covariance and correlations of a dataset assumed to be generated by an infinite-variance process seems crazy initially, but in fact the eigenvalue spectrum of these subsampled covariance matrices are well defined distributions."
  },
  {
    "objectID": "open_questions.html",
    "href": "open_questions.html",
    "title": "Some Questions (And Answers)",
    "section": "",
    "text": "Can we generalize the equivalence property of Fractional Lower-Order Moments (FLOMs) to other distributions where covariance exists?\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nThis becomes obvious from the definition of the characteristic function. Let \\(X = \\mu + \\gamma Z\\) be a random variable with mean \\(\\mu\\) (note that \\(\\mathbb EX \\nless \\infty\\) is not actually required. If X is stable with \\(\\alpha \\leq 1\\), we replace mean with location and it still works - but X need not be a location scale family), and characteristic function \\(\\varphi_X(t)\\). Then for any p for which \\(\\varphi_X(t)\\) is differentiable at \\(t=0\\), the p-th fractional moment exists (Laue, 1980) and can be factored to some constant that depends only on the distribution parameters \\(\\mathbf \\theta\\) times the scale parameter survives raised to the p-th power thanks to the product rule:\n\\[\n\\begin{aligned}\n\\mathbb E(|X-\\mu|^p) & = \\mathbb E(\\gamma Z_+^p)+\\mathbb E (\\gamma Z_-^p)\\\\\n& = (-i)^p \\bigg(\\frac{d^p}{dt^p}\\varphi_{\\gamma Z_+}(t)+\\frac{d^p}{dt^p}\\varphi_{\\gamma Z_-}(t)\\bigg)\\bigg |_{t=0} \\\\\n& = (-i)^p\\frac{d^p}{dt^p}\\bigg(\\varphi_{Z_+}(\\gamma t)+\\varphi_{Z_-}(\\gamma t)\\bigg)\\bigg |_{t=0} \\\\\n& = \\gamma^p c(\\mathbf \\theta)\n\\end{aligned}\n\\]\nWhen the second moment exists, note that \\(\\gamma^2\\) is at worst a rescaled parameterization of the variance of the distribution, since:\n\\[\n\\gamma^2 c(\\mathbf \\theta) = \\mathbb E(|X-\\mu|^2) = \\text{Var}(X) \\implies \\gamma^2 \\propto \\text{Var}(X)\n\\]\nThis result is helpful for a couple of reasons.\n\nIf we’re comparing random varables that differ only by a scale parameter (e.g. otherwise, they share \\(\\mathbf \\theta\\)), we can get a good estimate of “correlation” because these constants will cancel.\nIf we apply FLOMs to arbitrary random variables with finite variance, the p-th root of the FLOM will be proportionate to the actual standard deviation. This makes this estimator robust to model misspecification (think lognormal distribution)\n\n\n\n\n\nWhat p is actually optimal, given some \\(\\theta\\).\n\nDoes p actually depend on \\(\\alpha\\)? Can we choose some clever \\(\\theta\\) that will turn the Taylor series approximation into an exact result?\n\nTo what extent does the resulting correlation matrix result in eigenvalue regularization compared to a sample covariance matrix? What does that regularization function look like?\nIs RIE appropriate to use on sample covariance estimates derived from stable data?\n\n(Two approaches: universality when \\(\\alpha &gt;1\\), \\(\\Sigma\\) as a covariance matrix of a Gaussian vector)"
  },
  {
    "objectID": "progress.html",
    "href": "progress.html",
    "title": "Current Progress",
    "section": "",
    "text": "I’ve split the problem into two pieces: getting an “optimal” estimate of the covariance/correlation matrix underlying a stable-sub-Gaussian random vector, and adjusting for high-dimensionality. It’s a little hand-wavy, but I think this is reasonable since the RIE is a monotonically increasing function of the eigenvalues of the input. So, by the invariance of quantiles under monotone transformations, the median RIE estimate at each eigenvalue should be the same as the RIE of the median eigenvalue estimate."
  },
  {
    "objectID": "progress.html#estimating-sigma",
    "href": "progress.html#estimating-sigma",
    "title": "Current Progress",
    "section": "Estimating \\(\\Sigma\\)",
    "text": "Estimating \\(\\Sigma\\)\nWe want to estimate the covariance matrix \\(\\Sigma\\) of the underlying Gaussian vector that defines the dependence structure of our stable random variable \\(X\\).\nKodia and Garel propose a signed symmetric covariation coefficient:\n\\[\n\\text{scov} (X_1, X_2) = \\kappa_{(X_1, X_2)}\\bigg | \\frac{[X_1, X_2]_\\alpha [X_2, X_1]_\\alpha}{||X_1||_\\alpha ||X_2||_\\alpha} \\bigg|^{1/2}\n\\]\nHere, \\([X_1, X_2]_\\alpha\\) and \\(||X_1||_\\alpha\\) are the covariation of \\(X_1\\) on \\(X_2\\) and the covariation norm of \\(X_1\\), respectively, and \\(\\kappa\\) represents the “agreement” of the signs of the covariation terms.\nWhen \\(1 &lt; \\alpha &lt; 2\\), this can equivalently be represented using Fractional Lower Order Moments (FLOMs):\n\\[\nr = \\frac{(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p}}{2(\\mathbb E |X_1|^p\\mathbb E |X_2|^p)^{1/p}}\n\\]\nIn both cases, when \\(X\\) is a sub-Gaussian random vector with underlying Gaussian vector \\(G\\), these quantities coincide with the correlation coefficient between \\(G_1\\) and \\(G_2\\).\nKodia and Garel show that straightforward estimators based on these quantities are strongly consistent estimators of the Gaussian correlation matrix.\n\nBias of the FLOM Estimator\nNote that FLOMs aren’t unbiased estimators of scale/dependence unless \\(p=2\\).\nProposition 1 on p.32 of Nikias and Shao is:\n\\[\n\\mathbb E(|X|^p) = C(p,\\alpha)\\gamma^{p/\\alpha}\n\\]\nwhere \\(C(p,\\alpha)\\) is a known constant that depends only on p and the tail exponent of X and is used below, and \\(\\gamma\\) is the scale parameter of X. Note that the authors use a non-typical parameterization for \\(\\gamma\\) but this result holds in general for any distribution – rough/hand-wavy proof here.\nOne nice thing is that if we’re estimating a correlation-type quantity like scov and r above, these bias terms cancel out in the numerator and denominator, and we’re left only with the scale terms.\n\n\nVariance of the FLOM Estimator\nOne area that (surprisingly) has received little attention in the literature is the variance of these FLOM estimators. While the variance of \\(X\\) may be infinite, we can estimate its scale and dependence using FLOMs with finite variance.\nIt’s known that for \\(p&lt;\\alpha, \\mathbb E |X|^p &lt; \\infty\\). Therefore, the variance of \\(|X|^p\\) exists for \\(p&lt;\\alpha/2\\). Several papers estimate sample FLOM variance using Monte Carlo simulations, but I haven’t seen analytical representations in any papers. In this case, it’s a straightforward application of the expected value of the FLOM:\n\\[\n\\begin{aligned}\n\\text{Var}(\\frac{1}{N}\\sum_{i=1}^N |X|^p) & = N^{-2} \\text{Var}(\\sum_{i=1}^N |X|^p) \\\\\n&= N^{-1} [\\mathbb E(|X|^{2p}) - \\mathbb E(|X|^p)^2] \\\\\n&= N^{-1} [C(2p,\\alpha)\\gamma^{2p/\\alpha}-(C(p, \\alpha)\\gamma^{p/\\alpha})^2]\\\\\n&= \\frac{2^{2p+2}\\gamma^{2p/\\alpha}}{N \\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\}\n\\end{aligned}\n\\]\nWhat’s interesting is that simulation results are problematically misleading when \\(X\\) is nearly Gaussian:\n\nThe chart above simulates 10,000 samples to estimate the standard error of the FLOM estimator as a function of p at various values of \\(\\alpha\\). When p gets near the infinite variance regime, simulations will materially underestimate the variability of the estimator, especially when \\(\\alpha \\uparrow 2\\). Heuristically, this is because tails are only a little heavier than the Gaussian case, and you’re much less likely to get extreme realizations that traverse the full support of the distribution via simulation unless your sample size is massive. In these examples, the simulation approach requires tens of millions of simulations to get close to the analyical standard deviation of the estimator.\nI think this is an incredibly interesting aside that I haven’t seen covered in the literature so far.\nThis is relevant because some papers have suggested choosing optimal p to be \\(p \\lesssim \\alpha/2\\) (e.g. right on the boundary of finite variance).\nThe variance of the FLOM is monotonically increasing in p, and some have suggested making p as small as possible. The difficulty is that \\(\\lim_{p \\rightarrow 0} C(p, \\alpha) = \\infty\\). This problem doesn’t exist in the correlation estimator case.\n\n\nVariance of the Correlation Estimators\n\n\n\n\n\n\nImportant\n\n\n\nNote: The correlation-type estimators are being abandoned since they perform far worse than original Median Oracle despite some nice theoretical properties.\n\n\nWhen our estimator is a correlation-type estimator like scov and r above, any sample estimate is bounded on \\([-1,1]\\) and all moments of the estimator itself are finite for any p – even \\(p&gt;\\alpha\\)!\nLike with raw FLOMs, Monte Carlo has been used to estimate variance of these estimators in some papers, but only with relatively small simulation sizes and without comment on potential errors in the calculation of an optimal p.\nSince we can induce finite variance on the components of these estimators, it seems reasonable to invoke the delta method to approximate their asymptotic variance using Taylor series expansions to get a better estimate of a variance-minimizing choice of p.\nMy derivation of the correlation-type estimator variance approximation can be found here.\nUnfortunately, this variance approximation doesn’t work very nicely in practice for moderate-sized simulations:\n\nAnd worse, plugging it into the current Median Oracle algorithm in place of sample covariance results in worse estimates. I think this is happening because throwing away the magnitude information for a correlation-type estimator results in worse estimates. The good news is that an estimator based on the numerator of r should fix this problem. The bad news is that it reintroduces infinite estimator variance."
  },
  {
    "objectID": "corr_variance_derivation.html",
    "href": "corr_variance_derivation.html",
    "title": "Deriving Correlation Estimator Variance",
    "section": "",
    "text": "Starting with couple preliminaries that will be useful to derive an analyticaly tractable approximation for the variance of these correlation estimators, \\(\\hat r\\) and \\(\\hat{ \\text{scov}}\\) based on the sample FLOM estimators.\n\n\nFirst, deriving the expected value for W \\(\\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers. We can use the fact that we can decompose a \\(\\alpha\\)-SG(\\(\\sigma^2\\)) random variable into the product of the square root of W and normal random variable \\(G \\sim N(0,\\sigma^2)\\) independent of W:\n\\[\n\\begin{aligned}\n\\mathbb E(|X|^p) &= \\mathbb E(|W^{1/2}G|^p) \\\\\n&= \\mathbb E(W^{p/2}|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\mathbb E(|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\sigma^p 2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n&= \\mathbb E(W^{p/2}) (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n\\\\ \\text{So we get:} \\\\\n\\mathbb E(W^{p/2}) &= \\frac{\\mathbb E(|X|^p)}{ (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}\\\\\n&=  2^{-p}\\gamma^{-p}\\frac{\\sqrt \\pi}{\\Gamma(\\frac{1+p}{2})}\\times{\\frac{\\gamma^p 2^{p+1}\\Gamma(\\frac{p+1}{2})\\Gamma(-p/\\alpha)}{\\alpha \\sqrt \\pi \\Gamma(-p/2)}} \\\\\n&= \\frac{2\\Gamma(-p/\\alpha)}{\\alpha\\Gamma(-p/2)}, & p&lt;\\alpha/2\n\\end{aligned}\n\\]\nThis uses the well-known results for normal absolute moments, stable FLOMs, and a change of variables from the standard deviation of G to the scale parameter \\(\\gamma\\) of X.\n\n\n\nWhen \\(p&lt;\\alpha/2\\), \\(\\text{Cov}(|X_1|^p, |X_2|^p)\\) is finite and can be written in terms of the hypergeometric function for \\(-1&lt;p&lt;\\alpha/2\\):\n\\[\n\\begin{aligned}\n\\text{Cov}(|X_1|^p, |X_2|^p) &= \\mathbb E(|X_1|^p |X_2|^p) - \\mathbb E (|X_1|^p) \\mathbb E (|X_2|^p)\\\\\n&= \\mathbb E(|X_1 X_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\mathbb E(|\\sqrt W G_1 \\sqrt W G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p) \\mathbb E(|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^p \\sigma_1^p \\sigma_2^p}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^{p+1} \\gamma_1^{p/\\alpha} \\gamma_2^{p/\\alpha}}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\n\\end{aligned}\n\\]\nTherefore, letting \\(G_3 = G_1-G_2 \\implies X_3 = X_1-X_2\\), with \\(G_i\\) a mean-zero normal random variable correlated with the other two:\n\\[\n\\begin{aligned}\n\\text{Cov}(G_3, G_1) &= \\text{Cov}(G_1 - G_2, G_1)  \\\\\n&= \\text{Cov}(G_1, G_1)-\\text{Cov}(G_2, G_1)\\\\\n&= \\text{Var}(G_1)-\\text{Cov}(G_2, G_1) \\\\\n&= \\sigma_1^2 - \\rho \\sigma_1\\sigma_2 \\\\\n\\text{Thus, we have:}\\\\\n\\text{Cov}(G_3, G_2) &= \\text{Cov}(G_1 - G_2, G_2) \\\\\n&= \\rho \\sigma_1\\sigma_2 - \\sigma_2^2 \\\\\n\\text{Var}(G_3) &= \\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\n\\end{aligned}\n\\]\nThis implies that the correlation coefficients for \\(G_3\\) are:\n\\[\n\\begin{aligned}\n\\rho_{3,1} \\equiv \\frac{\\sigma_1^2 - \\rho \\sigma_1\\sigma_2}{\\sigma_1 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}} \\\\\n\\rho_{3,2} \\equiv \\frac{\\rho \\sigma_1\\sigma_2 - \\sigma_2^2 }{\\sigma_2 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}}\n\\end{aligned}\n\\]\nThis gives us \\(\\text{Cov}(|X_1-X_2|^p, |X_i|^p), i=1,2\\) for stable random variables \\(X_i\\) by using \\(\\rho_{3,i}\\) as the final parameter in the hypergeometric function above.\nWe can use all of this to get an approximation for the variance of our estimators when \\(p&lt;\\alpha/2\\)."
  },
  {
    "objectID": "corr_variance_derivation.html#preliminaries",
    "href": "corr_variance_derivation.html#preliminaries",
    "title": "Deriving Correlation Estimator Variance",
    "section": "",
    "text": "Starting with couple preliminaries that will be useful to derive an analyticaly tractable approximation for the variance of these correlation estimators, \\(\\hat r\\) and \\(\\hat{ \\text{scov}}\\) based on the sample FLOM estimators.\n\n\nFirst, deriving the expected value for W \\(\\sim S_{\\alpha/2}(\\cos(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers. We can use the fact that we can decompose a \\(\\alpha\\)-SG(\\(\\sigma^2\\)) random variable into the product of the square root of W and normal random variable \\(G \\sim N(0,\\sigma^2)\\) independent of W:\n\\[\n\\begin{aligned}\n\\mathbb E(|X|^p) &= \\mathbb E(|W^{1/2}G|^p) \\\\\n&= \\mathbb E(W^{p/2}|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\mathbb E(|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\sigma^p 2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n&= \\mathbb E(W^{p/2}) (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n\\\\ \\text{So we get:} \\\\\n\\mathbb E(W^{p/2}) &= \\frac{\\mathbb E(|X|^p)}{ (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}\\\\\n&=  2^{-p}\\gamma^{-p}\\frac{\\sqrt \\pi}{\\Gamma(\\frac{1+p}{2})}\\times{\\frac{\\gamma^p 2^{p+1}\\Gamma(\\frac{p+1}{2})\\Gamma(-p/\\alpha)}{\\alpha \\sqrt \\pi \\Gamma(-p/2)}} \\\\\n&= \\frac{2\\Gamma(-p/\\alpha)}{\\alpha\\Gamma(-p/2)}, & p&lt;\\alpha/2\n\\end{aligned}\n\\]\nThis uses the well-known results for normal absolute moments, stable FLOMs, and a change of variables from the standard deviation of G to the scale parameter \\(\\gamma\\) of X.\n\n\n\nWhen \\(p&lt;\\alpha/2\\), \\(\\text{Cov}(|X_1|^p, |X_2|^p)\\) is finite and can be written in terms of the hypergeometric function for \\(-1&lt;p&lt;\\alpha/2\\):\n\\[\n\\begin{aligned}\n\\text{Cov}(|X_1|^p, |X_2|^p) &= \\mathbb E(|X_1|^p |X_2|^p) - \\mathbb E (|X_1|^p) \\mathbb E (|X_2|^p)\\\\\n&= \\mathbb E(|X_1 X_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\mathbb E(|\\sqrt W G_1 \\sqrt W G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n& = \\mathbb E(W^p) \\mathbb E(|G_1 G_2|^p) - C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha} \\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^p \\sigma_1^p \\sigma_2^p}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\\\\\n&= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^{p+1} \\gamma_1^{p/\\alpha} \\gamma_2^{p/\\alpha}}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)- C^2(p,\\alpha)\\gamma_1^{p/\\alpha}\\gamma_2^{p/\\alpha}\n\\end{aligned}\n\\]\nTherefore, letting \\(G_3 = G_1-G_2 \\implies X_3 = X_1-X_2\\), with \\(G_i\\) a mean-zero normal random variable correlated with the other two:\n\\[\n\\begin{aligned}\n\\text{Cov}(G_3, G_1) &= \\text{Cov}(G_1 - G_2, G_1)  \\\\\n&= \\text{Cov}(G_1, G_1)-\\text{Cov}(G_2, G_1)\\\\\n&= \\text{Var}(G_1)-\\text{Cov}(G_2, G_1) \\\\\n&= \\sigma_1^2 - \\rho \\sigma_1\\sigma_2 \\\\\n\\text{Thus, we have:}\\\\\n\\text{Cov}(G_3, G_2) &= \\text{Cov}(G_1 - G_2, G_2) \\\\\n&= \\rho \\sigma_1\\sigma_2 - \\sigma_2^2 \\\\\n\\text{Var}(G_3) &= \\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2\n\\end{aligned}\n\\]\nThis implies that the correlation coefficients for \\(G_3\\) are:\n\\[\n\\begin{aligned}\n\\rho_{3,1} \\equiv \\frac{\\sigma_1^2 - \\rho \\sigma_1\\sigma_2}{\\sigma_1 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}} \\\\\n\\rho_{3,2} \\equiv \\frac{\\rho \\sigma_1\\sigma_2 - \\sigma_2^2 }{\\sigma_2 \\sqrt{\\sigma_1^2 + \\sigma_2^2 - 2\\rho \\sigma_1 \\sigma_2}}\n\\end{aligned}\n\\]\nThis gives us \\(\\text{Cov}(|X_1-X_2|^p, |X_i|^p), i=1,2\\) for stable random variables \\(X_i\\) by using \\(\\rho_{3,i}\\) as the final parameter in the hypergeometric function above.\nWe can use all of this to get an approximation for the variance of our estimators when \\(p&lt;\\alpha/2\\)."
  },
  {
    "objectID": "corr_variance_derivation.html#estimator-variance-for-r",
    "href": "corr_variance_derivation.html#estimator-variance-for-r",
    "title": "Deriving Correlation Estimator Variance",
    "section": "Estimator Variance for r",
    "text": "Estimator Variance for r\n\n\n\n\n\n\nImportant\n\n\n\nThis estimator below has been abandoned, but the preliminaries above are still important so I’m keeping this page active.\n\n\nSince \\(\\hat r = \\frac{(\\sum_{i=1}^N |X_{1i}|^p)^{2/p}+(\\sum_{i=1}^N |X_{2i}|^p)^{2/p}-(\\sum_{i=1}^N |X_{1i}- X_{2i}|^p)^{2/p}}{2(\\sum_{i=1}^N |X_{1i}|^p \\sum_{i=1}^N |X_{2i}|^p)^{1/p}}\\), we can set \\(Y_{ji} = |X_{ji}|^p, Z_j = \\sum_{i=1}^N Y_{ji}\\) and replace this with:\n\\[\n\\begin{aligned}\n\\hat r &= \\frac{(\\sum_{i=1}^N Y_{1i})^{2/p}+(\\sum_{i=1}^N Y_{2i})^{2/p}-(\\sum_{i=1}^N Y_{3i})^{2/p}}{2(\\sum_{i=1}^N Y_{1i} \\sum_{i=1}^N Y_{2i})^{1/p}} \\\\\n& = \\frac{Z_1^{2/p}+Z_2^{2/p}-Z_3^{2/p}}{2(Z_1Z_2)^{1/p}} \\\\\n\\text{We can define the epectation as:} \\\\\n\\zeta_i \\equiv \\mathbb EZ_i = N C(\\alpha, p) \\gamma_i^{p/\\alpha}\n\\end{aligned}\n\\]\nThe second-order Taylor expansion of \\(\\mathbb E [\\hat r]\\) around \\((\\zeta_1, \\zeta_2, \\zeta_3)\\) is:\n\\[\n\\begin{aligned}\n\\mathbb E [\\hat r] \\equiv \\mathbb E [f(Z_1,Z_2,Z_3)] &\\approx \\mathbb E \\bigg\\{f(\\zeta_1, \\zeta_2, \\zeta_3) + \\sum_{i=1}^3 \\frac{\\partial f}{\\partial Z_i}\\bigg |_{(\\zeta_1, \\zeta_2, \\zeta_3)} (Z_i - \\zeta_i) + \\frac 1 2 \\sum_{i=1}^3\\sum_{j=1}^3 \\frac{\\partial^2 f}{\\partial Z_i\\partial Z_j}\\bigg |_{(\\zeta_1, \\zeta_2, \\zeta_3)} (Z_i - \\zeta_i)(Z_j - \\zeta_j)\\bigg\\} \\\\\n&=\\frac{\\zeta_1^{2/p}+\\zeta_2^{2/p}-\\zeta_3^{2/p}}{2(\\zeta_1 \\zeta_2)^{1/p}} +\\frac 1 2 \\bigg(\\frac{\\partial^2 f}{\\partial Z_1^2}\\text{Var}(Z_1)+\\frac{\\partial^2 f}{\\partial Z_2^2}\\text{Var}(Z_2)+\\frac{\\partial^2 f}{\\partial Z_3^2}\\text{Var}(Z_3)+ \\\\ & (\\frac{\\partial^2 f}{\\partial Z_1\\partial Z_2} + \\frac{\\partial^2 f}{\\partial Z_2\\partial Z_1})\\text{Cov}(Z_1, Z_2)+(\\frac{\\partial^2 f}{\\partial Z_1\\partial Z_3} + \\frac{\\partial^2 f}{\\partial Z_3\\partial Z_1})\\text{Cov}(Z_1, Z_3)+ \\\\ &(\\frac{\\partial^2 f}{\\partial Z_2\\partial Z_3} + \\frac{\\partial^2 f}{\\partial Z_3\\partial Z_2})\\text{Cov}(Z_2, Z_3)\\bigg) \\\\\n& =\n\\end{aligned}\n\\]\nA quick and dirty evaluation of all higher-order terms shows both the moments and the derivatives go to zero as \\(N \\rightarrow \\infty\\), so this estimator should quickly become unbiased as the sample size increases.\nUsing the first-order Taylor expansion (to keep things less crazy) for the variance, and the fact that for large N, \\(\\mathbb E[\\hat r] \\approx f(\\zeta_1, \\zeta_2, \\zeta_3)\\):\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat r)& \\approx \\mathbb E\\bigg[\\bigg(f(Z_1,Z_2,Z_3)-f(\\zeta_1, \\zeta_2, \\zeta_3) \\bigg)^2\\bigg]\n\\\\ &\n= \\mathbb E\\bigg[\\bigg(f(Z_1,Z_2,Z_3)-f(\\zeta_1, \\zeta_2, \\zeta_3) + \\frac{\\partial f}{\\partial Z_1}(Z_1 - \\zeta_1) + \\frac{\\partial f}{\\partial Z_2}(Z_2 - \\zeta_2) +\n\\frac{\\partial f}{\\partial Z_3}(Z_3 - \\zeta_3) - f(Z_1,Z_2,Z_3)\n\\bigg)^2\\bigg] \\\\\n&= \\mathbb E \\bigg[\n    \\bigg(\\frac{\\partial f}{\\partial Z_1}\\bigg)^2(Z_1 - \\zeta_1)^2 +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg)^2(Z_2 - \\zeta_2)^2 +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg)^2(Z_2 - \\zeta_2)^2 +\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_3}(Z_3 - \\zeta_3)(Z_1 - \\zeta_1) + \\\\ &\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_2}(Z_2 - \\zeta_2)(Z_1 - \\zeta_1) +  2\\frac{\\partial f}{\\partial Z_2}\\frac{\\partial f}{\\partial Z_3}(Z_3 - \\zeta_3)(Z_2 - \\zeta_2)\n    \\bigg] \\\\\n    &= \\bigg(\\frac{\\partial f}{\\partial Z_1}\\bigg)^2\\text{Var}(Z_1) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg)^2\\text{Var}(Z_2) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_3}\\bigg)^2\\text{Var}(Z_3) +\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_2}\\text{Cov}(Z_1, Z_2) + \\\\ &\n     2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_3}\\text{Cov}(Z_1, Z_3) +  2\\frac{\\partial f}{\\partial Z_2}\\frac{\\partial f}{\\partial Z_3}\\text{Cov}(Z_2, Z_3) \\\\\n&= \\frac{(\\zeta_1^{2/p}-\\zeta_2^{2/p}+\\zeta_3^{2/p})^2}{4p^2\\zeta_1^2(\\zeta_1 \\zeta_2)^{2/p}}\\text{Var}(Z_1)+\\frac{(\\zeta_2^{2/p}-\\zeta_1^{2/p}+\\zeta_3^{2/p})^2}{4p^2\\zeta_2^2(\\zeta_1 \\zeta_2)^{2/p}}\\text{Var}(Z_2)+\\frac{\\zeta_3^{\\frac 4p -2}}{p^2(\\zeta_1 \\zeta_2)^{2/p}}\\text{Var}(Z_3)+\\\\ &\n2\\frac{(\\zeta_3^{4/p}+(\\zeta_1^{2/p}-\\zeta_2^{2/p})^2)}{4p^2(\\zeta_1\\zeta_2)^{\\frac{2+p}{p}}}\\text{Cov}(Z_1, Z_2)-\\dots\n\\end{aligned}\n\\]\nInitially, I started writing out this expansion to see if I could get any of the known terms to cancel or find a pattern that could lead to an exact solutionm, but unfortunately neither case seems to hold. I calculated this variance approximation numerically, and compared it to simulations with less than compelling results."
  },
  {
    "objectID": "progress.html#the-subordinated-covariance-estimator",
    "href": "progress.html#the-subordinated-covariance-estimator",
    "title": "Current Progress",
    "section": "The Subordinated Covariance Estimator",
    "text": "The Subordinated Covariance Estimator\nIf we just take the rescaled numerator of r, we get the following:\n\\[\n\\begin{aligned}\n&(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p} \\\\\n&= (C(p,\\alpha)\\gamma_1^p)^{2/p}+(C(p,\\alpha)\\gamma_2^p)^{2/p}-(C(p,\\alpha)\\gamma_3^p)^{2/p}\n\\\\\n& = C(p,\\alpha)^{2/p}\\bigg[\\gamma_1^2 + \\gamma_2^2 -\\gamma_3^2 \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -\\frac{\\sigma_3^2}{2}  \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -(\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2}-\\frac{2\\rho_{12}\\sigma_1\\sigma_2}{2}) \\bigg] \\\\\n&= C(p,\\alpha)^{2/p}\\rho_{12}\\sigma_1\\sigma_2\n\\end{aligned}\n\\]\nThe subordinated covariance estimator is defined as \\(\\hat s\\) below:\n\\[\n\\begin{aligned}\n\\hat s &= \\frac{(\\frac{1}{N}\\sum_{i=1}^N |X_{1i}|^p)^{2/p} + (\\frac{1}{N}\\sum_{i=1}^N |X_{2i}|^p)^{2/p} -(\\frac{1}{N}\\sum_{i=1}^N |X_{1i} - X_{2i}|^p)^{2/p}}{C(p,\\alpha)^{2/p}}\n\\end{aligned}\n\\]\nOne immediate benefit of this estimator is that, when \\(p=2\\), this estimator is exactly the sample covariance (which is an input into the Median Oracle estimator). This provides a very reasonable path to improving the performance of Median Oracle - when \\(\\alpha&lt;2\\), the sample covariance is a biased estimator of the subordinated covariance, and \\(p=2\\) may not be optimial anymore.\nThe problem, though, is that this formulation likely won’t have finite moments due to the elements being raised to the second power.\nI’m continuing to explore potential estimators of this type.\n[Last Updated on July 24, 2024]"
  },
  {
    "objectID": "sce_derivation.html",
    "href": "sce_derivation.html",
    "title": "Subordinated Covariance Estimator Derivation",
    "section": "",
    "text": "We can use the same preliminaries and random variable setup from this page to get approximations for the expected value and variance of what I’m calling the Subordinated Covariance Estimator, \\(\\hat s\\).\nFirst, it’s not immediately obvious that the moments of this estimator are finite. Luckily, we can see that if \\(p &lt; \\alpha &lt;2\\) for the non-Gaussian case, or \\(p \\leq 2\\) for the Gaussian \\(\\alpha =2\\) case, since \\(f(x) = x^\\frac{2}{p}\\) is concave when \\(\\frac 2 p &gt; 1\\)\nUsing a second-order Taylor expansion around the means for the expectation, and setting \\(Y_{ji} = |X_{ji}|^p, Z_j = \\sum_{i=1}^N Y_{ji}\\), we get “simplified” univariate expansions compared to the correlation-type estimators that approximate the covariance of the subordinated Gaussian vector:\n\\[\n\\begin{aligned}\n\\mathbb E(\\hat s) &= \\mathbb E \\frac{(\\frac{1}{N}\\sum_{i=1}^N |X_{1i}|^p)^{2/p} + (\\frac{1}{N}\\sum_{i=1}^N |X_{2i}|^p)^{2/p} -(\\frac{1}{N}\\sum_{i=1}^N |X_{1i} - X_{2i}|^p)^{2/p}}{C(p,\\alpha)^{2/p}} \\\\\n&= N^{-2/p}C(p,\\alpha)^{-2/p}\\mathbb E\\bigg[(\\sum_{i=1}^N |X_{1i}|^p)^{2/p}\\bigg]+ \\mathbb E\\bigg[(\\sum_{i=1}^N |X_{2i}|^p)^{2/p}\\bigg]-\\mathbb E\\bigg[(\\sum_{i=1}^N |X_{1i} - X_{1i}|^p)^{2/p}\\bigg] \\\\\n&\\approx N^{-2/p}C(p,\\alpha)^{-2/p} \\bigg[ \\zeta_1^{2/p}+\\zeta_2^{2/p}-\\zeta_3^{2/p}-\n    \\sum_{i=1}^3\\frac{(p-2)\\zeta_i^{\\frac{2}{p}-2}}{p^2}\\text{Var}(Z_i)\n\\bigg]\\\\\n&= N^{-2/p}C(p,\\alpha)^{-2/p} \\bigg[ \\{NC(p,\\alpha)\\}^{2/p}(\\gamma_1^2+\\gamma_2^2-\\gamma_3^2)- \\\\\n&\\frac{(p-2)\\{NC(p,\\alpha)\\}^{\\frac{2}{p}-2}}{p^2} \\frac{2^{2p+2}}{N \\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\} \\bigg(\\gamma_1^{2p+\\frac{2}{p}-2} + \\gamma_2^{2p+\\frac{2}{p}-2} - \\gamma_3^{2p+\\frac{2}{p}-2}\\bigg)\\bigg] \\\\\n&= (\\gamma_1^2+\\gamma_2^2-\\gamma_3^2) - \\frac{(p-2)}{N^3\\{pC(p,\\alpha)\\}^{2}} \\frac{2^{2p+2}}{\\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\} \\\\ &\\times\\bigg(\\gamma_1^{2p+\\frac{2}{p}-2} + \\gamma_2^{2p+\\frac{2}{p}-2} - \\gamma_3^{2p+\\frac{2}{p}-2}\\bigg) \\\\\n& = \\text{Cov}(G_1,G_2) -\n\\frac{(p-2)}{N^3\\{pC(p,\\alpha)\\}^{2}} \\frac{2^{2p+2}}{\\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\} \\\\ &\\times\\bigg(\\gamma_1^{2p+\\frac{2}{p}-2} + \\gamma_2^{2p+\\frac{2}{p}-2} - \\gamma_3^{2p+\\frac{2}{p}-2}\\bigg)\n\\end{aligned}\n\\]\nThe result above uses the expansion of the rescaled numerator of r from the progress page to move from the stable scale parameters to the covariance of the Gaussian vector.\nMoving on to the variance of \\(\\hat s\\), with \\(f(Z_1, Z_1, Z_1) = (C(p, \\alpha)N)^{-2/p}[ Z_1^{2/p}+Z_2^{2/p}-Z_3^{2/p}]\\):\n\\[\n\\begin{aligned}\n\\text{Var}(\\hat s) &\\approx  \n\\bigg(\\frac{\\partial f}{\\partial Z_1}\\bigg|_{\\zeta}\\bigg)^2\\text{Var}(Z_1) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_2}\\bigg|_{\\zeta}\\bigg)^2\\text{Var}(Z_2) +\n    \\bigg(\\frac{\\partial f}{\\partial Z_3}\\bigg|_{\\zeta}\\bigg)^2\\text{Var}(Z_3) +\n    2\\frac{\\partial f}{\\partial Z_1}\\frac{\\partial f}{\\partial Z_2}\\bigg|_{\\zeta}\\text{Cov}(Z_1, Z_2) + \\\\ &\n     2\\frac{\\partial f}{\\partial Z_1}\\bigg|_{\\zeta}\\frac{\\partial f}{\\partial Z_3}\\bigg|_{\\zeta}\\text{Cov}(Z_1, Z_3) +  2\\frac{\\partial f}{\\partial Z_2}\\frac{\\partial f}{\\partial Z_3}\\bigg|_{\\zeta}\\text{Cov}(Z_2, Z_3) \\\\\n&= C(p, \\alpha)^{-2}\\bigg[\\frac{4}{p^2} \\bigg\\{\\gamma_1^{(4p-2)p}\\text{Var}(Z_1)+\\gamma_2^{(4p-2)p}\\text{Var}(Z_2)+\\gamma_3^{(4p-2)p}\\text{Var}(Z_3)\\bigg\\}+\\\\ &\\frac{8}{p^2}\\bigg\\{(\\gamma_1^p)^{2/p-1}(\\gamma_2^p)^{2/p-1}\\text{Cov}(Z_1, Z_2)-(\\gamma_1^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(Z_1, Z_3)-(\\gamma_2^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(Z_2, Z_3)\\bigg\\}\n\\bigg]\\\\\n&= N^{-1}C(p, \\alpha)^{-2}\\bigg[\\frac{4}{p^2} \\bigg\\{\\gamma_1^{(4p-2)p}\\text{Var}(|X_1|^p)+\\gamma_2^{(4p-2)p}\\text{Var}(|X_2|^p)+\\gamma_3^{(4p-2)p}\\text{Var}(|X_3|^p)\\bigg\\}+\\\\\n&\\frac{8}{p^2}\\bigg\\{(\\gamma_1^p)^{2/p-1}(\\gamma_2^p)^{2/p-1}\\text{Cov}(|X_1|^p, |X_2|^p)-(\\gamma_1^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(|X_1|^p, |X_3|^p)-\\\\ &(\\gamma_2^p)^{2/p-1}(\\gamma_3^p)^{2/p-1}\\text{Cov}(|X_2|^p, |X_3|^p)\\bigg\\}\n\\bigg]\\\\\n\\end{aligned}\n\\]\nThere is some additional simplification that happens with the variance terms (they’re all identical except for a \\(\\gamma_i^{2p}\\)), but the expression becomes very complicated very quickly, and doesn’t have any obvious simplifications or tricks to make it more interpretable (unlike the expected value). At this point may as well be calculated numerically."
  },
  {
    "objectID": "index.html#research-plan",
    "href": "index.html#research-plan",
    "title": "Research Progress Site",
    "section": "Research Plan",
    "text": "Research Plan\n\nFind optimal estimator for underlying dependence\nInvestigate potential benefits of a median-of-means approach for heavy tails and small observations\nInvestigate high-dimensionality adjustments\nApplications"
  },
  {
    "objectID": "progress.html#the-subordinated-covariance-estimators",
    "href": "progress.html#the-subordinated-covariance-estimators",
    "title": "Current Progress",
    "section": "The Subordinated Covariance Estimators",
    "text": "The Subordinated Covariance Estimators\n\n\n\n\n\n\nImportant\n\n\n\nNote: This estimator is also being put on ice due to infinite moments. I’m continuing to investigate more estimators of this type to see if I can get something that reverts to sample covariance as a special case when the data are Gaussian, but has better moment properties. Alternatives to estimator variance might also be helpful.\n\n\nIf we just take the rescaled numerator of r, we get the following:\n\\[\n\\begin{aligned}\n&(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p} \\\\\n&= (C(p,\\alpha)\\gamma_1^p)^{2/p}+(C(p,\\alpha)\\gamma_2^p)^{2/p}-(C(p,\\alpha)\\gamma_3^p)^{2/p}\n\\\\\n& = C(p,\\alpha)^{2/p}\\bigg[\\gamma_1^2 + \\gamma_2^2 -\\gamma_3^2 \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -\\frac{\\sigma_3^2}{2}  \\bigg] \\\\\n&=C(p,\\alpha)^{2/p}\\bigg[\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2} -(\\frac{\\sigma_1^2}{2} + \\frac{\\sigma_2^2}{2}-\\frac{2\\rho_{12}\\sigma_1\\sigma_2}{2}) \\bigg] \\\\\n&= C(p,\\alpha)^{2/p}\\rho_{12}\\sigma_1\\sigma_2\n\\end{aligned}\n\\]\nThe subordinated covariance estimator is defined as \\(\\hat s\\) below:\n\\[\n\\begin{aligned}\n\\hat s &= \\frac{(\\frac{1}{N}\\sum_{i=1}^N |X_{1i}|^p)^{2/p} + (\\frac{1}{N}\\sum_{i=1}^N |X_{2i}|^p)^{2/p} -(\\frac{1}{N}\\sum_{i=1}^N |X_{1i} - X_{2i}|^p)^{2/p}}{C(p,\\alpha)^{2/p}}\n\\end{aligned}\n\\]\nOne immediate benefit of this estimator is that, when \\(p=2\\), this estimator is exactly the sample covariance (which is an input into the Median Oracle estimator). This provides a very reasonable path to improving the performance of Median Oracle - when \\(\\alpha&lt;2\\), the sample covariance is a biased estimator of the subordinated covariance, and \\(p=2\\) may not be optimial anymore.\nThe problem, though, is that this formulation cannot have finite moments (this follows from Jensen’s inequality; we need \\(p&gt;2\\) to be finite for finite moments, which is a contradiction.)\nI’m continuing to explore potential estimators of this type.\n\nSubordinated Correlation Estimator\nMy derivation of the FLOM covariance leads to (what I believe is) a novel estimator for the correlation coefficient of the subordinated Gaussian vector.\nUsing the fact that: \\[\n\\begin{aligned}\n\\mathbb E(|X_1 X_2|^p) &= \\frac{2\\Gamma(-2p/\\alpha)}{\\alpha\\Gamma(-p)}\\bigg(\\frac{2^{p+1} \\gamma_1^{p/} \\gamma_2^{p}}{\\pi}\\bigg)\\Gamma(\\frac{p+1}{2})^2{}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)\n\\end{aligned}\n\\]\nThis requires that \\(p &lt; \\alpha/2\\). From there, we can get the following estimator for the underlying correlation between \\(G_1\\) and \\(G_2\\):\n\\[\n\\begin{aligned}\n\\hat \\rho = \\bigg( \\frac 1 N \\sum_{i=1}^N \\text{sign}(X_1 X_2) \\bigg) \\bigg| {_2F_{1p}}^{-1}\\bigg( \\frac{\\alpha \\pi \\Gamma(-p)\\sum_{i=1}^N|X_1 X_2|^p}{2^{p+2}(\\gamma_1 \\gamma_2)^p\\Gamma(\\frac{p+1}{2})^2}\\bigg)\\bigg|^{1/2}\n\\end{aligned}\n\\]\nHere, I use \\({_2F_{1p}}^{-1}(\\cdot)\\) to denote the inverse of the hypergeometric function \\({}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2)\\), such that \\({_2F_{1p}}^{-1}({}_2F_1(-\\frac p 2, -\\frac p 2, \\frac 1 2, \\rho^2))=\\rho^2\\). While a bit nasty and not ideal for analyzing the behavior of this estimator, this can be implemented numerically without much trouble.\nIt’s worth taking a second to justify taking the mean of the sign function, since \\(\\mathbb E X_1 X_2 \\nless \\infty\\). Note that \\(\\text{sign}(XY) \\in \\{-1, 1\\}\\), so the random variable is bounded and all moments are finite. Equivalently, \\(\\mathbb E[\\text{sign}(XY)] = P(XY &gt; 0)- P(XY &lt; 0) \\in (-1,1)\\), which always exists.\nBecause of our expectation definition above, this estimator will converge to \\(\\rho\\) by the law of large numbers. Better yet, it will have finite variance when \\(p &lt; \\alpha/2\\), which is required to get the subordinator to have finite expectation. This is a somewhat stronger claim for finite variance than the correlation estimator candidate offered above (which was made simply on the basis of it being bounded). So it might have better small-sample performance.\nThere are a couple of problems with this estimator: first, I don’t really expect it to perform incredibly well because of the need to estimate other parameters as inputs. Second, it doesn’t fix the issue of correlation-only estimates having worse performance in Median Oracle. The latter is fixed by recombining it with estimates of \\(\\alpha, \\sigma_1, \\sigma_2\\), which can be made quickly and accurately without regard for the dimensionality of the problem (e.g. using MLE).\nIt’s definitely worth testing this estimator next.\n[Last Updated on July 31, 2024]"
  },
  {
    "objectID": "why_stable.html",
    "href": "why_stable.html",
    "title": "Why Stable?",
    "section": "",
    "text": "Note\n\n\n\nThis page tries to explain the benefits of a stable model conversationally. I think a more formal, example-filled, adequately sourced version of this discussion would be useful early in my dissertation. But hopefully, this version is helpful for understanding my desire to use the stable model!\nThe question has come up – why use an alpha-stable distribution to model investment returns in the first place?\nIt’s worth noting that my goal isn’t necessarily to build an estimator that only works on stable distributions, but rather to build one that doesn’t immediately break if the distribution of the data is heavy tailed. Working with the stable distribution gives us something to anchor to for theoretical results.\nEven when variance exists, my conjecture is that in the presence of heavy tails, a model that can accomodate infinite variance probably does better at measuring dependence in finite samples.\n(Or to paraphrase a comment I read on the internet: in the real world, a slowly converging integral is no better than one that doesn’t converge at all.)"
  },
  {
    "objectID": "why_stable.html#why-stable",
    "href": "why_stable.html#why-stable",
    "title": "Why Stable?",
    "section": "Why Stable?",
    "text": "Why Stable?\nStable distributions generalize the normal distribution, which has typically been used commonly in finance, allowing for heavier tails – and shocks or jumps.\nArguably, for any stochastic additive process with countably infinite increments (e.g. like log-returns), if “sufficiently large” summands aren’t suitably well-behaved, we have an argument for using a stable distribution. More concretely, when the central limit theorem provides a justification for using the normal distribution throughout mathematical finance, when the variance of those increments is infinite or the tails of the increments are large enough that their sums have slow convergence to the Gaussian regime, a stable distribution makes more sense.\nBy the Generalized CLT of Gnedenko and Kolmogorov (1954), the sums of any i.i.d. random variables must be alpha-stable in distribution.\n\n\n\nFrom Meucci’s “Risk and Asset Allocation”, stable distributions in the taxonomy of distributions; N denotes the normal.\n\n\nFinancial returns have been observed to be strongly non-Gaussian for nearly a century, and in the early 1960s, the stable distribution was suggested as a model for financial returns by Benoit Mandlebrot. Around that same time, significant research was done in extending Gaussian assumptions in asset pricing models to the more realistic stable distribution by future Nobel laureate Eugene Fama. More recently, books like Stable Paretian Models in Finance have extended more contemporary financial models to being driven by stable distributions.\n\nWhy Aren’t Stable Models More Common in Finance?\nDespite significant theory and justification for using stable distributions, they’re not commonly used in practice. One reason for that is that they’re less accessible to non-mathematicians than simpler Gaussian models, while also implying less explainability than more sophisticated models that only work in the rearview mirror (e.g. ex post, there is more noise).\nI’d speculate that another reason has to do with timing. When Harry Markowitz introduced Modern Portfolio Theory in the 1950s, he brought the idea that variance should be used as a measure of investment risk, and presented a framework for portfolio contruction that implied Gaussian returns - the mean-variance approach pioneered by Markowitz at this time only guarantees optimality if the distribution of returns is fully parameterized by its mean and variance, which forms a complete sufficient statistic for the Gaussian. This made the normal distribution a more appealing choice during this time.\nBy the time enough computational power was available to deal with stable distributions (and, for instance, calculate their PDFs and estimate parameters using MLE), that computational power could be used instead for nonparametric/empirical approaches, and many practitioners burned by failures of the Gaussian model were turned off by parameteric approaches. In some sense, stable distributions missed the boat despite arguably being a better model than either (empirical distributions in particular are problematic because of the “unseen” tail that’s impossible to estimate for events that haven’t happened yet).\nThe idea of using standard deviation to define investment volatility (and risk) has likely been one of the biggest challenges to seeing stable models get used more widely. Some have argued that infinite variance implies infinite risk, which is not correct.\nIn fact, stable distributions have well-defined scale parameters that behave just like standard deviation in the Gaussian case – as I’ll go into below, they are exactly related to standard deviation in the \\(\\alpha\\)-SG(\\(\\Sigma\\)) case.\nThese scale parameters have direct applications to portfolio optimization when simple returns are stable, and more importantly, they describe the decreasing marginal benefit to diversification that we see in the real world when returns come from heavy-tailed distributions (See Nolan, 2020, p. 35)."
  },
  {
    "objectID": "why_stable.html#why-sub-gaussian",
    "href": "why_stable.html#why-sub-gaussian",
    "title": "Why Stable?",
    "section": "Why Sub-Gaussian",
    "text": "Why Sub-Gaussian\nWhen thinking about stable distributions as a generalization of the Gaussian, the \\(\\alpha\\)-SG(\\(\\Sigma\\)) class of stable distributions can be thought of as a minimal relaxation: they share all the dependence structure of a Gaussian, but have the ability to present heavier tails and jumps.\nStable distrbutions lend themselves well to portfolio construction when they model simple returns; the dispersion matrix \\(\\Sigma\\) has been shown to be a coherent risk metric that can be used to build a portfolio when an agent is averse to risk. (See Kring, et al., 2009)\nWhile more complicated dependence structures are possible with a stable distribution by lifting the symmetry assumption, I think the tradeoff lies in the ability to estimate those parameters well enough as well as the futility of trying to accomodate those dependence structures in a portfolio whose returns are an affine transformation of the underlying return vector. In other words, the juice isn’t worth the squeeze for a more complex model."
  },
  {
    "objectID": "why_stable.html#why-infinite-variance",
    "href": "why_stable.html#why-infinite-variance",
    "title": "Why Stable?",
    "section": "Why Infinite Variance",
    "text": "Why Infinite Variance\nInfinite variance itself isn’t necessarily a desirable attribute for our model distribution. Rather, it is a side effect of having a target distribution that can accomodate realistic jumps/shocks/outliers.\nWhile it’s true that with a stable distribution we have positive probability of seeing an arbitrarily large realization, the same is true of the normal distribution too!\nSo, this question is less “why infinite variance?” and more “why not infinite variance?”\nUltimately, infinite variance says more about a distribution’s speed of tail decay than it does about the likelihood of an extreme event in general. I’ll attempt to address some possible objections to infinite variance models below:\nInfinite variance does not say anything about the variability of a distribution\nWhile variance can be a useful metric to compare the variability of distributions, once it’s infinite, it’s no longer meaningful to compare.\nAs an example, take the Cauchy distribution below, a symmetric distribution which has infinite integer moments:\n\nCompare this to a \\(N(10^8,1)\\) distribution, and taking random deviates from each, it’s clear that the Cauchy’s central tendency or typical realizations are not “higher” in any sense. It’s is simply no longer a meaningful quantity (no pun intended) because the integral fails to converge.\nSimilarly, the stable random variable \\(X \\sim S_{1.9}(0.01,0,0)\\) with 99.9985% of its probability density on [-1,1] has infinite variance. When the second moment doesn’t converge, that’s really only a commentary on the speed of tail decay relative to the expectation being integrated.\nInfinite variance models are a realistic choice for investment returns.\nEmpirically, investment returns are heteroskedastic. This has led to many more complicated models to explain the behavior of prices (regime models, stochastic volatility, etc.).\nBut looking at even a very simple Lèvy flight driven by a stable distribution, the empirical rolling standard deviation is quite well modeled by stable returns:\n\n\n\nFrom my Stable Bands paper (2023); Rolling 20-day standard deviation of S&P 500 returns in red vs. the same from an infinite variance stable distribution fit to S&P returns in blue.\n\n\nFinite variance does not solve the practical challenges of heavy tails\nWhen we allow the second moment to exist in a heavy-tailed distribution, we do regain some nice theoretical properties but, in practice, results will largely be indistinguishable from the infinite variance regime.\nFor example even if variance is finite, convergence of the integral (and by extension a sample-based estimate of it) may be so slow that finite-sample estimates are effectively stochastic. This is almost more problematic because it creates a tendency to be overconfident in higher-moment estimates.\nWith the stable model, the \\(\\alpha\\) parameter can be treated like a confidence metric for moment estimates; for instance, \\(\\alpha\\) near 2 suggests that the mean will likely be well behaved and reliably estimated in small samples.\nInfinite variance does not imply infinite or undefined variability.\nWhile variance may be undefined, other measures of variability, such as mean absolute deviation will exist when \\(\\alpha&gt;1\\). Likewise, median absolute deviation will always exist. Arguably, even if variance did exist as well, one of these more robust measures of variability may be a more appropriate/interpretable quantity in a heavy-tailed application like modeling financial returns.\nLikewise, while theoretical variance may be undefined for a stable distribution, sample variance from a realization of that stable distribution will always exist and controlling it is a common investment objective in portfolio construction.\nIn that case, it’s straightforward that the minimum stable scale portfolio will also be the portfolio with the smallest expected median realized standard deviation. Similar extensions can be made for other objectives.\nMoreover, by appreciating the fact that target metrics may not converge in expectation, it’s possible to treat them more realistically - e.g. by building a portfolio that has a 90% probability of not exceeding an undesirable sample standard deviation.\nInfinite variance does not mean that tail events are more likely.\nIt’s true that under some regularity conditions (e.g. unimodal and monotonically decreasing tails) a stable random variable will have higher variability (however you choose to measure it) than, say, a normal random variable with the same scale parameter. But that setup artificially handicaps the stable random variable.\nWith some of the variability “handled” by a lower \\(\\alpha\\), the stable random variable fit to the same data as a Gaussian will have a lower scale parameter.\nOne way to see this is that by choosing a Gaussian with the probability of an “extreme event” p, I can choose a scale parameter for a stable random variable with arbitrary choice of \\(\\alpha\\) such that the probability of the same extreme event is less than p.\nFor this reason, infinite variance distributions are more reasonable than normal distributions for modeling exceptionally low-volatility assets that endure rare, but proportionally meaningful price shocks.\nAn unbounded distribution is a reasonable choice for investment returns.\nBecause this is also a characteristic of the Gaussian, this question doesn’t get addressed often.\nThere is no economic theory I’m aware of that would justify hard bounds on investment returns. Anything is possible, however vanishingly small the probability of a quadrillion-percent move. In fact, because of the fact that investment returns are heavy-tailed imposing a hard upper limit (and getting it wrong) will have drastic consequences on the interpretation of the model."
  }
]