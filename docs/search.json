[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Progress Site",
    "section": "",
    "text": "I’ll likely add more structure to this site as I collect more results than I can fit in this format.\nThe links above provide general background on my problem setting, current progress, and some additional questions (and hopefully their answers) that might not be perfectly aligned with that path."
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "Background",
    "section": "",
    "text": "Since I realize that this problem setting may be a little unfamiliar, I’m including a brief background on \\(\\alpha\\)-stable random variables and the research question.\nI’m interested in estimating the dependence of a random vector when covariance may not exist and the dimension of the vector may be high relative to the number of observations. We’re principally interested in a symmetric, positive semi-definite covariance matrix-like object \\(\\Sigma\\) or its corresponding correlation matrix, since:\nIn financial markets, distributions of returns (and log-returns, which are used interchangeably and often without distinction) often have heavy tails. This leads to the relaxation of the finite variance assumption. Even if a process has finite variance, I my conjecture is that an estimator that admits heavy tails may be better (e.g. more efficient) in finite/small samples.\nIn the literature, correlation and covariance matrices are used interchangably, since the correlation matrix is just a covariance matrix of an alternative dataset that has been standardized."
  },
  {
    "objectID": "background.html#stable-distributions-and-heavy-tailed-random-vectors",
    "href": "background.html#stable-distributions-and-heavy-tailed-random-vectors",
    "title": "Background",
    "section": "Stable Distributions and Heavy-Tailed Random Vectors",
    "text": "Stable Distributions and Heavy-Tailed Random Vectors\nIf we relax the assumption that variance is finite, then a reasonable model for returns is the stable distribution, which has no closed form pdf in general but has where the dependence is fully defined by the spectral measure (typically denoted \\(\\Gamma\\)), a finite measure on the unit circle \\(\\mathbb S^d\\).\nStable distributions are a reasonable choice because they are infinitely divisible, and by the Generalized Central Limit Theorem of Gnedenko and Kolmogorov (1954), if sums of i.i.d. random variables converge to a distribution, it must be stable.\nThe normal distribution is a special case when \\(\\alpha = 2\\). It is the only case where variance is finite. The two other closed form pdfs of the stable family are the Cauchy (\\(\\alpha = 1\\)) and the Lévy (\\(\\alpha = 1/2\\)).\nStable distributions are admittedly confusing because of the complicated characteristic function and infinite variance, as well as the fact that some terms (e.g. sub-Gaussian) mean something totally different in the stable distribution context than in the broader statistics context.\nLikewise there are nearly a dozen known parameterizations of the stable distribution, which makes it hard to distinguish what an author means.\nA special case is the sub-Gaussian (aka subordinated Gaussian) symmetric \\(\\alpha\\)-stable distribution, which for a random vector X can be written:\n\\[\nX = \\sqrt W G\n\\]\nWhere W \\(\\sim S_{\\alpha/2}(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers, and \\(G \\sim N(0, \\Sigma)\\), a multivariate normal random vector independent of W.\nIn this sub-Gaussian random vector, the spectral measure \\(\\Gamma\\) is fully determined by \\(\\Sigma\\). In fact, \\(\\Gamma\\) symmetric (e.g. antipodal points have equal measure), and continuous. While it’s a bit counter-intuitive, even in the isotropic case where \\(\\Sigma = I_d\\), the identity matrix, the components of X are not independent. Indeed, the idea of orthogonality isn’t defined (although some generalizations of this idea are), since X is not defined on a (pre-)Hilbert space when \\(\\alpha &lt; 2\\). This is a feature, not a bug, since tail dependence is asymptotically zero in the Gaussian case, a contributor to the failure of the Gaussian copula model for derivative pricing during the 2008 financial crisis. This model errs on the side of overestimating risk.\nX has the following simplified, more familiar characteristic function:\n\\[\n\\mathbb E[e^{iu^\\text T X}] = exp\\bigg\\{-\\frac12 |u^\\text T \\Sigma u|^{\\alpha/2}\\bigg\\}\n\\]\nWhen \\(\\alpha =2\\), this is the Gaussian CF.\nWe want the best estimate of \\(\\Sigma\\) or its corresponding correlation matrix.\nThe best reference on multivariate stable distributions is Stable Non-Gaussian Random Processes by Samoradnitsky and Taqqu (1994), which contains virtually all known theoretical results as of its publication (despite the book’s 30th anniversary, not much has changed). For a more modern treatment that’s limited to univariate stable random variables, John Nolan’s aptly-named Univariate Stable Distributions: Models for Heavy Tailed Data from 2020 is recommended.\nOne important note is that parameterizations of stable distributions are very inconsistent in the literature."
  },
  {
    "objectID": "background.html#high-dimensional-dependence-estimation",
    "href": "background.html#high-dimensional-dependence-estimation",
    "title": "Background",
    "section": "High-Dimensional Dependence Estimation",
    "text": "High-Dimensional Dependence Estimation\nThe other piece of the research question puzzle is “high-dimensional” data. By high-dimensional, I mean that the dimension of the data is high relative to the number of observations; so 30 dimensions is quite high-dimensional when you only have 35 observations to estimate something like a covariance matrix from.\nThe current state-of-the-art approach used in finance is the Optimal Rotationally-Invariant Estimator (RIE), which uses results from Random Matrix Theory to find the optimal way to “clean” the eigenvalues of a sample covariance matrix C so that the squared Frobenius distance \\(||\\Sigma - C||_F^2\\) is minimized, even without being able to observe the true \\(\\Sigma\\), provided that the data are at least asymptotically normal.\nInitially, my plan was to explore how this recipe might change if the underlying data do not have finite variance (e.g. finding \\(\\Sigma\\) from a sub-Gaussian \\(\\alpha\\)-stable random vector described above). More recently, I’ve found some research that suggests that if \\(1 &lt; \\alpha \\leq 2\\), then the universality rules apply asymptotically and this same RIE formulation is appropriate to use for a Lévy matrix. If true, this would provide a theoretical justification for using this approach for heavy-tailed data."
  },
  {
    "objectID": "background.html#current-estimator",
    "href": "background.html#current-estimator",
    "title": "Background",
    "section": "Current Estimator",
    "text": "Current Estimator\nMy candidate estimator, Median Oracle, takes a very simple heuristic approach to estimating the quasi-correlation from a matrix \\(X\\) of returns. Assume \\(X\\) is a \\(t\\times n\\) matrix consisting of \\(t\\) independent observations of a market of \\(n\\) assets, and a sub-sample size \\(d_in\\):\n\n(Optional_ Rescale the columns of \\(X\\) to be mean-zero with unit variance, such that we’re estimating \\(\\Sigma\\) as a correlation matrix.\nFor \\(i=1,..., B\\):\n\nDraw \\(b \\sim Unif[1,t-d_in)\\)\nAt each \\(b\\), subsample \\(X_b = X_{b:(b+d_in-1)}, \\cdot\\)\nCompute the sample covariance matrix \\(\\Sigma_b = \\frac{1}{d_{in}-1}X_b^\\text T X_b\\).\nCompute the Rotationally Invariant Estimator (RIE) adjustment to the eigenvalues \\(\\hat\\lambda_{1}^{(1)}, ..., \\hat\\lambda_{i}^{(n)}\\) of \\(\\Sigma_b\\), and store them in \\(Q \\in \\mathbb R^{B \\times n}\\)\n\nFor for each eigenvalue \\(\\lambda^{(j)}\\) of the covariance matrix, \\(j = 1, ..., n\\), estimate \\(\\hat \\lambda^{(j)} = \\text{median}(\\hat\\lambda_{i}^{(j)})\\), the median across all \\(B\\) samples.\nReconstruct the covariance matrix using \\(\\hat\\Lambda = \\text{diag}(\\hat \\Lambda^{(n)},...,\\hat \\lambda^{(1)})\\) and the eigenvectors of the full-sample covariance matrix \\(\\Sigma = V^\\text{T} \\Lambda V\\).\nRescale by forecasted asset-wise standard deviation to get a final estimate of the underlying covariance matrix.\n\nTaking the sample covariance and correlations of a dataset assumed to be generated by an infinite-variance process seems crazy initially, but in fact the eigenvalue spectrum of these subsampled covariance matrices are well defined distributions."
  },
  {
    "objectID": "open_questions.html",
    "href": "open_questions.html",
    "title": "Some Questions (And Answers)",
    "section": "",
    "text": "Can we generalize the equivalence property of Fractional Lower-Order Moments (FLOMs) to other distributions where covariance exists?\n\n\n\n\n\n\n\nAnswer: Yes.\n\n\n\n\n\nThis becomes obvious from the definition of the characteristic function. Let \\(X = \\mu + \\gamma Z\\) be a random variable with mean \\(\\mu\\) (note that \\(\\mathbb EX \\nless \\infty\\) is not actually required. If X is stable with \\(\\alpha \\leq 1\\), we replace mean with location and it still works - but X need not be a location scale family), and characteristic function \\(\\varphi_X(t)\\). Then for any p for which \\(\\varphi_X(t)\\) is differentiable at \\(t=0\\), the p-th fractional moment exists (Laue, 1980) and can be factored to some constant that depends only on the distribution parameters \\(\\mathbf \\theta\\) times the scale parameter survives raised to the p-th power thanks to the product rule:\n\\[\n\\begin{aligned}\n\\mathbb E(|X-\\mu|^p) & = \\mathbb E(\\gamma Z_+^p)+\\mathbb E (\\gamma Z_-^p)\\\\\n& = (-i)^p \\bigg(\\frac{d^p}{dt^p}\\varphi_{\\gamma Z_+}(t)+\\frac{d^p}{dt^p}\\varphi_{\\gamma Z_-}(t)\\bigg)\\bigg |_{t=0} \\\\\n& = (-i)^p\\frac{d^p}{dt^p}\\bigg(\\varphi_{Z_+}(\\gamma t)+\\varphi_{Z_-}(\\gamma t)\\bigg)\\bigg |_{t=0} \\\\\n& = \\gamma^p c(\\mathbf \\theta)\n\\end{aligned}\n\\]\nWhen the second moment exists, note that \\(\\gamma^2\\) is at worst a rescaled parameterization of the variance of the distribution, since:\n\\[\n\\gamma^2 c(\\mathbf \\theta) = \\mathbb E(|X-\\mu|^2) = \\text{Var}(X) \\implies \\gamma^2 \\propto \\text{Var}(X)\n\\]\nThis result is helpful for a couple of reasons.\n\nIf we’re comparing random varables that differ only by a scale parameter (e.g. otherwise, they share \\(\\mathbf \\theta\\)), we can get a good estimate of “correlation” because these constants will cancel.\nIf we apply FLOMs to arbitrary random variables with finite variance, the p-th root of the FLOM will be proportionate to the actual standard deviation. This makes this estimator robust to model misspecification (think lognormal distribution)\n\n\n\n\n\nWhat p is actually optimal, given some \\(\\theta\\).\nTo what extent does the resulting correlation matrix result in eigenvalue regularization compared to a sample covariance matrix? What does that regularization function look like?\nIs RIE appropriate to use on sample covariance estimates derived from stable data?\n\n(Two approaches: universality when \\(\\alpha &gt;1\\), \\(\\Sigma\\) as a covariance matrix of a Gaussian vector)"
  },
  {
    "objectID": "progress.html",
    "href": "progress.html",
    "title": "Current Progress",
    "section": "",
    "text": "I’ve split the problem into two pieces: getting an “optimal” estimate of the covariance/correlation matrix underlying a stable-sub-Gaussian random vector, and adjusting for high-dimensionality. It’s a little hand-wavy, but I think this is reasonable since the RIE is a monotonically increasing function of the eigenvalues of the input. So, by the invariance of quantiles under monotone transformations, the median RIE estimate at each eigenvalue should be the same as the RIE of the median eigenvalue estimate."
  },
  {
    "objectID": "progress.html#estimating-sigma",
    "href": "progress.html#estimating-sigma",
    "title": "Current Progress",
    "section": "Estimating \\(\\Sigma\\)",
    "text": "Estimating \\(\\Sigma\\)\nKodia and Garel propose a signed symmetric covariation coefficient:\n\\[\n\\text{scov} (X_1, X_2) = \\kappa_{(X_1, X_2)}\\bigg | \\frac{[X_1, X_2]_\\alpha [X_2, X_1]_\\alpha}{||X_1||_\\alpha ||X_2||_\\alpha} \\bigg|^{1/2}\n\\]\nHere, \\([X_1, X_2]_\\alpha\\) and \\(||X_1||_\\alpha\\) are the covariation of \\(X_1\\) on \\(X-2\\) and the covariation norm of \\(X_1\\), respectively, and \\(\\kappa\\) represents the “agreement” of the signs of the covariation terms.\nWhen \\(1 &lt; \\alpha &lt; 2\\), this can equivalently be represented using Fractional Lower Order Moments (FLOMs):\n\\[\nr = \\frac{(\\mathbb E |X_1|^p)^{2/p}+(\\mathbb E |X_2|^p)^{2/p}-(\\mathbb E |X_1- X_2|^p)^{2/p}}{2(\\mathbb E |X_1|^p\\mathbb E |X_2|^p)^{1/p}}\n\\]\nIn both cases, when \\(X\\) is a sub-Gaussian random vector with underlying Gaussian vector \\(G\\), these quantities coincide with the correlation coefficient between \\(G_1\\) and \\(G_2\\).\nKodia and Garel show that straightforward estimators based on these quantities are strongly consistent estimators of the Gaussian correlation matrix.\n\nBias of the FLOM Estimator\nNote that FLOMs aren’t unbiased estimators of scale/dependence unless \\(p=2\\).\nProposition 1 on p.32 of Nikias and Shao is:\n\\[\n\\mathbb E(|X|^p) = C(p,\\alpha)\\gamma^{p/\\alpha}\n\\]\nwhere \\(C(p,\\alpha)\\) is a known constant that depends only on p and the tail exponent of X and is used below, and \\(\\gamma\\) is the scale parameter of X. Note that the authors use a non-typical parameterization for \\(\\gamma\\) but this result holds in general for any distribution – rough proof here.\nOne nice thing is that if we’re estimating a correlation-type quantity like scov and r above, these bias terms cancel out in the numerator and denominator, and we’re left only with the scale terms.\n\n\nVariance of the FLOM Estimator\nOne area that (surprisingly) has received little attention in the literature is the variance of these FLOM estimators. While the variance of \\(X\\) may be infinite, we can estimate its scale and dependence using FLOMs with finite variance.\nIt’s known that for \\(p&lt;\\alpha, \\mathbb E |X|^p &lt; \\infty\\). Therefore, the variance of \\(|X|^p\\) exists for \\(p&lt;\\alpha/2\\). Several papers estimate sample FLOM variance using Monte Carlo simulations, but I haven’t seen analytical representations in any papers. In this case, it’s a straightforward application of the expected value of the FLOM:\n\\[\n\\begin{aligned}\n\\text{Var}(\\frac{1}{N}\\sum_{i=1}^N |X|^p) & = N^{-2} \\text{Var}(\\sum_{i=1}^N |X|^p) \\\\\n&= N^{-1} [\\mathbb E(|X|^{2p}) - \\mathbb E(|X|^p)^2] \\\\\n&= N^{-1} [C(2p,\\alpha)\\gamma^{2p/\\alpha}-(C(p, \\alpha)\\gamma^{p/\\alpha})^2]\\\\\n&= \\frac{2^{2p+2}\\gamma^{2p/\\alpha}}{N \\alpha \\sqrt \\pi}\\bigg\\{ \\frac{\\Gamma(\\frac{2p+1}{2})\\Gamma(-2p/\\alpha)}{\\Gamma(-p)} - \\frac{\\Gamma(\\frac{p+1}{2})^2 \\Gamma(-p/\\alpha)^2 }{\\alpha \\sqrt\\pi\\Gamma(-p/2)^2} \\bigg\\}\n\\end{aligned}\n\\]\nWhat’s interesting is that simulation results are problematically misleading when \\(X\\) is nearly Gaussian:\n\nThe chart above simulates 10,000 samples to estimate the standard error of the FLOM estimator as a function of p at various values of \\(\\alpha\\). When p gets near the infinite variance regime, simulations will materially underestimate the variability of the estimator, especially when \\(\\alpha \\uparrow 2\\). Heuristically, this is because tails are only a little heavier than the Gaussian case, and you’re much less likely to get extreme realizations that traverse the full support of the distribution via simulation unless your sample size is massive. In these examples, the simulation approach requires tens of millions of simulations to get close to the analyical standard deviation of the estimator.\nI think this is an incredibly interesting aside that I haven’t seen covered in the literature so far.\nThis is relevant because some papers have suggested choosing optimal p to be \\(p \\lesssim \\alpha/2\\) (e.g. right on the boundary of finite variance).\nThe variance of the FLOM is monotonically increasing in p, and some have suggested making p as small as possible. The difficulty is that \\(\\lim_{p \\rightarrow 0} C(p, \\alpha) = \\infty\\). This problem doesn’t exist in the correlation estimator case.\n\n\nVariance of the Correlation Estimators\nWhen our estimator is a correlation-type estimator like scov and r above, any sample estimate is bounded on \\([-1,1]\\) and all moments of the estimator itself are finite for any p – even \\(p&gt;\\alpha\\)!\nLike with raw FLOMs, Monte Carlo has been used to estimate variance of these estimators in some papers, but only with relatively small simulation sizes and without comment on potential errors in the calculation of an optimal p.\nSince we can induce finite variance on the components of these estimators, it seems reasonable to approximate their asymptotic variance using Taylor series expansions to get a better estimate of a variance-minimizing choice of p.\nMy derivation of the correlation-type estimator variance approximation can be found here.\n[Last Updated on July 16, 2024]"
  },
  {
    "objectID": "corr_variance_derivation.html",
    "href": "corr_variance_derivation.html",
    "title": "Deriving Correlation Estimator Variance",
    "section": "",
    "text": "Starting with couple preliminaries that will be useful to derive an analyticaly tractable expression for the variance of these correlation estimators, \\(\\hat r\\) and \\(\\hat{ \\text{scov}}\\) based on the sample FLOM estimators.\n\n\nFirst, deriving the expected value for W \\(\\sim S_{\\alpha/2}(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers. We can use the fact that we can decompose a sg-S\\(\\alpha\\)S(1) random variable into the product of W and normal random variable \\(G \\sim N(0,1)\\) independent of W:\n\\[\n\\begin{aligned}\n\\mathbb E(|X|^p) &= \\mathbb E(|W^{1/2}G|^p) \\\\\n&= \\mathbb E(W^{p/2}|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\mathbb E(|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\sigma^p 2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n&= \\mathbb E(W^{p/2}) (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n\\\\ \\text{So we get:} \\\\\n\\mathbb E(W^{p/2}) &= \\frac{ (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}{\\mathbb E(|X|^p)}\\\\\n&=  \\frac{(2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}{\\frac{\\gamma^p 2^{p+1}\\Gamma(\\frac{p+1}{2})\\Gamma(-p/\\alpha)}{\\alpha \\sqrt \\pi \\Gamma(-p/2)}} \\\\\n&= \\frac{2\\Gamma(-p/\\alpha)}{\\alpha\\Gamma(-p/2)}, & p&lt;\\alpha/2\n\\end{aligned}\n\\]\nThis uses the well-known results for normal absolute moments, stable FLOMs, and a change of variables from the standard deviation of G to the scale parameter \\(\\gamma\\) of X."
  },
  {
    "objectID": "corr_variance_derivation.html#preliminaries",
    "href": "corr_variance_derivation.html#preliminaries",
    "title": "Deriving Correlation Estimator Variance",
    "section": "",
    "text": "Starting with couple preliminaries that will be useful to derive an analyticaly tractable expression for the variance of these correlation estimators, \\(\\hat r\\) and \\(\\hat{ \\text{scov}}\\) based on the sample FLOM estimators.\n\n\nFirst, deriving the expected value for W \\(\\sim S_{\\alpha/2}(\\pi \\alpha/4)^{(2/\\alpha)},1, 0;1)\\), a maximally skewed scalar random variable with support on the positive real numbers. We can use the fact that we can decompose a sg-S\\(\\alpha\\)S(1) random variable into the product of W and normal random variable \\(G \\sim N(0,1)\\) independent of W:\n\\[\n\\begin{aligned}\n\\mathbb E(|X|^p) &= \\mathbb E(|W^{1/2}G|^p) \\\\\n&= \\mathbb E(W^{p/2}|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\mathbb E(|G|^p) \\\\\n&= \\mathbb E(W^{p/2}) \\sigma^p 2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n&= \\mathbb E(W^{p/2}) (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi} \\\\\n\\\\ \\text{So we get:} \\\\\n\\mathbb E(W^{p/2}) &= \\frac{ (2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}{\\mathbb E(|X|^p)}\\\\\n&=  \\frac{(2^{p/2}\\gamma^p)2^{p/2} \\frac{\\Gamma(\\frac{1+p}{2})}{\\sqrt \\pi}}{\\frac{\\gamma^p 2^{p+1}\\Gamma(\\frac{p+1}{2})\\Gamma(-p/\\alpha)}{\\alpha \\sqrt \\pi \\Gamma(-p/2)}} \\\\\n&= \\frac{2\\Gamma(-p/\\alpha)}{\\alpha\\Gamma(-p/2)}, & p&lt;\\alpha/2\n\\end{aligned}\n\\]\nThis uses the well-known results for normal absolute moments, stable FLOMs, and a change of variables from the standard deviation of G to the scale parameter \\(\\gamma\\) of X."
  }
]