---
title: "Restricted EM Derivation"
---

Given our $\alpha$-SG($\Sigma$) vector $X = \sqrt W G$, with underlying latent stable subordinator W, and Gaussian vector G, note that $X|W=w \sim N(\boldsymbol \mu,w\Sigma)$ we can write our complete data likelihood function as:

$$
\begin{aligned}
\mathcal L_c(\Theta) &\equiv f(\Theta | \textbf x, w) \\
& = f_{X|w}(\Theta, \textbf x|w)f_W(\Theta)
\end{aligned}
$$

Where $\Theta = (\theta, \alpha)^\text T$, $\theta = (\mu, \Sigma)$.
So the log likelihood for the observed data is:

$$
\begin{aligned}
\ell_c(\Theta|\textbf x, w) &= \sum_{i=1}^N \log f_{X|w}(\theta, x_i|w) + \sum_{i=1}^N \log f_W(\alpha, w_i ) \\
& = C - \frac n 2 \log |\Sigma| - \frac 1 2 \sum_{i=1}^n \frac{(\textbf x_i - \boldsymbol\mu)^\text T \Sigma ^{-1}(\textbf x_i - \boldsymbol\mu)}{w_i}+\sum_{i=1}^n \log f_W(\alpha, w_i ) 
\end{aligned}
$$

As in Teimouri et. al., the conditional expectation of the complete data log-likelihood is:

$$
\begin{aligned}
Q(\Theta|\Theta^{(t)}) = C - \frac n 2 \log |\Sigma| - \frac 1 2 \sum_{i=1}^n (\textbf x_i - \boldsymbol\mu)^\text T \Sigma ^{-1}(\textbf x_i - \boldsymbol\mu)e_i^{(t,s)}+\sum_{i=1}^n \mathbb E [\log f_W(\alpha, w_i ) | \textbf x_i, \Theta^{(t)} ]
\end{aligned}
$$

Here, the CM step involves the posterior mean of 1/W, denoted $e_i^{(t,s)}$ in the Teimouri paper. The paper provides an approximation of this value, but doesn't derive all the intermediate steps, which are at least helpful to be aware of. Using the fact that $X|W=w \sim N(\boldsymbol \mu,w\Sigma)$

$$
\begin{aligned}
e_i^{(t,s)} \equiv \mathbb E[W^{-1} | \textbf x_i, \boldsymbol \mu^{(t)}, \Sigma^{(s)}, \alpha^{(s)}] &= \int_0^\infty w^{-1} f_{W|\textbf X}(w|\textbf x)dw \\ 
& = \int_0^\infty w^{-1} \frac{f_{\textbf X, W}(\textbf x,w)}{f_{\textbf X}(x)}dw \\
& = \frac {1}{f_\textbf X(x)}\int_0^\infty w^{-1} f_{\textbf X, W}(\textbf x,w)dw \\
& =   \frac{\int_0^\infty w^{-1}f_W(w)f_{\textbf X|W}(\textbf x|w)dw}{\int_0^\infty f_W(w)f_{\textbf X|W}(\mathbf x|w)dw} \\
&=  \frac{\int_0^\infty w^{-1}f_W(w)f_{\textbf X|W}(\textbf x|w)dw}{\int_0^\infty f_W(w)f_{\textbf X|W}(\mathbf x|w)dw} \\
&= \frac{\int_0^\infty w^{-d/2-1}f_W(w|\alpha^{(t)})\exp\{-\frac{(\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})}{2w}\}dw}{\int_0^\infty w^{-d/2} f_W(w|\alpha^{(t)})\exp\{-\frac{(\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})}{2w}\}dw} \\
& \approx \frac{\sum_{i=1}^M w_i^{-d/2-1}\exp\{-\frac{(\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})}{2w_i}\}}{\sum_{i=1}^M w_i^{-d/2}\exp\{-\frac{(\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})}{2w_i}\}}
\end{aligned}
$$

Where $w_i \sim S_{\alpha^{(t)}/2}(\cos \frac{\pi \alpha}{4}^{2/\alpha}, 1, 0)$ for large M. Alternatively, subject to some straightforward regularity conditions, Teimouri et al. suggest swapping the Monte Carlo integration with a series representation for an almost exact evaluation of $f_W(.|\alpha^{(t)})$.

So we can use the following (admittedly horrible) representation:

$$
\begin{aligned}
e_i^{(t,s)} 
&= \frac{\int_0^\infty w^{-d/2-1}f_W(w|\alpha^{(t)})\exp\{-\frac{(\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})}{2w}\}dw}{\int_0^\infty w^{-d/2} f_W(w|\alpha^{(t)})\exp\{-\frac{(\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})}{2w}\}dw} \\
& \approx \frac{\sum_{j=1}^k \frac{(-1)^j}{\Gamma(j+1)}\Gamma(\frac{j\alpha^{(t)}+2}{2})\Gamma(\frac{j\alpha^{(t)}+d+2}{2})\sin(\frac{j \pi \alpha^{(t)}}{2})(D_i^{(t)})^{-\frac{j\alpha^{(t)}+d+2}{2}}}{\sum_{j=1}^k \frac{(-1)^j}{\Gamma(j+1)}\Gamma(\frac{j\alpha^{(t)}+2}{2})\Gamma(\frac{j\alpha^{(t)}+d}{2})\sin(\frac{j \pi \alpha^{(t)}}{2})(D_i^{(t)})^{-\frac{j\alpha^{(t)}+d}{2}}}
\end{aligned}
$$

where $D_i^{(t)} = (\textbf x - \boldsymbol\mu^{(t)})^\text T {(\Sigma^{(t)})} ^{-1}(\textbf x - \boldsymbol\mu^{(t)})$, $k = [\min(168, 168/\alpha^{(t)})]$, and the following condition is met (e.g. $D_i$ is sufficiently large):

$$
D_i^{(t)} \in \bigg( \bigg[\frac{\Gamma(\frac{k\alpha^{(t)}+\alpha^{(t)}+2}{2})\Gamma(\frac{k\alpha^{(t)}+\alpha^{(t)}+2+d}{2})}{(k+1)\Gamma(\frac{k\alpha^{(t)}+2}{2})\Gamma(\frac{k\alpha^{(t)}+2+d}{2})}\bigg]^{2/\alpha}, \infty\bigg)
$$

Despite being unpleasant to write in Latex, these expressions and conditions are extremely quick to compute/simulate/etc. 

Finally, returning to the complete data log likelihood, there's the issue of the second term, which doesn't have a closed form or clear path to maximization. To simplify the process, we use the fact that the $\alpha$ parameter is identical across all of the marginals and a fast univariate numerical MLE already implemented in several packages to quickly estimate the MLE for this parameter and treat it as fixed. We can either do this for an arbitrary marginal or sample of marginals, or if the dimensonality is small enough, by estimating all marginal $\alpha$ parameters and then averaging them.

From there, we can perform conditional EM as follows:

**E-step:** Estimate the posterior mean $e_i^{(t,t)} = \mathbb E[W^{-1} | \textbf x_i, \boldsymbol \mu^{(t)}, \Sigma^{(t)}, \alpha_{MLE}]$ as described above.

**M-step:** Update $\theta = (\boldsymbol \mu, \Sigma)$ as:

$\boldsymbol \mu^{(t+1)} = \frac{\sum_{i=1}^n \textbf x_i e_i^{(t,t)}}{\sum_{i=1}^n e_i^{(t,t)}}$, $\Sigma^{(t+1)} =  \sum_{i=1}^n (\textbf x_i - \boldsymbol\mu^{(t)}) (\textbf x_i - \boldsymbol\mu^{(t)})^\text T e_i^{(t,t)}/\sum_{i=1}^n e_i^{(t,t)}$

Repeat until estimates converge.

Besides fixing $\alpha$ with the MLE, another departure from Teimouri et al. is that we don't actually care about the estimated values of $\theta$ themselves. Instead, we only want to retain the vector $\hat{\textbf e}$, since the data are then approximately normal and we can use either the sample covariance matrix (the true MLE of $\Sigma$), or an adjustment for dimensionality that outperforms it.

As a reminder, note that we're estimating the covariance matrix of the normal vector, not the dispersion matrix of the stable vector $X$. The two may differ by a scaling constant of 1/2 depending on the way they're parameterized.
